{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b24b37a",
   "metadata": {},
   "source": [
    "# PPO\n",
    "\n",
    "We will use Ray libiray to implement the PPO model, with code modified from the official website of Ray.\n",
    "\n",
    "Note: 4000 timesteps per iteration.\n",
    "\n",
    "Code reference: https://docs.ray.io/en/latest/rllib/rllib-training.html#basic-python-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8117377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if needed: !pip install -U \"ray[rllib]\"\n",
    "import numpy as np\n",
    "from random import random\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import torch, torchvision, cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray import tune\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune import JupyterNotebookReporter, CLIReporter, ProgressReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce5bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device('cuda')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011698cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=[12345,42,42,42]\n",
    "def randomize(seed_rng=seeds[0], seed_np=seeds[1], seed_torch=seeds[2], seed_tf=seeds[3]):\n",
    "    tf.keras.backend.clear_session()\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    rng = np.random.default_rng(seed_rng)\n",
    "    np.random.seed(seed_np)\n",
    "    torch.manual_seed(seed_torch)\n",
    "    tf.random.set_seed(seed_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea9f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "#set config\n",
    "config_ppo = ppo.DEFAULT_CONFIG.copy()\n",
    "config_ppo[\"num_gpus\"] = 0\n",
    "config_ppo[\"num_workers\"] = 1\n",
    "config_ppo[\"framework\"] = 'torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c793ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 21:49:10,735\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-04-16 21:49:10,736\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1294)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1294)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1294)\u001b[0m [W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "#instantiate trainer\n",
    "trainer_ppo = ppo.PPOTrainer(config=config_ppo, env=\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99722a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 1,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'torch',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'vf_share_layers': -1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df146f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO training function\n",
    "\n",
    "def train(trainer, iteration=100, checkpoint_freq=10, metric='episode_reward_mean', seeds=seeds):\n",
    "    start_time=time.time()\n",
    "    randomize(seeds[0],seeds[1],seeds[2],seeds[3])\n",
    "    metrics = []\n",
    "    checkpoint = None\n",
    "    for i in range(iteration):\n",
    "        result = trainer.train()\n",
    "        print(result[metric])\n",
    "        metrics.append(result[metric])\n",
    "\n",
    "    if (i+1) % checkpoint_freq == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)\n",
    "    \n",
    "    end_time=time.time()\n",
    "    print(\"{} training iterations done in {:.2f} seconds\".format(iteration, end_time-start_time))\n",
    "\n",
    "    return metrics, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771087eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2857142857142856\n",
      "1.9655172413793103\n",
      "2 training iterations done in 975.37 seconds\n"
     ]
    }
   ],
   "source": [
    "metrics, checkpoint = train(trainer_ppo, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a33b1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters to tune\n",
    "config_ppo_tune={\n",
    "        \"env\": \"Breakout-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"framework\": \"torch\",\n",
    "        \"clip_param\": tune.grid_search([0.3, 0.2]),\n",
    "        \"lr\": tune.grid_search([5e-5, 1e-5]),\n",
    "    }\n",
    "#set stopping criteria & reporting style/frequency for tuning\n",
    "stop = tune.stopper.MaximumIterationStopper(max_iter=20)\n",
    "reporter = JupyterNotebookReporter(overwrite=False, max_report_frequency=1000)   #unit: seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc28ef58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-16 22:05:31 (running for 00:00:00.14)<br>Memory usage on this node: 7.5/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (4 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=1291)\u001b[0m 2022-04-16 22:05:37,538\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1291)\u001b[0m 2022-04-16 22:05:37,538\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1295)\u001b[0m 2022-04-16 22:05:37,538\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1295)\u001b[0m 2022-04-16 22:05:37,538\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1292)\u001b[0m 2022-04-16 22:05:37,538\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1292)\u001b[0m 2022-04-16 22:05:37,538\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1293)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1293)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1290)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1290)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1288)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1288)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1293)\u001b[0m [W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1290)\u001b[0m [W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1288)\u001b[0m [W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1291)\u001b[0m [W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1295)\u001b[0m [W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1292)\u001b[0m [W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-15-06\n",
      "  done: false\n",
      "  episode_len_mean: 260.05882352941177\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.588235294117647\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 17\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.856729704574391e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: .inf\n",
      "          policy_loss: -0.0019813127214870145\n",
      "          total_loss: .inf\n",
      "          vf_explained_var: 0.010308999848622147\n",
      "          vf_loss: 3.8098805889166814\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.47613488975357\n",
      "    ram_util_percent: 95.3486381322957\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05220127415579577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8456789830481223\n",
      "    mean_inference_ms: 3.579776604692211\n",
      "    mean_raw_obs_processing_ms: 0.22406305142922034\n",
      "  time_since_restore: 563.8701219558716\n",
      "  time_this_iter_s: 563.8701219558716\n",
      "  time_total_s: 563.8701219558716\n",
      "  timers:\n",
      "    learn_throughput: 7.347\n",
      "    learn_time_ms: 544458.245\n",
      "    load_throughput: 4678532.069\n",
      "    load_time_ms: 0.855\n",
      "    sample_throughput: 205.947\n",
      "    sample_time_ms: 19422.438\n",
      "    update_time_ms: 21.828\n",
      "  timestamp: 1650143706\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 286.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.8666666666666667\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 15\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.3054092405860939\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.3719902557431302\n",
      "          policy_loss: 0.0563145706314914\n",
      "          total_loss: 4.086629588309155\n",
      "          vf_explained_var: -0.05123328015368472\n",
      "          vf_loss: 3.9559169850442357\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0006468305304\n",
      "    ram_util_percent: 95.34036222509704\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0514832772901135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8528029969321941\n",
      "    mean_inference_ms: 3.5779355794481864\n",
      "    mean_raw_obs_processing_ms: 0.22403772578660147\n",
      "  time_since_restore: 564.5031476020813\n",
      "  time_this_iter_s: 564.5031476020813\n",
      "  time_total_s: 564.5031476020813\n",
      "  timers:\n",
      "    learn_throughput: 7.338\n",
      "    learn_time_ms: 545118.975\n",
      "    load_throughput: 4814122.238\n",
      "    load_time_ms: 0.831\n",
      "    sample_throughput: 206.085\n",
      "    sample_time_ms: 19409.495\n",
      "    update_time_ms: 6.827\n",
      "  timestamp: 1650143707\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 311.14285714285717\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 2.5\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 14\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.05886136686841842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030015626456148813\n",
      "          policy_loss: -0.006240319711486659\n",
      "          total_loss: 0.6368079545900166\n",
      "          vf_explained_var: 0.33861924531639265\n",
      "          vf_loss: 0.6370451505586344\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.44877102199224\n",
      "    ram_util_percent: 95.34915912031047\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0519735221414678\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.851001896818886\n",
      "    mean_inference_ms: 3.56352213054143\n",
      "    mean_raw_obs_processing_ms: 0.2056991598124029\n",
      "  time_since_restore: 564.9309160709381\n",
      "  time_this_iter_s: 564.9309160709381\n",
      "  time_total_s: 564.9309160709381\n",
      "  timers:\n",
      "    learn_throughput: 7.331\n",
      "    learn_time_ms: 545635.861\n",
      "    load_throughput: 637335.359\n",
      "    load_time_ms: 6.276\n",
      "    sample_throughput: 207.302\n",
      "    sample_time_ms: 19295.556\n",
      "    update_time_ms: 21.812\n",
      "  timestamp: 1650143707\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-16 22:22:11 (running for 00:16:40.46)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         563.87 </td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 1.58824</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           260.059</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         564.503</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 1.86667</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           286.2  </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         564.931</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 2.5    </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">           311.143</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 287.5483870967742\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.903225806451613\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 31\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.3902261104314558\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021033264640635187\n",
      "          policy_loss: -0.011938265256798997\n",
      "          total_loss: 0.3451614955304972\n",
      "          vf_explained_var: 0.22069576626182885\n",
      "          vf_loss: 0.35078978193623406\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.54732724902217\n",
      "    ram_util_percent: 95.16140808344197\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05143024634501144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.849966845853768\n",
      "    mean_inference_ms: 3.569673346517773\n",
      "    mean_raw_obs_processing_ms: 0.22383190679371062\n",
      "  time_since_restore: 1125.4900596141815\n",
      "  time_this_iter_s: 560.9869120121002\n",
      "  time_total_s: 1125.4900596141815\n",
      "  timers:\n",
      "    learn_throughput: 7.36\n",
      "    learn_time_ms: 543452.597\n",
      "    load_throughput: 5073243.423\n",
      "    load_time_ms: 0.788\n",
      "    sample_throughput: 13.704\n",
      "    sample_time_ms: 291881.669\n",
      "    update_time_ms: 8.492\n",
      "  timestamp: 1650144268\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 273.28125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.84375\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 32\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.00041324404939528436\n",
      "          total_loss: 0.4452460960570162\n",
      "          vf_explained_var: 0.22327972599255141\n",
      "          vf_loss: 0.4456593403011881\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.93259452411995\n",
      "    ram_util_percent: 95.17209908735333\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05210955893357462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8472641804591572\n",
      "    mean_inference_ms: 3.5763613316183136\n",
      "    mean_raw_obs_processing_ms: 0.22626245036961606\n",
      "  time_since_restore: 1125.8455011844635\n",
      "  time_this_iter_s: 561.9753792285919\n",
      "  time_total_s: 1125.8455011844635\n",
      "  timers:\n",
      "    learn_throughput: 7.36\n",
      "    learn_time_ms: 543484.21\n",
      "    load_throughput: 5115784.723\n",
      "    load_time_ms: 0.782\n",
      "    sample_throughput: 13.713\n",
      "    sample_time_ms: 291701.748\n",
      "    update_time_ms: 15.63\n",
      "  timestamp: 1650144268\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-24-30\n",
      "  done: false\n",
      "  episode_len_mean: 285.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 2.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 30\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.028016985933326424\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0019629370738696457\n",
      "          policy_loss: 0.0013091092708931175\n",
      "          total_loss: 0.17180820445790487\n",
      "          vf_explained_var: 0.5018116720902023\n",
      "          vf_loss: 0.16991021286856423\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.47074122236671\n",
      "    ram_util_percent: 95.16410923276983\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05187776919943961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8491365898061863\n",
      "    mean_inference_ms: 3.5567679167700037\n",
      "    mean_raw_obs_processing_ms: 0.2093256215591456\n",
      "  time_since_restore: 1127.2786781787872\n",
      "  time_this_iter_s: 562.3477621078491\n",
      "  time_total_s: 1127.2786781787872\n",
      "  timers:\n",
      "    learn_throughput: 7.349\n",
      "    learn_time_ms: 544260.111\n",
      "    load_throughput: 1138750.831\n",
      "    load_time_ms: 3.513\n",
      "    sample_throughput: 13.688\n",
      "    sample_time_ms: 292221.512\n",
      "    update_time_ms: 20.21\n",
      "  timestamp: 1650144270\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: f344c_00002\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 276.1458333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.7291666666666667\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 48\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.3812310146548415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013202755082486932\n",
      "          policy_loss: -0.022567906078471933\n",
      "          total_loss: 0.04751635391004784\n",
      "          vf_explained_var: 0.3592787069659079\n",
      "          vf_loss: 0.06414301964023741\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.889206762028614\n",
      "    ram_util_percent: 95.23745123537061\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05135040260788243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8488575588550882\n",
      "    mean_inference_ms: 3.566854430740793\n",
      "    mean_raw_obs_processing_ms: 0.2239680156669749\n",
      "  time_since_restore: 1688.778455734253\n",
      "  time_this_iter_s: 563.2883961200714\n",
      "  time_total_s: 1688.778455734253\n",
      "  timers:\n",
      "    learn_throughput: 7.358\n",
      "    learn_time_ms: 543637.612\n",
      "    load_throughput: 5117084.994\n",
      "    load_time_ms: 0.782\n",
      "    sample_throughput: 10.482\n",
      "    sample_time_ms: 381613.285\n",
      "    update_time_ms: 14.863\n",
      "  timestamp: 1650144831\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 272.42857142857144\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.836734693877551\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 49\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.004746703250754264\n",
      "          total_loss: 0.13172655885798795\n",
      "          vf_explained_var: 0.4944769884309461\n",
      "          vf_loss: 0.13647325339536834\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.38722294654498\n",
      "    ram_util_percent: 95.2264667535854\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0519856008482221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8467280476962658\n",
      "    mean_inference_ms: 3.574512465198916\n",
      "    mean_raw_obs_processing_ms: 0.22523297565848072\n",
      "  time_since_restore: 1688.884771347046\n",
      "  time_this_iter_s: 563.0392701625824\n",
      "  time_total_s: 1688.884771347046\n",
      "  timers:\n",
      "    learn_throughput: 7.358\n",
      "    learn_time_ms: 543592.394\n",
      "    load_throughput: 5234156.406\n",
      "    load_time_ms: 0.764\n",
      "    sample_throughput: 10.479\n",
      "    sample_time_ms: 381721.479\n",
      "    update_time_ms: 14.014\n",
      "  timestamp: 1650144831\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f344c_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-33-53\n",
      "  done: false\n",
      "  episode_len_mean: 281.9782608695652\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 2.0434782608695654\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 46\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.04160231812503464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015384514787033494\n",
      "          policy_loss: 0.009687073500726813\n",
      "          total_loss: 0.1274550463757678\n",
      "          vf_explained_var: 0.5180497203462867\n",
      "          vf_loss: 0.11546029535232372\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.30871261378414\n",
      "    ram_util_percent: 95.232899869961\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05177946456086982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8480991098495191\n",
      "    mean_inference_ms: 3.5552806821536636\n",
      "    mean_raw_obs_processing_ms: 0.2104257834057903\n",
      "  time_since_restore: 1690.8691725730896\n",
      "  time_this_iter_s: 563.5904943943024\n",
      "  time_total_s: 1690.8691725730896\n",
      "  timers:\n",
      "    learn_throughput: 7.349\n",
      "    learn_time_ms: 544266.031\n",
      "    load_throughput: 1564649.59\n",
      "    load_time_ms: 2.556\n",
      "    sample_throughput: 10.465\n",
      "    sample_time_ms: 382226.33\n",
      "    update_time_ms: 15.469\n",
      "  timestamp: 1650144833\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-16 22:38:52 (running for 00:33:21.30)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         1688.88</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 1.83673</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           272.429</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         1688.78</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 1.72917</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           276.146</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         1690.87</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 2.04348</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           281.978</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-43-15\n",
      "  done: false\n",
      "  episode_len_mean: 279.234375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.8125\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 64\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.38075154990919174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008660254109997101\n",
      "          policy_loss: -0.0237756265036922\n",
      "          total_loss: 0.017749836052122017\n",
      "          vf_explained_var: 0.6143425121102282\n",
      "          vf_loss: 0.03762834877206353\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.06501950585176\n",
      "    ram_util_percent: 95.17152145643692\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05129425643999207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8484342215713145\n",
      "    mean_inference_ms: 3.565008443593467\n",
      "    mean_raw_obs_processing_ms: 0.22337425829055324\n",
      "  time_since_restore: 2252.7481796741486\n",
      "  time_this_iter_s: 563.9697239398956\n",
      "  time_total_s: 2252.7481796741486\n",
      "  timers:\n",
      "    learn_throughput: 7.354\n",
      "    learn_time_ms: 543906.237\n",
      "    load_throughput: 5299602.306\n",
      "    load_time_ms: 0.755\n",
      "    sample_throughput: 9.367\n",
      "    sample_time_ms: 427027.675\n",
      "    update_time_ms: 18.904\n",
      "  timestamp: 1650145395\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-43-16\n",
      "  done: false\n",
      "  episode_len_mean: 273.7230769230769\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.8615384615384616\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 65\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: 0.004772945998176452\n",
      "          total_loss: 0.13622267803487678\n",
      "          vf_explained_var: 0.3612139464065593\n",
      "          vf_loss: 0.13144973586163214\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.77587548638132\n",
      "    ram_util_percent: 95.1604409857328\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05190803287109988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.84579815659442\n",
      "    mean_inference_ms: 3.573076825806895\n",
      "    mean_raw_obs_processing_ms: 0.22405127894277635\n",
      "  time_since_restore: 2253.723261117935\n",
      "  time_this_iter_s: 564.8384897708893\n",
      "  time_total_s: 2253.723261117935\n",
      "  timers:\n",
      "    learn_throughput: 7.352\n",
      "    learn_time_ms: 544070.846\n",
      "    load_throughput: 5136930.802\n",
      "    load_time_ms: 0.779\n",
      "    sample_throughput: 9.366\n",
      "    sample_time_ms: 427068.389\n",
      "    update_time_ms: 15.117\n",
      "  timestamp: 1650145396\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-43-19\n",
      "  done: false\n",
      "  episode_len_mean: 280.015873015873\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 2.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 63\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.07395486436904439\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003897827776007531\n",
      "          policy_loss: -0.00144654786194204\n",
      "          total_loss: 0.05726952020063876\n",
      "          vf_explained_var: 0.4258567565871823\n",
      "          vf_loss: 0.058131392937033406\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.50428015564203\n",
      "    ram_util_percent: 95.17496757457846\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171249895218529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8474374076394376\n",
      "    mean_inference_ms: 3.5533068307096483\n",
      "    mean_raw_obs_processing_ms: 0.21159452816217084\n",
      "  time_since_restore: 2256.08531165123\n",
      "  time_this_iter_s: 565.2161390781403\n",
      "  time_total_s: 2256.08531165123\n",
      "  timers:\n",
      "    learn_throughput: 7.345\n",
      "    learn_time_ms: 544556.95\n",
      "    load_throughput: 1887413.207\n",
      "    load_time_ms: 2.119\n",
      "    sample_throughput: 9.353\n",
      "    sample_time_ms: 427680.546\n",
      "    update_time_ms: 13.279\n",
      "  timestamp: 1650145399\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: f344c_00002\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-52-44\n",
      "  done: false\n",
      "  episode_len_mean: 282.39240506329116\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.860759493670886\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 79\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.3753646934705396\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011751516599580144\n",
      "          policy_loss: -0.019745525570526237\n",
      "          total_loss: 0.03482769262371084\n",
      "          vf_explained_var: 0.497453829549974\n",
      "          vf_loss: 0.04928503718492525\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.685585585585585\n",
      "    ram_util_percent: 95.2906048906049\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0513163349118003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8488985496758137\n",
      "    mean_inference_ms: 3.5686160058452865\n",
      "    mean_raw_obs_processing_ms: 0.22320029958981083\n",
      "  time_since_restore: 2821.7513127326965\n",
      "  time_this_iter_s: 569.003133058548\n",
      "  time_total_s: 2821.7513127326965\n",
      "  timers:\n",
      "    learn_throughput: 7.34\n",
      "    learn_time_ms: 544931.716\n",
      "    load_throughput: 5140078.431\n",
      "    load_time_ms: 0.778\n",
      "    sample_throughput: 8.799\n",
      "    sample_time_ms: 454571.879\n",
      "    update_time_ms: 17.027\n",
      "  timestamp: 1650145964\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-52-46\n",
      "  done: false\n",
      "  episode_len_mean: 274.4074074074074\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.8888888888888888\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 81\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.0023438302018950062\n",
      "          total_loss: 0.035102970425480155\n",
      "          vf_explained_var: 0.5529360479565077\n",
      "          vf_loss: 0.03744679881079543\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.111553273427475\n",
      "    ram_util_percent: 95.30064184852377\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051915880591649935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8459164572455001\n",
      "    mean_inference_ms: 3.5769668312593814\n",
      "    mean_raw_obs_processing_ms: 0.2234468227032624\n",
      "  time_since_restore: 2823.238101243973\n",
      "  time_this_iter_s: 569.5148401260376\n",
      "  time_total_s: 2823.238101243973\n",
      "  timers:\n",
      "    learn_throughput: 7.337\n",
      "    learn_time_ms: 545156.256\n",
      "    load_throughput: 5191612.823\n",
      "    load_time_ms: 0.77\n",
      "    sample_throughput: 8.796\n",
      "    sample_time_ms: 454772.667\n",
      "    update_time_ms: 13.329\n",
      "  timestamp: 1650145966\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f344c_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_22-52-48\n",
      "  done: false\n",
      "  episode_len_mean: 273.18518518518516\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.8641975308641976\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 81\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.02416586984039387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023998906270626355\n",
      "          policy_loss: -0.01003260413004506\n",
      "          total_loss: 0.05535131969698204\n",
      "          vf_explained_var: 0.23574588971753274\n",
      "          vf_loss: 0.0635840077103386\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.20064267352185\n",
      "    ram_util_percent: 95.29074550128534\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05173793238104491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8478596454812146\n",
      "    mean_inference_ms: 3.5562235409126837\n",
      "    mean_raw_obs_processing_ms: 0.21341429676122484\n",
      "  time_since_restore: 2825.8942897319794\n",
      "  time_this_iter_s: 569.8089780807495\n",
      "  time_total_s: 2825.8942897319794\n",
      "  timers:\n",
      "    learn_throughput: 7.331\n",
      "    learn_time_ms: 545629.839\n",
      "    load_throughput: 1433386.532\n",
      "    load_time_ms: 2.791\n",
      "    sample_throughput: 8.787\n",
      "    sample_time_ms: 455216.017\n",
      "    update_time_ms: 11.813\n",
      "  timestamp: 1650145968\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-16 22:55:33 (running for 00:50:02.31)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2823.24</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 1.88889</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           274.407</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2821.75</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 1.86076</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           282.392</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2825.89</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 1.8642 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           273.185</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 281.4736842105263\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.8421052631578947\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 95\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.43192795524674077\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012403165225698677\n",
      "          policy_loss: -0.026949060565581724\n",
      "          total_loss: 0.042160559649408985\n",
      "          vf_explained_var: 0.3511443598296053\n",
      "          vf_loss: 0.06352819603597445\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.0034749034749\n",
      "    ram_util_percent: 95.39163449163448\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05133513883378869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8494684475263797\n",
      "    mean_inference_ms: 3.572185583609908\n",
      "    mean_raw_obs_processing_ms: 0.22316395775572276\n",
      "  time_since_restore: 3390.393212556839\n",
      "  time_this_iter_s: 568.6418998241425\n",
      "  time_total_s: 3390.393212556839\n",
      "  timers:\n",
      "    learn_throughput: 7.332\n",
      "    learn_time_ms: 545586.981\n",
      "    load_throughput: 5041230.769\n",
      "    load_time_ms: 0.793\n",
      "    sample_throughput: 8.446\n",
      "    sample_time_ms: 473615.262\n",
      "    update_time_ms: 16.141\n",
      "  timestamp: 1650146533\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-02-14\n",
      "  done: false\n",
      "  episode_len_mean: 272.83673469387753\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.8673469387755102\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 98\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.01875\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: 0.001643940205535581\n",
      "          total_loss: 0.0527834795440437\n",
      "          vf_explained_var: 0.3978968494681902\n",
      "          vf_loss: 0.05113953987126469\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.61184041184041\n",
      "    ram_util_percent: 95.3966537966538\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05193841923984632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8462312968561317\n",
      "    mean_inference_ms: 3.5804211865624556\n",
      "    mean_raw_obs_processing_ms: 0.2231138299298822\n",
      "  time_since_restore: 3391.9162771701813\n",
      "  time_this_iter_s: 568.6781759262085\n",
      "  time_total_s: 3391.9162771701813\n",
      "  timers:\n",
      "    learn_throughput: 7.329\n",
      "    learn_time_ms: 545750.075\n",
      "    load_throughput: 5245612.09\n",
      "    load_time_ms: 0.763\n",
      "    sample_throughput: 8.441\n",
      "    sample_time_ms: 473888.46\n",
      "    update_time_ms: 12.68\n",
      "  timestamp: 1650146534\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-02-17\n",
      "  done: false\n",
      "  episode_len_mean: 275.9375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.9166666666666667\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 96\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.11250000000000003\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.009709751560102075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00447898294264088\n",
      "          policy_loss: 0.001512516394097318\n",
      "          total_loss: 0.058954562539465086\n",
      "          vf_explained_var: 0.5114929376109953\n",
      "          vf_loss: 0.05693815908224512\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.352252252252256\n",
      "    ram_util_percent: 95.3958815958816\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05178442236752121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8483716487828311\n",
      "    mean_inference_ms: 3.5596887891174016\n",
      "    mean_raw_obs_processing_ms: 0.21428333717270043\n",
      "  time_since_restore: 3394.621629714966\n",
      "  time_this_iter_s: 568.7273399829865\n",
      "  time_total_s: 3394.621629714966\n",
      "  timers:\n",
      "    learn_throughput: 7.324\n",
      "    learn_time_ms: 546176.7\n",
      "    load_throughput: 1626460.972\n",
      "    load_time_ms: 2.459\n",
      "    sample_throughput: 8.433\n",
      "    sample_time_ms: 474306.472\n",
      "    update_time_ms: 11.6\n",
      "  timestamp: 1650146537\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: f344c_00002\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-11-39\n",
      "  done: false\n",
      "  episode_len_mean: 285.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.92\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 109\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.44165603276542437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011443441317507843\n",
      "          policy_loss: -0.04255878589646791\n",
      "          total_loss: 0.0035400841209877244\n",
      "          vf_explained_var: 0.6486260198136812\n",
      "          vf_loss: 0.04094932115416453\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.15064766839379\n",
      "    ram_util_percent: 95.20155440414509\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05132456203138536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8493720423927872\n",
      "    mean_inference_ms: 3.573266958754104\n",
      "    mean_raw_obs_processing_ms: 0.22264651838133515\n",
      "  time_since_restore: 3956.3281724452972\n",
      "  time_this_iter_s: 565.9349598884583\n",
      "  time_total_s: 3956.3281724452972\n",
      "  timers:\n",
      "    learn_throughput: 7.329\n",
      "    learn_time_ms: 545751.521\n",
      "    load_throughput: 5125720.67\n",
      "    load_time_ms: 0.78\n",
      "    sample_throughput: 8.212\n",
      "    sample_time_ms: 487108.818\n",
      "    update_time_ms: 17.001\n",
      "  timestamp: 1650147099\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: f344c_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-11-41\n",
      "  done: false\n",
      "  episode_len_mean: 275.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.92\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 114\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.008549073921336282\n",
      "          total_loss: 0.017683792939930353\n",
      "          vf_explained_var: 0.6503314929623758\n",
      "          vf_loss: 0.026232864535463755\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.7830310880829\n",
      "    ram_util_percent: 95.21217616580311\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05190098961293696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8462674070123556\n",
      "    mean_inference_ms: 3.582388742846449\n",
      "    mean_raw_obs_processing_ms: 0.22264994097396718\n",
      "  time_since_restore: 3958.010929107666\n",
      "  time_this_iter_s: 566.0946519374847\n",
      "  time_total_s: 3958.010929107666\n",
      "  timers:\n",
      "    learn_throughput: 7.328\n",
      "    learn_time_ms: 545853.342\n",
      "    load_throughput: 5267099.251\n",
      "    load_time_ms: 0.759\n",
      "    sample_throughput: 8.207\n",
      "    sample_time_ms: 487383.65\n",
      "    update_time_ms: 13.024\n",
      "  timestamp: 1650147101\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-11-44\n",
      "  done: false\n",
      "  episode_len_mean: 270.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.83\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 113\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.056250000000000015\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.0030116380473989394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.4438171739243576e-06\n",
      "          policy_loss: -0.008515351380832413\n",
      "          total_loss: 0.04545696866677831\n",
      "          vf_explained_var: 0.4506220571456417\n",
      "          vf_loss: 0.053972125527078425\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.91653746770026\n",
      "    ram_util_percent: 95.1877260981912\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180365450981578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8483654673735043\n",
      "    mean_inference_ms: 3.5622174431596556\n",
      "    mean_raw_obs_processing_ms: 0.21625849331083116\n",
      "  time_since_restore: 3961.221390724182\n",
      "  time_this_iter_s: 566.5997610092163\n",
      "  time_total_s: 3961.221390724182\n",
      "  timers:\n",
      "    learn_throughput: 7.322\n",
      "    learn_time_ms: 546288.82\n",
      "    load_throughput: 1804140.287\n",
      "    load_time_ms: 2.217\n",
      "    sample_throughput: 8.201\n",
      "    sample_time_ms: 487775.017\n",
      "    update_time_ms: 10.874\n",
      "  timestamp: 1650147104\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-16 23:12:14 (running for 01:06:43.25)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         3958.01</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">    1.92</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            275.74</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         3956.33</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">    1.92</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            285.3 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         3961.22</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">    1.83</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            270.81</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-21-39\n",
      "  done: false\n",
      "  episode_len_mean: 290.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 2.02\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 123\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.45427675040498855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013301539845565691\n",
      "          policy_loss: -0.04088703249130518\n",
      "          total_loss: -0.004614013568648407\n",
      "          vf_explained_var: 0.5195522898627866\n",
      "          vf_loss: 0.03028732718253929\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.142610837438426\n",
      "    ram_util_percent: 95.56145320197044\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05131349330801289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8495333086900604\n",
      "    mean_inference_ms: 3.5745795134351654\n",
      "    mean_raw_obs_processing_ms: 0.2219406571519947\n",
      "  time_since_restore: 4555.8724064826965\n",
      "  time_this_iter_s: 599.5442340373993\n",
      "  time_total_s: 4555.8724064826965\n",
      "  timers:\n",
      "    learn_throughput: 7.272\n",
      "    learn_time_ms: 550070.613\n",
      "    load_throughput: 5135947.959\n",
      "    load_time_ms: 0.779\n",
      "    sample_throughput: 8.049\n",
      "    sample_time_ms: 496969.884\n",
      "    update_time_ms: 18.887\n",
      "  timestamp: 1650147699\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 270.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.81\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 132\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0046875\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: 0.0018682900155263563\n",
      "          total_loss: 0.042728910892100264\n",
      "          vf_explained_var: 0.18673828007072532\n",
      "          vf_loss: 0.04086062007894119\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.05865030674847\n",
      "    ram_util_percent: 95.55447852760737\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05187199506764299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8456616994436712\n",
      "    mean_inference_ms: 3.584773765098242\n",
      "    mean_raw_obs_processing_ms: 0.22152919496598977\n",
      "  time_since_restore: 4558.80986905098\n",
      "  time_this_iter_s: 600.7989399433136\n",
      "  time_total_s: 4558.80986905098\n",
      "  timers:\n",
      "    learn_throughput: 7.27\n",
      "    learn_time_ms: 550200.303\n",
      "    load_throughput: 3979533.549\n",
      "    load_time_ms: 1.005\n",
      "    sample_throughput: 8.044\n",
      "    sample_time_ms: 497293.892\n",
      "    update_time_ms: 12.595\n",
      "  timestamp: 1650147701\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-21-44\n",
      "  done: false\n",
      "  episode_len_mean: 273.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.86\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 128\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.028125000000000008\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.00547033111807034\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00048615745671631627\n",
      "          policy_loss: 0.012696470159997222\n",
      "          total_loss: 0.0894619275736923\n",
      "          vf_explained_var: 0.4119963229343455\n",
      "          vf_loss: 0.07675178130207364\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.345018450184504\n",
      "    ram_util_percent: 95.56113161131613\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05182681153306271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8486406304389217\n",
      "    mean_inference_ms: 3.564833345221849\n",
      "    mean_raw_obs_processing_ms: 0.2172574594313432\n",
      "  time_since_restore: 4561.040697813034\n",
      "  time_this_iter_s: 599.8193070888519\n",
      "  time_total_s: 4561.040697813034\n",
      "  timers:\n",
      "    learn_throughput: 7.265\n",
      "    learn_time_ms: 550595.214\n",
      "    load_throughput: 1959354.287\n",
      "    load_time_ms: 2.041\n",
      "    sample_throughput: 8.039\n",
      "    sample_time_ms: 497561.801\n",
      "    update_time_ms: 10.509\n",
      "  timestamp: 1650147704\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-16 23:28:55 (running for 01:23:24.07)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         4558.81</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">    1.81</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            270.55</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         4555.87</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">    2.02</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            290.8 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         4561.04</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">    1.86</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            273.79</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-31-55\n",
      "  done: false\n",
      "  episode_len_mean: 284.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.91\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 140\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.46316933300225965\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011067126589757092\n",
      "          policy_loss: -0.0259483426904446\n",
      "          total_loss: -0.004958228545603893\n",
      "          vf_explained_var: 0.320629955491712\n",
      "          vf_loss: 0.01600990809615381\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.93886883273164\n",
      "    ram_util_percent: 95.5338146811071\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05138015487975322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8509581915340158\n",
      "    mean_inference_ms: 3.5811886045940753\n",
      "    mean_raw_obs_processing_ms: 0.2214596210976659\n",
      "  time_since_restore: 5172.035199403763\n",
      "  time_this_iter_s: 616.1627929210663\n",
      "  time_total_s: 5172.035199403763\n",
      "  timers:\n",
      "    learn_throughput: 7.205\n",
      "    learn_time_ms: 555135.881\n",
      "    load_throughput: 5094468.234\n",
      "    load_time_ms: 0.785\n",
      "    sample_throughput: 7.866\n",
      "    sample_time_ms: 508514.677\n",
      "    update_time_ms: 19.535\n",
      "  timestamp: 1650148315\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-31-57\n",
      "  done: false\n",
      "  episode_len_mean: 270.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.83\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 148\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00234375\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.0038969490316606336\n",
      "          total_loss: 0.01764432104624888\n",
      "          vf_explained_var: 0.7052298349077983\n",
      "          vf_loss: 0.021541268958069223\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.80036101083032\n",
      "    ram_util_percent: 95.53176895306858\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051977203752253454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8463293042112671\n",
      "    mean_inference_ms: 3.5917387775490384\n",
      "    mean_raw_obs_processing_ms: 0.2214064860648447\n",
      "  time_since_restore: 5174.345319032669\n",
      "  time_this_iter_s: 615.5354499816895\n",
      "  time_total_s: 5174.345319032669\n",
      "  timers:\n",
      "    learn_throughput: 7.205\n",
      "    learn_time_ms: 555159.438\n",
      "    load_throughput: 4089454.92\n",
      "    load_time_ms: 0.978\n",
      "    sample_throughput: 7.861\n",
      "    sample_time_ms: 508856.155\n",
      "    update_time_ms: 12.486\n",
      "  timestamp: 1650148317\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-32-01\n",
      "  done: false\n",
      "  episode_len_mean: 275.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.88\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 144\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.014062500000000004\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.004598972703767872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011311786937674376\n",
      "          policy_loss: 0.004518390573080509\n",
      "          total_loss: 0.043155574208245645\n",
      "          vf_explained_var: 0.5297025467118909\n",
      "          vf_loss: 0.03862127650689374\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.527349397590356\n",
      "    ram_util_percent: 95.51807228915662\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05196626736451611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8499419225389512\n",
      "    mean_inference_ms: 3.572045904138487\n",
      "    mean_raw_obs_processing_ms: 0.21873131067378415\n",
      "  time_since_restore: 5177.883583545685\n",
      "  time_this_iter_s: 616.8428857326508\n",
      "  time_total_s: 5177.883583545685\n",
      "  timers:\n",
      "    learn_throughput: 7.199\n",
      "    learn_time_ms: 555661.677\n",
      "    load_throughput: 2117266.027\n",
      "    load_time_ms: 1.889\n",
      "    sample_throughput: 7.857\n",
      "    sample_time_ms: 509101.094\n",
      "    update_time_ms: 10.619\n",
      "  timestamp: 1650148321\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: f344c_00002\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-41-56\n",
      "  done: false\n",
      "  episode_len_mean: 287.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.97\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 157\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.4585921200533067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010924955003780944\n",
      "          policy_loss: -0.040043299113430325\n",
      "          total_loss: -0.01919074710859086\n",
      "          vf_explained_var: 0.4357340844087703\n",
      "          vf_loss: 0.015936320799437943\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.503194103194105\n",
      "    ram_util_percent: 94.98660933660935\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051517709458949214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8529592743351947\n",
      "    mean_inference_ms: 3.5912042592024096\n",
      "    mean_raw_obs_processing_ms: 0.22143961177191632\n",
      "  time_since_restore: 5772.761436462402\n",
      "  time_this_iter_s: 600.7262370586395\n",
      "  time_total_s: 5772.761436462402\n",
      "  timers:\n",
      "    learn_throughput: 7.173\n",
      "    learn_time_ms: 557647.867\n",
      "    load_throughput: 5073396.837\n",
      "    load_time_ms: 0.788\n",
      "    sample_throughput: 7.703\n",
      "    sample_time_ms: 519276.913\n",
      "    update_time_ms: 20.189\n",
      "  timestamp: 1650148916\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-41-57\n",
      "  done: false\n",
      "  episode_len_mean: 269.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.81\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 165\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.001171875\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.004810633618504771\n",
      "          total_loss: 0.019526459439907985\n",
      "          vf_explained_var: 0.4216453552246094\n",
      "          vf_loss: 0.02433709025568259\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.82337023370233\n",
      "    ram_util_percent: 94.98745387453873\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052163263372212276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8480364684748026\n",
      "    mean_inference_ms: 3.6024003545338252\n",
      "    mean_raw_obs_processing_ms: 0.22183322210875878\n",
      "  time_since_restore: 5774.799058914185\n",
      "  time_this_iter_s: 600.4537398815155\n",
      "  time_total_s: 5774.799058914185\n",
      "  timers:\n",
      "    learn_throughput: 7.173\n",
      "    learn_time_ms: 557647.012\n",
      "    load_throughput: 4181341.84\n",
      "    load_time_ms: 0.957\n",
      "    sample_throughput: 7.7\n",
      "    sample_time_ms: 519496.965\n",
      "    update_time_ms: 11.919\n",
      "  timestamp: 1650148917\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: f344c_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-42-01\n",
      "  done: false\n",
      "  episode_len_mean: 269.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.78\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 162\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.007031250000000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.00017707610883418613\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0236705148065323e-06\n",
      "          policy_loss: -0.003839955385774374\n",
      "          total_loss: 0.025622439069041818\n",
      "          vf_explained_var: 0.3928989855832951\n",
      "          vf_loss: 0.029462380013278416\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.03677736777368\n",
      "    ram_util_percent: 94.9959409594096\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052199372870942194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8519811282743487\n",
      "    mean_inference_ms: 3.583461961811039\n",
      "    mean_raw_obs_processing_ms: 0.2202199903736471\n",
      "  time_since_restore: 5778.3370015621185\n",
      "  time_this_iter_s: 600.4534180164337\n",
      "  time_total_s: 5778.3370015621185\n",
      "  timers:\n",
      "    learn_throughput: 7.167\n",
      "    learn_time_ms: 558115.857\n",
      "    load_throughput: 2261019.38\n",
      "    load_time_ms: 1.769\n",
      "    sample_throughput: 7.695\n",
      "    sample_time_ms: 519837.826\n",
      "    update_time_ms: 10.381\n",
      "  timestamp: 1650148921\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-16 23:45:36 (running for 01:40:04.87)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5774.8 </td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    1.81</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.58</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5772.76</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    1.97</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            287.73</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5778.34</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    1.78</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.69</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-51-58\n",
      "  done: false\n",
      "  episode_len_mean: 280.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.83\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 174\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.4760539632971569\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012754872121057857\n",
      "          policy_loss: -0.04106193048909547\n",
      "          total_loss: -0.024250947693062404\n",
      "          vf_explained_var: 0.4656703108741391\n",
      "          vf_loss: 0.011071290024956788\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.97132352941176\n",
      "    ram_util_percent: 95.26838235294117\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051681324008807296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8551296604917765\n",
      "    mean_inference_ms: 3.601380451222109\n",
      "    mean_raw_obs_processing_ms: 0.22182836520632632\n",
      "  time_since_restore: 6375.652755498886\n",
      "  time_this_iter_s: 602.8913190364838\n",
      "  time_total_s: 6375.652755498886\n",
      "  timers:\n",
      "    learn_throughput: 7.125\n",
      "    learn_time_ms: 561377.478\n",
      "    load_throughput: 5079079.68\n",
      "    load_time_ms: 0.788\n",
      "    sample_throughput: 6.927\n",
      "    sample_time_ms: 577409.761\n",
      "    update_time_ms: 21.841\n",
      "  timestamp: 1650149518\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-52-01\n",
      "  done: false\n",
      "  episode_len_mean: 269.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.8\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 181\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0005859375\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: 0.009294177871197462\n",
      "          total_loss: 0.025972566015577764\n",
      "          vf_explained_var: 0.7852823888742796\n",
      "          vf_loss: 0.016678394139894553\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.948284313725495\n",
      "    ram_util_percent: 95.26764705882353\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052338307235142115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8495891058822784\n",
      "    mean_inference_ms: 3.6107137800070657\n",
      "    mean_raw_obs_processing_ms: 0.22213073094343297\n",
      "  time_since_restore: 6377.940653800964\n",
      "  time_this_iter_s: 603.1415948867798\n",
      "  time_total_s: 6377.940653800964\n",
      "  timers:\n",
      "    learn_throughput: 7.124\n",
      "    learn_time_ms: 561479.412\n",
      "    load_throughput: 4227276.759\n",
      "    load_time_ms: 0.946\n",
      "    sample_throughput: 6.925\n",
      "    sample_time_ms: 577591.979\n",
      "    update_time_ms: 10.452\n",
      "  timestamp: 1650149521\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-16_23-52-04\n",
      "  done: false\n",
      "  episode_len_mean: 274.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.89\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 179\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.003515625000000001\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 9.11055630347873e-05\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.6480847691939575e-08\n",
      "          policy_loss: 0.0036694840437942935\n",
      "          total_loss: 0.03911733712264968\n",
      "          vf_explained_var: 0.46702564301029326\n",
      "          vf_loss: 0.03544785356379405\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.88690330477357\n",
      "    ram_util_percent: 95.2828641370869\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052434082621692475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8538190530382823\n",
      "    mean_inference_ms: 3.593776403409826\n",
      "    mean_raw_obs_processing_ms: 0.22092059113722012\n",
      "  time_since_restore: 6381.31863451004\n",
      "  time_this_iter_s: 602.9816329479218\n",
      "  time_total_s: 6381.31863451004\n",
      "  timers:\n",
      "    learn_throughput: 7.12\n",
      "    learn_time_ms: 561815.085\n",
      "    load_throughput: 3297020.006\n",
      "    load_time_ms: 1.213\n",
      "    sample_throughput: 6.921\n",
      "    sample_time_ms: 577960.159\n",
      "    update_time_ms: 11.503\n",
      "  timestamp: 1650149524\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: f344c_00002\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 278.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.79\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 191\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.503168108466492\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013225323442092223\n",
      "          policy_loss: -0.03822838455008002\n",
      "          total_loss: -0.023094447670123912\n",
      "          vf_explained_var: 0.516315887307608\n",
      "          vf_loss: 0.009182543254592605\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.00528905289053\n",
      "    ram_util_percent: 95.02939729397293\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05189556540498223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8576809514618756\n",
      "    mean_inference_ms: 3.613688780492702\n",
      "    mean_raw_obs_processing_ms: 0.2224177060514981\n",
      "  time_since_restore: 6978.207007646561\n",
      "  time_this_iter_s: 602.5542521476746\n",
      "  time_total_s: 6978.207007646561\n",
      "  timers:\n",
      "    learn_throughput: 7.076\n",
      "    learn_time_ms: 565314.162\n",
      "    load_throughput: 3377736.259\n",
      "    load_time_ms: 1.184\n",
      "    sample_throughput: 6.88\n",
      "    sample_time_ms: 581358.763\n",
      "    update_time_ms: 21.97\n",
      "  timestamp: 1650150121\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-02-03\n",
      "  done: false\n",
      "  episode_len_mean: 269.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.79\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 198\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00029296875\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.004160785146297947\n",
      "          total_loss: 0.002931202411331156\n",
      "          vf_explained_var: 0.6441040840200198\n",
      "          vf_loss: 0.007091983179787114\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.54796547472256\n",
      "    ram_util_percent: 95.02946979038224\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05259274581977803\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8516822564981942\n",
      "    mean_inference_ms: 3.6235278951102443\n",
      "    mean_raw_obs_processing_ms: 0.22271635780203922\n",
      "  time_since_restore: 6979.958307027817\n",
      "  time_this_iter_s: 602.0176532268524\n",
      "  time_total_s: 6979.958307027817\n",
      "  timers:\n",
      "    learn_throughput: 7.076\n",
      "    learn_time_ms: 565270.184\n",
      "    load_throughput: 4107934.673\n",
      "    load_time_ms: 0.974\n",
      "    sample_throughput: 6.877\n",
      "    sample_time_ms: 581633.759\n",
      "    update_time_ms: 10.473\n",
      "  timestamp: 1650150123\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: f344c_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-02-07\n",
      "  done: false\n",
      "  episode_len_mean: 269.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.78\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 196\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0017578125000000005\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 3.958811364872838e-05\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.523702846577519e-06\n",
      "          policy_loss: -0.004512671182953542\n",
      "          total_loss: 0.026732827842684485\n",
      "          vf_explained_var: 0.47241535859723244\n",
      "          vf_loss: 0.031245490156054018\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.262638717632555\n",
      "    ram_util_percent: 95.02959309494452\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05271361447820852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8561799224915686\n",
      "    mean_inference_ms: 3.606522231993908\n",
      "    mean_raw_obs_processing_ms: 0.22208321587865654\n",
      "  time_since_restore: 6983.955768585205\n",
      "  time_this_iter_s: 602.6371340751648\n",
      "  time_total_s: 6983.955768585205\n",
      "  timers:\n",
      "    learn_throughput: 7.072\n",
      "    learn_time_ms: 565645.16\n",
      "    load_throughput: 3168501.605\n",
      "    load_time_ms: 1.262\n",
      "    sample_throughput: 6.875\n",
      "    sample_time_ms: 581856.066\n",
      "    update_time_ms: 11.989\n",
      "  timestamp: 1650150127\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 00:02:16 (running for 01:56:45.08)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         6979.96</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">    1.79</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.46</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         6978.21</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">    1.79</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            278.13</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         6983.96</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">    1.78</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.39</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-12-11\n",
      "  done: false\n",
      "  episode_len_mean: 280.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.83\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 205\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.4899537385631633\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0161272569878618\n",
      "          policy_loss: -0.0460533171071\n",
      "          total_loss: -0.027607345627120104\n",
      "          vf_explained_var: 0.6286137457175921\n",
      "          vf_loss: 0.0111887032714402\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.79525547445255\n",
      "    ram_util_percent: 95.10109489051095\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052127064141568674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8604501100612586\n",
      "    mean_inference_ms: 3.62787693938093\n",
      "    mean_raw_obs_processing_ms: 0.2232850875684718\n",
      "  time_since_restore: 7587.953273773193\n",
      "  time_this_iter_s: 609.7462661266327\n",
      "  time_total_s: 7587.953273773193\n",
      "  timers:\n",
      "    learn_throughput: 7.021\n",
      "    learn_time_ms: 569733.799\n",
      "    load_throughput: 3365539.819\n",
      "    load_time_ms: 1.189\n",
      "    sample_throughput: 6.831\n",
      "    sample_time_ms: 585527.185\n",
      "    update_time_ms: 20.018\n",
      "  timestamp: 1650150731\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-12-11\n",
      "  done: false\n",
      "  episode_len_mean: 268.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.79\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 214\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.000146484375\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.0007559422764085955\n",
      "          total_loss: 0.009047774521894352\n",
      "          vf_explained_var: 0.5231276476895937\n",
      "          vf_loss: 0.009803718699064727\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.65902439024389\n",
      "    ram_util_percent: 95.13085365853657\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0528996116653334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8546042202258337\n",
      "    mean_inference_ms: 3.639948549608572\n",
      "    mean_raw_obs_processing_ms: 0.2235195352266902\n",
      "  time_since_restore: 7588.372501134872\n",
      "  time_this_iter_s: 608.4141941070557\n",
      "  time_total_s: 7588.372501134872\n",
      "  timers:\n",
      "    learn_throughput: 7.023\n",
      "    learn_time_ms: 569595.123\n",
      "    load_throughput: 4043092.346\n",
      "    load_time_ms: 0.989\n",
      "    sample_throughput: 6.83\n",
      "    sample_time_ms: 585637.44\n",
      "    update_time_ms: 10.15\n",
      "  timestamp: 1650150731\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-12-16\n",
      "  done: false\n",
      "  episode_len_mean: 268.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.77\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 213\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0008789062500000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.002737082563411634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0059387623825245765\n",
      "          policy_loss: -0.0027726587178486008\n",
      "          total_loss: 0.03029373178924484\n",
      "          vf_explained_var: 0.44848600606764516\n",
      "          vf_loss: 0.033061172527771805\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.33333333333333\n",
      "    ram_util_percent: 95.07992700729926\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.053077519457377736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8594526898766881\n",
      "    mean_inference_ms: 3.62370004076943\n",
      "    mean_raw_obs_processing_ms: 0.22348077358605237\n",
      "  time_since_restore: 7593.34095454216\n",
      "  time_this_iter_s: 609.385185956955\n",
      "  time_total_s: 7593.34095454216\n",
      "  timers:\n",
      "    learn_throughput: 7.018\n",
      "    learn_time_ms: 569988.558\n",
      "    load_throughput: 3145865.632\n",
      "    load_time_ms: 1.272\n",
      "    sample_throughput: 6.827\n",
      "    sample_time_ms: 585923.347\n",
      "    update_time_ms: 12.401\n",
      "  timestamp: 1650150736\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 00:18:56 (running for 02:13:25.39)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         7588.37</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">    1.79</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            268.53</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         7587.95</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">    1.83</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            280.34</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         7593.34</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">    1.77</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            268.17</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-22-06\n",
      "  done: false\n",
      "  episode_len_mean: 270.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.65\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 223\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5054884787849201\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013847109551193055\n",
      "          policy_loss: -0.03875620976511029\n",
      "          total_loss: -0.025906232078008913\n",
      "          vf_explained_var: 0.20773549515713927\n",
      "          vf_loss: 0.006618778036671981\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.69006211180124\n",
      "    ram_util_percent: 95.34757763975153\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05249210707269171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8648546643761041\n",
      "    mean_inference_ms: 3.6515635746004045\n",
      "    mean_raw_obs_processing_ms: 0.22511335323710813\n",
      "  time_since_restore: 8183.128245830536\n",
      "  time_this_iter_s: 595.1749720573425\n",
      "  time_total_s: 8183.128245830536\n",
      "  timers:\n",
      "    learn_throughput: 6.986\n",
      "    learn_time_ms: 572607.401\n",
      "    load_throughput: 3363920.279\n",
      "    load_time_ms: 1.189\n",
      "    sample_throughput: 6.777\n",
      "    sample_time_ms: 590193.713\n",
      "    update_time_ms: 20.24\n",
      "  timestamp: 1650151326\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-22-07\n",
      "  done: false\n",
      "  episode_len_mean: 272.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.87\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 229\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.32421875e-05\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: 0.00026083380104072635\n",
      "          total_loss: 0.015083307343014385\n",
      "          vf_explained_var: 0.5061583334399807\n",
      "          vf_loss: 0.014822478619809712\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.23217391304348\n",
      "    ram_util_percent: 95.34310559006211\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05325255896574802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8582687224114433\n",
      "    mean_inference_ms: 3.660183978042929\n",
      "    mean_raw_obs_processing_ms: 0.22433389334004655\n",
      "  time_since_restore: 8183.865435361862\n",
      "  time_this_iter_s: 595.4929342269897\n",
      "  time_total_s: 8183.865435361862\n",
      "  timers:\n",
      "    learn_throughput: 6.988\n",
      "    learn_time_ms: 572402.342\n",
      "    load_throughput: 4092303.339\n",
      "    load_time_ms: 0.977\n",
      "    sample_throughput: 6.777\n",
      "    sample_time_ms: 590227.453\n",
      "    update_time_ms: 8.908\n",
      "  timestamp: 1650151327\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-22-11\n",
      "  done: false\n",
      "  episode_len_mean: 264.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.71\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 230\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0008789062500000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.0005581870682115236\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008394532513468415\n",
      "          policy_loss: -0.002813451476795699\n",
      "          total_loss: 0.016543442319505296\n",
      "          vf_explained_var: 0.4974334300205272\n",
      "          vf_loss: 0.019349516763962724\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.18146766169154\n",
      "    ram_util_percent: 95.32338308457712\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.053518345668843885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8634600295531976\n",
      "    mean_inference_ms: 3.646158929820208\n",
      "    mean_raw_obs_processing_ms: 0.22521209706796955\n",
      "  time_since_restore: 8188.209646463394\n",
      "  time_this_iter_s: 594.8686919212341\n",
      "  time_total_s: 8188.209646463394\n",
      "  timers:\n",
      "    learn_throughput: 6.984\n",
      "    learn_time_ms: 572751.249\n",
      "    load_throughput: 3157707.553\n",
      "    load_time_ms: 1.267\n",
      "    sample_throughput: 6.774\n",
      "    sample_time_ms: 590474.787\n",
      "    update_time_ms: 12.428\n",
      "  timestamp: 1650151331\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: f344c_00002\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-31-29\n",
      "  done: false\n",
      "  episode_len_mean: 274.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.67\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 237\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.4392035155527053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009662289966605326\n",
      "          policy_loss: -0.03881994413151856\n",
      "          total_loss: -0.02528531744798285\n",
      "          vf_explained_var: 0.6416018602027688\n",
      "          vf_loss: 0.009186596987986191\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.318276762402085\n",
      "    ram_util_percent: 95.37310704960836\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052697448529719484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8673274464015113\n",
      "    mean_inference_ms: 3.6656260313193356\n",
      "    mean_raw_obs_processing_ms: 0.2261095624920366\n",
      "  time_since_restore: 8745.823660850525\n",
      "  time_this_iter_s: 562.695415019989\n",
      "  time_total_s: 8745.823660850525\n",
      "  timers:\n",
      "    learn_throughput: 6.993\n",
      "    learn_time_ms: 572034.762\n",
      "    load_throughput: 3289779.207\n",
      "    load_time_ms: 1.216\n",
      "    sample_throughput: 6.745\n",
      "    sample_time_ms: 593004.59\n",
      "    update_time_ms: 21.55\n",
      "  timestamp: 1650151889\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-31-29\n",
      "  done: false\n",
      "  episode_len_mean: 272.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.87\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 247\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.662109375e-05\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: 0.001214192759606146\n",
      "          total_loss: 0.00446993944145018\n",
      "          vf_explained_var: 0.6644293978009173\n",
      "          vf_loss: 0.003255742506790025\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.675979112271534\n",
      "    ram_util_percent: 95.37127937336814\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0535545463945884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8615835101082555\n",
      "    mean_inference_ms: 3.678497151063062\n",
      "    mean_raw_obs_processing_ms: 0.2250539287319733\n",
      "  time_since_restore: 8746.279307365417\n",
      "  time_this_iter_s: 562.4138720035553\n",
      "  time_total_s: 8746.279307365417\n",
      "  timers:\n",
      "    learn_throughput: 6.996\n",
      "    learn_time_ms: 571750.159\n",
      "    load_throughput: 4110350.099\n",
      "    load_time_ms: 0.973\n",
      "    sample_throughput: 6.746\n",
      "    sample_time_ms: 592970.606\n",
      "    update_time_ms: 9.837\n",
      "  timestamp: 1650151889\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: f344c_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-31-34\n",
      "  done: false\n",
      "  episode_len_mean: 266.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.75\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 245\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0008789062500000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 2.016890777832286e-06\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -8.099837353199629e-10\n",
      "          policy_loss: 1.9359632685620296e-06\n",
      "          total_loss: 0.02195901231519798\n",
      "          vf_explained_var: 0.6780290960624654\n",
      "          vf_loss: 0.021957077587684316\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.22679738562091\n",
      "    ram_util_percent: 95.38444444444445\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.053798928553583016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8660134626368506\n",
      "    mean_inference_ms: 3.660391522308264\n",
      "    mean_raw_obs_processing_ms: 0.22618556749625018\n",
      "  time_since_restore: 8750.774177312851\n",
      "  time_this_iter_s: 562.5645308494568\n",
      "  time_total_s: 8750.774177312851\n",
      "  timers:\n",
      "    learn_throughput: 6.992\n",
      "    learn_time_ms: 572087.141\n",
      "    load_throughput: 5011564.955\n",
      "    load_time_ms: 0.798\n",
      "    sample_throughput: 6.743\n",
      "    sample_time_ms: 593173.948\n",
      "    update_time_ms: 13.72\n",
      "  timestamp: 1650151894\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 00:35:37 (running for 02:30:05.96)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         8746.28</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">    1.87</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            272.16</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         8745.82</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">    1.67</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            274.79</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         8750.77</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">    1.75</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            266.31</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-40-50\n",
      "  done: false\n",
      "  episode_len_mean: 269.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.83\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 264\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8310546875e-05\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -7.937761444237924e-05\n",
      "          total_loss: 0.0011812520363638477\n",
      "          vf_explained_var: 0.6085876039279404\n",
      "          vf_loss: 0.0012606279068921884\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.9921568627451\n",
      "    ram_util_percent: 95.30013071895425\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.053731631621219994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8638028143854106\n",
      "    mean_inference_ms: 3.6907467116371366\n",
      "    mean_raw_obs_processing_ms: 0.22564791598631304\n",
      "  time_since_restore: 9307.386718511581\n",
      "  time_this_iter_s: 561.1074111461639\n",
      "  time_total_s: 9307.386718511581\n",
      "  timers:\n",
      "    learn_throughput: 7.005\n",
      "    learn_time_ms: 571047.754\n",
      "    load_throughput: 4122976.506\n",
      "    load_time_ms: 0.97\n",
      "    sample_throughput: 6.754\n",
      "    sample_time_ms: 592262.242\n",
      "    update_time_ms: 12.053\n",
      "  timestamp: 1650152450\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-40-50\n",
      "  done: false\n",
      "  episode_len_mean: 277.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.69\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 253\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.4657853505944693\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011895000370521856\n",
      "          policy_loss: -0.0356111592312734\n",
      "          total_loss: -0.021532031061298022\n",
      "          vf_explained_var: 0.580913386614092\n",
      "          vf_loss: 0.00872637732761852\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.772026143790846\n",
      "    ram_util_percent: 95.298431372549\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052866480715030306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8694236639916778\n",
      "    mean_inference_ms: 3.677994241573952\n",
      "    mean_raw_obs_processing_ms: 0.22697709251244963\n",
      "  time_since_restore: 9307.41218161583\n",
      "  time_this_iter_s: 561.5885207653046\n",
      "  time_total_s: 9307.41218161583\n",
      "  timers:\n",
      "    learn_throughput: 7.001\n",
      "    learn_time_ms: 571377.42\n",
      "    load_throughput: 3310748.101\n",
      "    load_time_ms: 1.208\n",
      "    sample_throughput: 6.752\n",
      "    sample_time_ms: 592389.549\n",
      "    update_time_ms: 21.719\n",
      "  timestamp: 1650152450\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-40-56\n",
      "  done: false\n",
      "  episode_len_mean: 268.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.79\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 262\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0004394531250000001\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 9.860598451481447e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -8.940906359581416e-11\n",
      "          policy_loss: 0.00017401227547276404\n",
      "          total_loss: 0.019506099326435915\n",
      "          vf_explained_var: 0.6278008619944254\n",
      "          vf_loss: 0.01933208897319292\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.57154046997388\n",
      "    ram_util_percent: 95.28485639686686\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.054029462735371306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8682490830825287\n",
      "    mean_inference_ms: 3.6724494741349827\n",
      "    mean_raw_obs_processing_ms: 0.2270566666624493\n",
      "  time_since_restore: 9312.966797351837\n",
      "  time_this_iter_s: 562.1926200389862\n",
      "  time_total_s: 9312.966797351837\n",
      "  timers:\n",
      "    learn_throughput: 6.999\n",
      "    learn_time_ms: 571495.563\n",
      "    load_throughput: 4814951.211\n",
      "    load_time_ms: 0.831\n",
      "    sample_throughput: 6.752\n",
      "    sample_time_ms: 592447.92\n",
      "    update_time_ms: 13.67\n",
      "  timestamp: 1650152456\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: f344c_00002\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-50-13\n",
      "  done: false\n",
      "  episode_len_mean: 267.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.77\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 281\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.1552734375e-06\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.001472592624204774\n",
      "          total_loss: 0.0012802251494459568\n",
      "          vf_explained_var: 0.6690461092738695\n",
      "          vf_loss: 0.0027528193065463513\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.255947712418305\n",
      "    ram_util_percent: 95.36575163398692\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05383397539696367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8652406586478223\n",
      "    mean_inference_ms: 3.699359038914557\n",
      "    mean_raw_obs_processing_ms: 0.2261849638239796\n",
      "  time_since_restore: 9870.455260515213\n",
      "  time_this_iter_s: 563.0685420036316\n",
      "  time_total_s: 9870.455260515213\n",
      "  timers:\n",
      "    learn_throughput: 7.008\n",
      "    learn_time_ms: 570776.466\n",
      "    load_throughput: 4142931.648\n",
      "    load_time_ms: 0.965\n",
      "    sample_throughput: 6.762\n",
      "    sample_time_ms: 591525.713\n",
      "    update_time_ms: 13.063\n",
      "  timestamp: 1650153013\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-50-14\n",
      "  done: false\n",
      "  episode_len_mean: 288.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.89\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 266\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.4587693037204845\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011185192226551363\n",
      "          policy_loss: -0.0351447509681826\n",
      "          total_loss: -0.024903169311633114\n",
      "          vf_explained_var: 0.797405065836445\n",
      "          vf_loss: 0.005208243978408827\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.16245110821383\n",
      "    ram_util_percent: 95.35045632333767\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05295023645776533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8705355942068056\n",
      "    mean_inference_ms: 3.6852170959254833\n",
      "    mean_raw_obs_processing_ms: 0.22734006964013395\n",
      "  time_since_restore: 9870.570727586746\n",
      "  time_this_iter_s: 563.1585459709167\n",
      "  time_total_s: 9870.570727586746\n",
      "  timers:\n",
      "    learn_throughput: 7.004\n",
      "    learn_time_ms: 571085.915\n",
      "    load_throughput: 3278144.551\n",
      "    load_time_ms: 1.22\n",
      "    sample_throughput: 6.76\n",
      "    sample_time_ms: 591745.729\n",
      "    update_time_ms: 20.508\n",
      "  timestamp: 1650153014\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: f344c_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-50-19\n",
      "  done: false\n",
      "  episode_len_mean: 266.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.74\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 279\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00021972656250000006\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 1.0515211744544308e-06\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.0521101805176275e-10\n",
      "          policy_loss: 0.002013384574843991\n",
      "          total_loss: 0.01636524185267908\n",
      "          vf_explained_var: 0.5720271580321814\n",
      "          vf_loss: 0.014351855049670865\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.15430809399477\n",
      "    ram_util_percent: 95.3771540469974\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05417359286014323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8698091997355586\n",
      "    mean_inference_ms: 3.680905994556275\n",
      "    mean_raw_obs_processing_ms: 0.2278405842169776\n",
      "  time_since_restore: 9876.092497348785\n",
      "  time_this_iter_s: 563.1256999969482\n",
      "  time_total_s: 9876.092497348785\n",
      "  timers:\n",
      "    learn_throughput: 7.004\n",
      "    learn_time_ms: 571133.857\n",
      "    load_throughput: 4854658.989\n",
      "    load_time_ms: 0.824\n",
      "    sample_throughput: 6.758\n",
      "    sample_time_ms: 591865.374\n",
      "    update_time_ms: 14.815\n",
      "  timestamp: 1650153019\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 00:52:18 (running for 02:46:46.69)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         9870.46</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">    1.77</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            267.6 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         9870.57</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">    1.89</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            288.81</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         9876.09</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">    1.74</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            266.72</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 268.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.79\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 298\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.57763671875e-06\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.0008001737296581268\n",
      "          total_loss: 0.0020156282741296033\n",
      "          vf_explained_var: 0.41024860278252634\n",
      "          vf_loss: 0.0028157989059822753\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.803267973856215\n",
      "    ram_util_percent: 95.38313725490197\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05382196471046106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8658442609036741\n",
      "    mean_inference_ms: 3.701762532096826\n",
      "    mean_raw_obs_processing_ms: 0.22628995765653623\n",
      "  time_since_restore: 10432.87311553955\n",
      "  time_this_iter_s: 562.4178550243378\n",
      "  time_total_s: 10432.87311553955\n",
      "  timers:\n",
      "    learn_throughput: 7.054\n",
      "    learn_time_ms: 567028.146\n",
      "    load_throughput: 5290327.626\n",
      "    load_time_ms: 0.756\n",
      "    sample_throughput: 6.766\n",
      "    sample_time_ms: 591169.241\n",
      "    update_time_ms: 13.217\n",
      "  timestamp: 1650153576\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 292.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.93\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 280\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.44951868443399345\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011628376768556837\n",
      "          policy_loss: -0.03209158806681072\n",
      "          total_loss: -0.02319465105231571\n",
      "          vf_explained_var: 0.7908036417858575\n",
      "          vf_loss: 0.003664169340176628\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.70875816993464\n",
      "    ram_util_percent: 95.3711111111111\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05298298236388768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8710952079422427\n",
      "    mean_inference_ms: 3.689692388872872\n",
      "    mean_raw_obs_processing_ms: 0.22742802949067767\n",
      "  time_since_restore: 10433.3116106987\n",
      "  time_this_iter_s: 562.7408831119537\n",
      "  time_total_s: 10433.3116106987\n",
      "  timers:\n",
      "    learn_throughput: 7.05\n",
      "    learn_time_ms: 567398.231\n",
      "    load_throughput: 3284883.894\n",
      "    load_time_ms: 1.218\n",
      "    sample_throughput: 6.763\n",
      "    sample_time_ms: 591456.385\n",
      "    update_time_ms: 17.844\n",
      "  timestamp: 1650153576\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_00-59-41\n",
      "  done: false\n",
      "  episode_len_mean: 267.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.77\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 296\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00010986328125000003\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 4.0880791525597165e-06\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.3354504500275694e-11\n",
      "          policy_loss: -0.0001373312906712614\n",
      "          total_loss: 0.021379709959290522\n",
      "          vf_explained_var: 0.7348021000944158\n",
      "          vf_loss: 0.0215170344630737\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.69529411764706\n",
      "    ram_util_percent: 95.36915032679738\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0542096242911585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8703010309931324\n",
      "    mean_inference_ms: 3.6832797998942306\n",
      "    mean_raw_obs_processing_ms: 0.22830255491013263\n",
      "  time_since_restore: 10438.065180063248\n",
      "  time_this_iter_s: 561.9726827144623\n",
      "  time_total_s: 10438.065180063248\n",
      "  timers:\n",
      "    learn_throughput: 7.05\n",
      "    learn_time_ms: 567345.311\n",
      "    load_throughput: 4916112.169\n",
      "    load_time_ms: 0.814\n",
      "    sample_throughput: 6.762\n",
      "    sample_time_ms: 591505.894\n",
      "    update_time_ms: 15.372\n",
      "  timestamp: 1650153581\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 01:08:58 (running for 03:03:26.84)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 PENDING, 3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>RUNNING </td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         10432.9</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">    1.79</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            268.05</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>RUNNING </td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         10433.3</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">    1.93</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            292.54</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>RUNNING </td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         10438.1</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">    1.77</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            267.65</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>PENDING </td><td>              </td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-09-12\n",
      "  done: false\n",
      "  episode_len_mean: 266.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.73\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 315\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.288818359375e-06\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: 0.008141640723953325\n",
      "          total_loss: 0.01180895526983565\n",
      "          vf_explained_var: 0.47800556901962526\n",
      "          vf_loss: 0.00366731065150816\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.76261203585146\n",
      "    ram_util_percent: 95.56747759282972\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.053724815255677016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.865400677354542\n",
      "    mean_inference_ms: 3.698838472937939\n",
      "    mean_raw_obs_processing_ms: 0.2261845606489336\n",
      "  time_since_restore: 11009.05542063713\n",
      "  time_this_iter_s: 576.18230509758\n",
      "  time_total_s: 11009.05542063713\n",
      "  timers:\n",
      "    learn_throughput: 7.102\n",
      "    learn_time_ms: 563236.91\n",
      "    load_throughput: 5381452.399\n",
      "    load_time_ms: 0.743\n",
      "    sample_throughput: 6.811\n",
      "    sample_time_ms: 587270.976\n",
      "    update_time_ms: 19.509\n",
      "  timestamp: 1650154152\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: f344c_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-09-13\n",
      "  done: false\n",
      "  episode_len_mean: 300.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 2.04\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 294\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.46923430773519703\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013207852835660908\n",
      "          policy_loss: -0.04844830771232204\n",
      "          total_loss: -0.041052197673050086\n",
      "          vf_explained_var: 0.6088923000520275\n",
      "          vf_loss: 0.0014525757148368685\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.923943661971826\n",
      "    ram_util_percent: 95.5562099871959\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05295091058747211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8709282093734032\n",
      "    mean_inference_ms: 3.690270488496354\n",
      "    mean_raw_obs_processing_ms: 0.22723828979390304\n",
      "  time_since_restore: 11009.950615644455\n",
      "  time_this_iter_s: 576.639004945755\n",
      "  time_total_s: 11009.950615644455\n",
      "  timers:\n",
      "    learn_throughput: 7.097\n",
      "    learn_time_ms: 563579.946\n",
      "    load_throughput: 3313952.515\n",
      "    load_time_ms: 1.207\n",
      "    sample_throughput: 6.807\n",
      "    sample_time_ms: 587632.919\n",
      "    update_time_ms: 16.309\n",
      "  timestamp: 1650154153\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: f344c_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-09-18\n",
      "  done: false\n",
      "  episode_len_mean: 269.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.79\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 313\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.4931640625000015e-05\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 1.2878410309036964e-06\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.9631345352631167e-10\n",
      "          policy_loss: -0.0005421876426666013\n",
      "          total_loss: 0.01200754801312121\n",
      "          vf_explained_var: 0.7705133980320346\n",
      "          vf_loss: 0.012549730656187861\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.31340996168583\n",
      "    ram_util_percent: 95.56245210727968\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05414365387063823\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8697762774267249\n",
      "    mean_inference_ms: 3.6799670324680807\n",
      "    mean_raw_obs_processing_ms: 0.22852614287074008\n",
      "  time_since_restore: 11015.394224405289\n",
      "  time_this_iter_s: 577.329044342041\n",
      "  time_total_s: 11015.394224405289\n",
      "  timers:\n",
      "    learn_throughput: 7.098\n",
      "    learn_time_ms: 563537.134\n",
      "    load_throughput: 4868888.502\n",
      "    load_time_ms: 0.822\n",
      "    sample_throughput: 6.808\n",
      "    sample_time_ms: 587573.162\n",
      "    update_time_ms: 15.182\n",
      "  timestamp: 1650154158\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: f344c_00002\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-18-36\n",
      "  done: true\n",
      "  episode_len_mean: 265.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.72\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 331\n",
      "  experiment_id: ce7968635bf3477eae29603782a5d12a\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1444091796875e-06\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0\n",
      "          policy_loss: -0.004196268115793505\n",
      "          total_loss: -0.00048263307300306133\n",
      "          vf_explained_var: 0.6699409773272853\n",
      "          vf_loss: 0.0037136291285947975\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.490402075226974\n",
      "    ram_util_percent: 95.30103761348899\n",
      "  pid: 1292\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.053597574511619527\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.864380748286641\n",
      "    mean_inference_ms: 3.693017116600115\n",
      "    mean_raw_obs_processing_ms: 0.22601411986410916\n",
      "  time_since_restore: 11573.113706827164\n",
      "  time_this_iter_s: 564.058286190033\n",
      "  time_total_s: 11573.113706827164\n",
      "  timers:\n",
      "    learn_throughput: 7.148\n",
      "    learn_time_ms: 559574.231\n",
      "    load_throughput: 5465425.286\n",
      "    load_time_ms: 0.732\n",
      "    sample_throughput: 6.855\n",
      "    sample_time_ms: 583513.771\n",
      "    update_time_ms: 19.858\n",
      "  timestamp: 1650154716\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: f344c_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=1292)\u001b[0m 2022-04-17 01:18:36,726\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00001:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-18-37\n",
      "  done: true\n",
      "  episode_len_mean: 301.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 2.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 310\n",
      "  experiment_id: c0ac0c3a0c054f709df65accf82842fb\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.49059651413912414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016120589995970318\n",
      "          policy_loss: -0.047291200800288105\n",
      "          total_loss: -0.037515818357707995\n",
      "          vf_explained_var: 0.5630609376456148\n",
      "          vf_loss: 0.0025211180469693405\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.75727272727273\n",
      "    ram_util_percent: 95.3087012987013\n",
      "  pid: 1291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0528694780557625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8701618932165828\n",
      "    mean_inference_ms: 3.68726710811473\n",
      "    mean_raw_obs_processing_ms: 0.22684703463490302\n",
      "  time_since_restore: 11573.401009559631\n",
      "  time_this_iter_s: 563.4503939151764\n",
      "  time_total_s: 11573.401009559631\n",
      "  timers:\n",
      "    learn_throughput: 7.145\n",
      "    learn_time_ms: 559841.72\n",
      "    load_throughput: 2341584.111\n",
      "    load_time_ms: 1.708\n",
      "    sample_throughput: 6.851\n",
      "    sample_time_ms: 583830.747\n",
      "    update_time_ms: 14.261\n",
      "  timestamp: 1650154717\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: f344c_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=1291)\u001b[0m 2022-04-17 01:18:37,169\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00002:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-18-41\n",
      "  done: true\n",
      "  episode_len_mean: 269.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.8\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 330\n",
      "  experiment_id: 39af55ef22764cddae11efeec83f3da7\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.7465820312500007e-05\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 1.473657292698986e-06\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.379333371888142e-09\n",
      "          policy_loss: -0.025537107023660854\n",
      "          total_loss: -0.014157315491828868\n",
      "          vf_explained_var: 0.38984249221381323\n",
      "          vf_loss: 0.011379789195013463\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.2212987012987\n",
      "    ram_util_percent: 95.2935064935065\n",
      "  pid: 1295\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05402102651825702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8686751196841885\n",
      "    mean_inference_ms: 3.6730568704694497\n",
      "    mean_raw_obs_processing_ms: 0.22844185717512067\n",
      "  time_since_restore: 11578.157482624054\n",
      "  time_this_iter_s: 562.7632582187653\n",
      "  time_total_s: 11578.157482624054\n",
      "  timers:\n",
      "    learn_throughput: 7.146\n",
      "    learn_time_ms: 559757.204\n",
      "    load_throughput: 4781195.782\n",
      "    load_time_ms: 0.837\n",
      "    sample_throughput: 6.852\n",
      "    sample_time_ms: 583778.217\n",
      "    update_time_ms: 14.753\n",
      "  timestamp: 1650154721\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: f344c_00002\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=1295)\u001b[0m 2022-04-17 01:18:41,752\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1289)\u001b[0m 2022-04-17 01:18:47,316\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1289)\u001b[0m 2022-04-17 01:18:47,316\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=68322)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=68322)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=68322)\u001b[0m [W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=1289)\u001b[0m [W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 01:25:39 (running for 03:20:07.82)<br>Memory usage on this node: 7.5/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         11573.1</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.72</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            265.24</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         11573.4</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    2.06</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            301.31</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         11578.2</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.8 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-27-04\n",
      "  done: false\n",
      "  episode_len_mean: 285.8666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 15\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.379115408232352\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.1060263405396244\n",
      "          policy_loss: -9.609105125550301e-05\n",
      "          total_loss: 30.819202728681667\n",
      "          vf_explained_var: -0.24978321354876282\n",
      "          vf_loss: 30.79809363375428\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.60925110132159\n",
      "    ram_util_percent: 93.65844346549193\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.035530327260866676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.731585950024335\n",
      "    mean_inference_ms: 2.5645628835939815\n",
      "    mean_raw_obs_processing_ms: 0.15833991016634877\n",
      "  time_since_restore: 491.37337279319763\n",
      "  time_this_iter_s: 491.37337279319763\n",
      "  time_total_s: 491.37337279319763\n",
      "  timers:\n",
      "    learn_throughput: 8.386\n",
      "    learn_time_ms: 476957.364\n",
      "    load_throughput: 5594270.09\n",
      "    load_time_ms: 0.715\n",
      "    sample_throughput: 277.615\n",
      "    sample_time_ms: 14408.432\n",
      "    update_time_ms: 6.186\n",
      "  timestamp: 1650155224\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-35-17\n",
      "  done: false\n",
      "  episode_len_mean: 285.61290322580646\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.967741935483871\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 31\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.5503644812010949\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04663677793670965\n",
      "          policy_loss: -0.0006809886547446411\n",
      "          total_loss: 1.852205825310641\n",
      "          vf_explained_var: -0.1761339811227655\n",
      "          vf_loss: 1.8388957876311516\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.705563689604688\n",
      "    ram_util_percent: 94.33440702781844\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.035535018521272076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7306056568149206\n",
      "    mean_inference_ms: 2.5610081155555\n",
      "    mean_raw_obs_processing_ms: 0.15937584208051167\n",
      "  time_since_restore: 984.1004569530487\n",
      "  time_this_iter_s: 492.7270841598511\n",
      "  time_total_s: 984.1004569530487\n",
      "  timers:\n",
      "    learn_throughput: 8.373\n",
      "    learn_time_ms: 477738.44\n",
      "    load_throughput: 6046933.141\n",
      "    load_time_ms: 0.661\n",
      "    sample_throughput: 15.823\n",
      "    sample_time_ms: 252799.626\n",
      "    update_time_ms: 5.301\n",
      "  timestamp: 1650155717\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 01:42:19 (running for 03:36:47.88)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">           984.1</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\"> 1.96774</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           285.613</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         11573.1</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 1.72   </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           265.24 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         11573.4</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 2.06   </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           301.31 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         11578.2</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 1.8    </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           269.97 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-43-30\n",
      "  done: false\n",
      "  episode_len_mean: 285.17391304347825\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.9565217391304348\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 46\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.5451796177253928\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019976579607528428\n",
      "          policy_loss: -0.0008169968702620076\n",
      "          total_loss: 0.42318711617525906\n",
      "          vf_explained_var: -0.03916778160679725\n",
      "          vf_loss: 0.41501464912848124\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.933040935672516\n",
      "    ram_util_percent: 94.33669590643274\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.035710215088398216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7312922766699551\n",
      "    mean_inference_ms: 2.565502840545662\n",
      "    mean_raw_obs_processing_ms: 0.16059615032288713\n",
      "  time_since_restore: 1477.692322731018\n",
      "  time_this_iter_s: 493.59186577796936\n",
      "  time_total_s: 1477.692322731018\n",
      "  timers:\n",
      "    learn_throughput: 8.365\n",
      "    learn_time_ms: 478200.782\n",
      "    load_throughput: 6023414.074\n",
      "    load_time_ms: 0.664\n",
      "    sample_throughput: 12.017\n",
      "    sample_time_ms: 332866.338\n",
      "    update_time_ms: 5.09\n",
      "  timestamp: 1650156210\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-51-44\n",
      "  done: false\n",
      "  episode_len_mean: 289.21311475409834\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.0327868852459017\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 61\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.5283211994235234\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01988533576007614\n",
      "          policy_loss: 0.0012015280183604968\n",
      "          total_loss: 0.1356251372745441\n",
      "          vf_explained_var: 0.17967723422153023\n",
      "          vf_loss: 0.12547520714141028\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.50964912280702\n",
      "    ram_util_percent: 94.16198830409357\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.035866514541015415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7321494629545365\n",
      "    mean_inference_ms: 2.569864676269668\n",
      "    mean_raw_obs_processing_ms: 0.16082773726863364\n",
      "  time_since_restore: 1971.3927307128906\n",
      "  time_this_iter_s: 493.70040798187256\n",
      "  time_total_s: 1971.3927307128906\n",
      "  timers:\n",
      "    learn_throughput: 8.36\n",
      "    learn_time_ms: 478474.371\n",
      "    load_throughput: 6328039.981\n",
      "    load_time_ms: 0.632\n",
      "    sample_throughput: 10.723\n",
      "    sample_time_ms: 373045.262\n",
      "    update_time_ms: 4.772\n",
      "  timestamp: 1650156704\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 01:58:59 (running for 03:53:27.98)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1971.39</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> 2.03279</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           289.213</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.1 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 1.72   </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           265.24 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.4 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 2.06   </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           301.31 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11578.2 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 1.8    </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           269.97 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_01-59-56\n",
      "  done: false\n",
      "  episode_len_mean: 288.4868421052632\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.013157894736842\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 76\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.5266530774293408\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01483079296248865\n",
      "          policy_loss: -0.0030524143828980386\n",
      "          total_loss: 0.12385526427045285\n",
      "          vf_explained_var: 0.27869206122172774\n",
      "          vf_loss: 0.120233821167901\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.087812041116006\n",
      "    ram_util_percent: 94.28414096916299\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03603316610512266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7326758947862722\n",
      "    mean_inference_ms: 2.573056515710579\n",
      "    mean_raw_obs_processing_ms: 0.16107656560356004\n",
      "  time_since_restore: 2463.508248567581\n",
      "  time_this_iter_s: 492.11551785469055\n",
      "  time_total_s: 2463.508248567581\n",
      "  timers:\n",
      "    learn_throughput: 8.362\n",
      "    learn_time_ms: 478334.486\n",
      "    load_throughput: 6566939.095\n",
      "    load_time_ms: 0.609\n",
      "    sample_throughput: 10.071\n",
      "    sample_time_ms: 397167.142\n",
      "    update_time_ms: 4.894\n",
      "  timestamp: 1650157196\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_02-08-07\n",
      "  done: false\n",
      "  episode_len_mean: 291.2637362637363\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.065934065934066\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 91\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.4840013247504029\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015434076649340807\n",
      "          policy_loss: 0.0007834296403152328\n",
      "          total_loss: 0.10456453977423089\n",
      "          vf_explained_var: 0.5902887412296829\n",
      "          vf_loss: 0.09683577621015169\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.55029411764706\n",
      "    ram_util_percent: 94.27264705882352\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036174565029075983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7331350843541032\n",
      "    mean_inference_ms: 2.575566156660564\n",
      "    mean_raw_obs_processing_ms: 0.16113021801829275\n",
      "  time_since_restore: 2954.572364807129\n",
      "  time_this_iter_s: 491.06411623954773\n",
      "  time_total_s: 2954.572364807129\n",
      "  timers:\n",
      "    learn_throughput: 8.367\n",
      "    learn_time_ms: 478057.286\n",
      "    load_throughput: 6709991.734\n",
      "    load_time_ms: 0.596\n",
      "    sample_throughput: 9.685\n",
      "    sample_time_ms: 413003.207\n",
      "    update_time_ms: 4.684\n",
      "  timestamp: 1650157687\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 02:15:40 (running for 04:10:08.89)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         2954.57</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> 2.06593</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           291.264</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.1 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 1.72   </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           265.24 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.4 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 2.06   </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           301.31 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11578.2 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 1.8    </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           269.97 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_02-16-20\n",
      "  done: false\n",
      "  episode_len_mean: 297.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.19\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 105\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.49062545743360314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01396660071173928\n",
      "          policy_loss: -0.0055859532485145235\n",
      "          total_loss: 0.07548921664260448\n",
      "          vf_explained_var: 0.6776008386124847\n",
      "          vf_loss: 0.07479019905791007\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.325920471281297\n",
      "    ram_util_percent: 94.52842415316641\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036322330502275554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7336399368106129\n",
      "    mean_inference_ms: 2.578094848681323\n",
      "    mean_raw_obs_processing_ms: 0.16102780664080157\n",
      "  time_since_restore: 3446.6347510814667\n",
      "  time_this_iter_s: 492.06238627433777\n",
      "  time_total_s: 3446.6347510814667\n",
      "  timers:\n",
      "    learn_throughput: 8.368\n",
      "    learn_time_ms: 478008.07\n",
      "    load_throughput: 6917624.551\n",
      "    load_time_ms: 0.578\n",
      "    sample_throughput: 9.431\n",
      "    sample_time_ms: 424150.977\n",
      "    update_time_ms: 4.745\n",
      "  timestamp: 1650158180\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_02-24-33\n",
      "  done: false\n",
      "  episode_len_mean: 296.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.17\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 120\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.5048607091589641\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020080696725217607\n",
      "          policy_loss: -0.008290007075555222\n",
      "          total_loss: 0.07857654711409341\n",
      "          vf_explained_var: 0.35019981226613445\n",
      "          vf_loss: 0.07783023947349159\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.581818181818186\n",
      "    ram_util_percent: 94.49428152492669\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03655808429286367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7344443700432324\n",
      "    mean_inference_ms: 2.582240502239259\n",
      "    mean_raw_obs_processing_ms: 0.1609902961486012\n",
      "  time_since_restore: 3940.5488998889923\n",
      "  time_this_iter_s: 493.91414880752563\n",
      "  time_total_s: 3940.5488998889923\n",
      "  timers:\n",
      "    learn_throughput: 8.365\n",
      "    learn_time_ms: 478206.009\n",
      "    load_throughput: 7074143.678\n",
      "    load_time_ms: 0.565\n",
      "    sample_throughput: 9.245\n",
      "    sample_time_ms: 432645.129\n",
      "    update_time_ms: 4.731\n",
      "  timestamp: 1650158673\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 02:32:20 (running for 04:26:49.03)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         3940.55</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">    2.17</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            296.33</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.1 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.72</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            265.24</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.4 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    2.06</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            301.31</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11578.2 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.8 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_02-32-49\n",
      "  done: false\n",
      "  episode_len_mean: 300.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.25\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 133\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.4711443171706251\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008532864542119593\n",
      "          policy_loss: -0.01818322605122962\n",
      "          total_loss: 0.051975210581190125\n",
      "          vf_explained_var: 0.7652885383816176\n",
      "          vf_loss: 0.06439875364403731\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.715328467153284\n",
      "    ram_util_percent: 94.51824817518248\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677439070091749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7352925608217014\n",
      "    mean_inference_ms: 2.5864116854189514\n",
      "    mean_raw_obs_processing_ms: 0.1606526916802375\n",
      "  time_since_restore: 4436.549961090088\n",
      "  time_this_iter_s: 496.0010612010956\n",
      "  time_total_s: 4436.549961090088\n",
      "  timers:\n",
      "    learn_throughput: 8.358\n",
      "    learn_time_ms: 478580.429\n",
      "    load_throughput: 7152090.944\n",
      "    load_time_ms: 0.559\n",
      "    sample_throughput: 9.102\n",
      "    sample_time_ms: 439467.181\n",
      "    update_time_ms: 4.637\n",
      "  timestamp: 1650159169\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_02-41-04\n",
      "  done: false\n",
      "  episode_len_mean: 302.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 2.33\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 148\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.48877922697092896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014531790987928329\n",
      "          policy_loss: 0.002810547083255745\n",
      "          total_loss: 0.07171913634219597\n",
      "          vf_explained_var: 0.3384556478710585\n",
      "          vf_loss: 0.059099630419645575\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.03426061493412\n",
      "    ram_util_percent: 94.44582723279649\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03697415107402183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7359548213957082\n",
      "    mean_inference_ms: 2.5892845580704242\n",
      "    mean_raw_obs_processing_ms: 0.16004395844369323\n",
      "  time_since_restore: 4931.172266960144\n",
      "  time_this_iter_s: 494.62230587005615\n",
      "  time_total_s: 4931.172266960144\n",
      "  timers:\n",
      "    learn_throughput: 8.355\n",
      "    learn_time_ms: 478738.613\n",
      "    load_throughput: 7194963.547\n",
      "    load_time_ms: 0.556\n",
      "    sample_throughput: 8.986\n",
      "    sample_time_ms: 445126.785\n",
      "    update_time_ms: 4.525\n",
      "  timestamp: 1650159664\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 02:49:01 (running for 04:43:29.86)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         4931.17</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    2.33</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            302.75</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.1 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.72</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            265.24</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.4 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    2.06</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            301.31</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11578.2 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.8 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_02-49-17\n",
      "  done: false\n",
      "  episode_len_mean: 303.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 2.33\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 163\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.48154122701255225\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007918996331198443\n",
      "          policy_loss: -0.0019187463457465814\n",
      "          total_loss: 0.04908023008861409\n",
      "          vf_explained_var: 0.6381293322450371\n",
      "          vf_loss: 0.0456536543660707\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.697205882352943\n",
      "    ram_util_percent: 94.50529411764707\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037133013851561446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7363502815232836\n",
      "    mean_inference_ms: 2.591218014262404\n",
      "    mean_raw_obs_processing_ms: 0.1595793258449976\n",
      "  time_since_restore: 5424.240122079849\n",
      "  time_this_iter_s: 493.0678551197052\n",
      "  time_total_s: 5424.240122079849\n",
      "  timers:\n",
      "    learn_throughput: 8.352\n",
      "    learn_time_ms: 478902.886\n",
      "    load_throughput: 7385963.46\n",
      "    load_time_ms: 0.542\n",
      "    sample_throughput: 8.111\n",
      "    sample_time_ms: 493151.808\n",
      "    update_time_ms: 4.281\n",
      "  timestamp: 1650160157\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_02-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 298.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 2.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 179\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.48439075917966906\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007947636180371975\n",
      "          policy_loss: -0.010032763025192643\n",
      "          total_loss: 0.03406432817917898\n",
      "          vf_explained_var: 0.4547559060717142\n",
      "          vf_loss: 0.03873243684409767\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.70441176470588\n",
      "    ram_util_percent: 94.66588235294118\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03725510016609374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7368130771261672\n",
      "    mean_inference_ms: 2.5931465676341277\n",
      "    mean_raw_obs_processing_ms: 0.15915267599126126\n",
      "  time_since_restore: 5916.327229261398\n",
      "  time_this_iter_s: 492.0871071815491\n",
      "  time_total_s: 5916.327229261398\n",
      "  timers:\n",
      "    learn_throughput: 8.354\n",
      "    learn_time_ms: 478805.511\n",
      "    load_throughput: 7334302.077\n",
      "    load_time_ms: 0.545\n",
      "    sample_throughput: 8.108\n",
      "    sample_time_ms: 493348.089\n",
      "    update_time_ms: 4.431\n",
      "  timestamp: 1650160649\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 03:05:41 (running for 05:00:10.24)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         5916.33</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">    2.27</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            298.1 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.1 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.72</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            265.24</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.4 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    2.06</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            301.31</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11578.2 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.8 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_03-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 290.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 2.12\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 196\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.4694592509378669\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007686514659504668\n",
      "          policy_loss: -0.0053668993975847\n",
      "          total_loss: 0.05015805113587468\n",
      "          vf_explained_var: 0.21782262036877295\n",
      "          vf_loss: 0.05033655244586689\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.76593245227606\n",
      "    ram_util_percent: 94.51776798825257\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03736371446486049\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7371657945596052\n",
      "    mean_inference_ms: 2.5949298852012026\n",
      "    mean_raw_obs_processing_ms: 0.1590789530947692\n",
      "  time_since_restore: 6409.322686910629\n",
      "  time_this_iter_s: 492.99545764923096\n",
      "  time_total_s: 6409.322686910629\n",
      "  timers:\n",
      "    learn_throughput: 8.355\n",
      "    learn_time_ms: 478733.12\n",
      "    load_throughput: 7451903.704\n",
      "    load_time_ms: 0.537\n",
      "    sample_throughput: 8.109\n",
      "    sample_time_ms: 493265.858\n",
      "    update_time_ms: 4.45\n",
      "  timestamp: 1650161142\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_03-13-55\n",
      "  done: false\n",
      "  episode_len_mean: 287.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 2.07\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 211\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.42306806559684457\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026814955009553765\n",
      "          policy_loss: 0.002153156626649121\n",
      "          total_loss: 0.10508511958415971\n",
      "          vf_explained_var: 0.46143367335360536\n",
      "          vf_loss: 0.08483186861780542\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.3119293078056\n",
      "    ram_util_percent: 94.42061855670104\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03744611652275007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7373871689481143\n",
      "    mean_inference_ms: 2.5962759001351934\n",
      "    mean_raw_obs_processing_ms: 0.15919471217588846\n",
      "  time_since_restore: 6901.736483812332\n",
      "  time_this_iter_s: 492.4137969017029\n",
      "  time_total_s: 6901.736483812332\n",
      "  timers:\n",
      "    learn_throughput: 8.358\n",
      "    learn_time_ms: 478611.42\n",
      "    load_throughput: 7440007.095\n",
      "    load_time_ms: 0.538\n",
      "    sample_throughput: 8.111\n",
      "    sample_time_ms: 493183.911\n",
      "    update_time_ms: 4.4\n",
      "  timestamp: 1650161635\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_03-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 285.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 2.02\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 227\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.4344458617830789\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003364090854474405\n",
      "          policy_loss: 0.006675876614948114\n",
      "          total_loss: 0.046571772015072725\n",
      "          vf_explained_var: 0.5302839733580107\n",
      "          vf_loss: 0.03648975297548277\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.505433186490457\n",
      "    ram_util_percent: 94.70587371512484\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751754289987506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.737570179880866\n",
      "    mean_inference_ms: 2.5974806918998627\n",
      "    mean_raw_obs_processing_ms: 0.15942488593281406\n",
      "  time_since_restore: 7394.631100893021\n",
      "  time_this_iter_s: 492.8946170806885\n",
      "  time_total_s: 7394.631100893021\n",
      "  timers:\n",
      "    learn_throughput: 8.356\n",
      "    learn_time_ms: 478688.93\n",
      "    load_throughput: 4813845.977\n",
      "    load_time_ms: 0.831\n",
      "    sample_throughput: 8.113\n",
      "    sample_time_ms: 493062.885\n",
      "    update_time_ms: 4.381\n",
      "  timestamp: 1650162128\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 03:22:22 (running for 05:16:50.94)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         7394.63</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">    2.02</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            285.39</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.1 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.72</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            265.24</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.4 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    2.06</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            301.31</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11578.2 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.8 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_03-30-20\n",
      "  done: false\n",
      "  episode_len_mean: 285.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 2.04\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 243\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062499999999999\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.4442397880778518\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005666596565418435\n",
      "          policy_loss: -0.00957773768052619\n",
      "          total_loss: 0.017864833785200953\n",
      "          vf_explained_var: 0.6461421645456745\n",
      "          vf_loss: 0.024573857379296134\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.977941176470587\n",
      "    ram_util_percent: 94.75132352941176\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03756465417250117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7376722147057834\n",
      "    mean_inference_ms: 2.5982567120259885\n",
      "    mean_raw_obs_processing_ms: 0.15966235250242908\n",
      "  time_since_restore: 7887.26301407814\n",
      "  time_this_iter_s: 492.63191318511963\n",
      "  time_total_s: 7887.26301407814\n",
      "  timers:\n",
      "    learn_throughput: 8.353\n",
      "    learn_time_ms: 478841.214\n",
      "    load_throughput: 4836047.504\n",
      "    load_time_ms: 0.827\n",
      "    sample_throughput: 8.111\n",
      "    sample_time_ms: 493146.192\n",
      "    update_time_ms: 4.422\n",
      "  timestamp: 1650162620\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_03-38-32\n",
      "  done: false\n",
      "  episode_len_mean: 284.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 257\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062499999999999\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.4497322079475208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007099089997250086\n",
      "          policy_loss: -0.005388174444356913\n",
      "          total_loss: 0.028742889559487263\n",
      "          vf_explained_var: 0.7778571113463371\n",
      "          vf_loss: 0.030537148545025497\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.4419734904271\n",
      "    ram_util_percent: 94.61104565537556\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03759546360851631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7377101145910391\n",
      "    mean_inference_ms: 2.598582700074847\n",
      "    mean_raw_obs_processing_ms: 0.15977267977151857\n",
      "  time_since_restore: 8379.143106222153\n",
      "  time_this_iter_s: 491.88009214401245\n",
      "  time_total_s: 8379.143106222153\n",
      "  timers:\n",
      "    learn_throughput: 8.354\n",
      "    learn_time_ms: 478829.404\n",
      "    load_throughput: 4821870.437\n",
      "    load_time_ms: 0.83\n",
      "    sample_throughput: 8.109\n",
      "    sample_time_ms: 493291.374\n",
      "    update_time_ms: 4.319\n",
      "  timestamp: 1650163112\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 03:39:03 (running for 05:33:31.68)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         8379.14</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">    2   </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            284.84</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.1 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.72</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            265.24</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.4 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    2.06</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            301.31</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11578.2 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.8 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_03-46-46\n",
      "  done: false\n",
      "  episode_len_mean: 289.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.07\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 272\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.5062499999999999\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.44317545205034237\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0046000897623099495\n",
      "          policy_loss: -0.012308293767273427\n",
      "          total_loss: 0.015245238422811712\n",
      "          vf_explained_var: 0.7649436395655396\n",
      "          vf_loss: 0.025224736433536295\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.156451612903226\n",
      "    ram_util_percent: 94.47844574780058\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03762756588604719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7377065805114645\n",
      "    mean_inference_ms: 2.5986461264150207\n",
      "    mean_raw_obs_processing_ms: 0.159864665520565\n",
      "  time_since_restore: 8872.6303794384\n",
      "  time_this_iter_s: 493.48727321624756\n",
      "  time_total_s: 8872.6303794384\n",
      "  timers:\n",
      "    learn_throughput: 8.355\n",
      "    learn_time_ms: 478782.783\n",
      "    load_throughput: 4741201.605\n",
      "    load_time_ms: 0.844\n",
      "    sample_throughput: 8.109\n",
      "    sample_time_ms: 493278.371\n",
      "    update_time_ms: 4.248\n",
      "  timestamp: 1650163606\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: f344c_00003\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_03-54-58\n",
      "  done: false\n",
      "  episode_len_mean: 290.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.08\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 288\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.25312499999999993\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.4517807431118463\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012172216155221544\n",
      "          policy_loss: -0.005036231612045599\n",
      "          total_loss: 0.04244115646898506\n",
      "          vf_explained_var: 0.43986200715905877\n",
      "          vf_loss: 0.04439629600984195\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.976288659793816\n",
      "    ram_util_percent: 94.41222385861562\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03765629219283602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7376456434594093\n",
      "    mean_inference_ms: 2.598500185364116\n",
      "    mean_raw_obs_processing_ms: 0.15980957158732875\n",
      "  time_since_restore: 9364.72022151947\n",
      "  time_this_iter_s: 492.08984208106995\n",
      "  time_total_s: 9364.72022151947\n",
      "  timers:\n",
      "    learn_throughput: 8.361\n",
      "    learn_time_ms: 478396.563\n",
      "    load_throughput: 4738389.584\n",
      "    load_time_ms: 0.844\n",
      "    sample_throughput: 8.11\n",
      "    sample_time_ms: 493227.122\n",
      "    update_time_ms: 4.28\n",
      "  timestamp: 1650164098\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 03:55:43 (running for 05:50:12.41)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>RUNNING   </td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         9364.72</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">    2.08</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            290.09</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.1 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.72</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            265.24</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.4 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    2.06</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            301.31</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11578.2 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.8 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_f344c_00003:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_04-03-11\n",
      "  done: true\n",
      "  episode_len_mean: 294.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 2.13\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 301\n",
      "  experiment_id: db2e14456bea4059a9c309dc98b3da87\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.25312499999999993\n",
      "          cur_lr: 9.999999999999999e-06\n",
      "          entropy: 0.4786388610960335\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006128655936956773\n",
      "          policy_loss: -0.007708323709867014\n",
      "          total_loss: 0.02820050039185306\n",
      "          vf_explained_var: 0.7797568678855896\n",
      "          vf_loss: 0.03435750762150893\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.305433186490454\n",
      "    ram_util_percent: 94.62760646108663\n",
      "  pid: 1289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03767887594363699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7376272240628416\n",
      "    mean_inference_ms: 2.598418444451184\n",
      "    mean_raw_obs_processing_ms: 0.1597322927998713\n",
      "  time_since_restore: 9857.938361644745\n",
      "  time_this_iter_s: 493.21814012527466\n",
      "  time_total_s: 9857.938361644745\n",
      "  timers:\n",
      "    learn_throughput: 8.364\n",
      "    learn_time_ms: 478260.619\n",
      "    load_throughput: 4765442.254\n",
      "    load_time_ms: 0.839\n",
      "    sample_throughput: 8.116\n",
      "    sample_time_ms: 492834.201\n",
      "    update_time_ms: 5.637\n",
      "  timestamp: 1650164591\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: f344c_00003\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=1289)\u001b[0m 2022-04-17 04:03:11,685\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 04:03:11 (running for 05:57:40.32)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-16_22-05-31<br>Number of trials: 4/4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00000</td><td>TERMINATED</td><td>127.0.0.1:1292</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.1 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.72</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            265.24</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00001</td><td>TERMINATED</td><td>127.0.0.1:1291</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11573.4 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    2.06</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            301.31</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00002</td><td>TERMINATED</td><td>127.0.0.1:1295</td><td style=\"text-align: right;\">         0.3</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11578.2 </td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    1.8 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            269.97</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_f344c_00003</td><td>TERMINATED</td><td>127.0.0.1:1289</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         9857.94</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">    2.13</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            294.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 04:03:12,527\tINFO tune.py:639 -- Total run time: 21461.12 seconds (21460.30 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No `mode` has been passed and  `default_mode` has not been set. Please specify the `mode` parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_1276/1384672134.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 1st the path, 2nd the metric value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m checkpoints = analysis.get_trial_checkpoints_paths(\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode_reward_mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     metric=\"episode_reward_mean\")\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rosetta_rl/lib/python3.9/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36mget_best_trial\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \"\"\"\n\u001b[1;32m    495\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"avg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last-5-avg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last-10-avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rosetta_rl/lib/python3.9/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36m_validate_mode\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    735\u001b[0m                 \u001b[0;34m\"No `mode` has been passed and  `default_mode` has \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \"not been set. Please specify the `mode` parameter.\")\n",
      "\u001b[0;31mValueError\u001b[0m: No `mode` has been passed and  `default_mode` has not been set. Please specify the `mode` parameter."
     ]
    }
   ],
   "source": [
    "#tune hyperparameters and save checkpoints\n",
    "randomize()\n",
    "# tune.run() allows setting a custom log directory (other than ``~/ray-results``)\n",
    "# and automatically saving the trained agent\n",
    "analysis = tune.run(\n",
    "    ppo.PPOTrainer,\n",
    "    config=config_ppo_tune,\n",
    "    stop=stop,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=True,)\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "# if there are multiple trials, select a specific trial or automatically\n",
    "# choose the best one according to a given metric\n",
    "last_checkpoint = analysis.get_last_checkpoint(\n",
    "    metric=\"episode_reward_mean\", mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94dcef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 10:31:10 (running for 00:00:00.13)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_10-31-10<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_1ddd7_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">5e-05</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_1ddd7_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">1e-05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=33071)\u001b[0m 2022-04-17 10:31:23,090\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=33071)\u001b[0m 2022-04-17 10:31:23,091\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=33071)\u001b[0m 2022-04-17 10:31:23,091\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=33070)\u001b[0m 2022-04-17 10:31:23,090\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=33070)\u001b[0m 2022-04-17 10:31:23,091\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=33070)\u001b[0m 2022-04-17 10:31:23,091\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=33069)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=33069)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=33104)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=33104)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=33071)\u001b[0m 2022-04-17 10:31:38,966\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=33070)\u001b[0m 2022-04-17 10:31:39,759\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-35-01\n",
      "  done: false\n",
      "  episode_len_mean: 255.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.375\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 8\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8008100986480713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.048180609941482544\n",
      "          model: {}\n",
      "          policy_loss: 0.026325799524784088\n",
      "          total_loss: 2.2265074253082275\n",
      "          vf_explained_var: -0.03373093903064728\n",
      "          vf_loss: 2.190545082092285\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.0286219081272\n",
      "    ram_util_percent: 95.5\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07357820216729265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7769954827272186\n",
      "    mean_inference_ms: 1.1984406933430523\n",
      "    mean_raw_obs_processing_ms: 0.12723727990197886\n",
      "  time_since_restore: 211.47838497161865\n",
      "  time_this_iter_s: 211.47838497161865\n",
      "  time_total_s: 211.47838497161865\n",
      "  timers:\n",
      "    learn_throughput: 19.784\n",
      "    learn_time_ms: 202183.407\n",
      "    load_throughput: 7145321.976\n",
      "    load_time_ms: 0.56\n",
      "    sample_throughput: 439.581\n",
      "    sample_time_ms: 9099.576\n",
      "    update_time_ms: 46.007\n",
      "  timestamp: 1650188101\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-35-02\n",
      "  done: false\n",
      "  episode_len_mean: 216.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6666666666666666\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.005987533833831549\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0205257348716259\n",
      "          model: {}\n",
      "          policy_loss: 0.013962501659989357\n",
      "          total_loss: 3.2452495098114014\n",
      "          vf_explained_var: -0.00811012927442789\n",
      "          vf_loss: 3.2271816730499268\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.59084507042253\n",
      "    ram_util_percent: 95.48415492957747\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07464056818284442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7786089347261574\n",
      "    mean_inference_ms: 1.2387352804933838\n",
      "    mean_raw_obs_processing_ms: 0.10210983278274059\n",
      "  time_since_restore: 211.73747396469116\n",
      "  time_this_iter_s: 211.73747396469116\n",
      "  time_total_s: 211.73747396469116\n",
      "  timers:\n",
      "    learn_throughput: 19.746\n",
      "    learn_time_ms: 202572.913\n",
      "    load_throughput: 5745621.918\n",
      "    load_time_ms: 0.696\n",
      "    sample_throughput: 435.652\n",
      "    sample_time_ms: 9181.647\n",
      "    update_time_ms: 17.825\n",
      "  timestamp: 1650188102\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-38-31\n",
      "  done: false\n",
      "  episode_len_mean: 486.88235294117646\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.411764705882353\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 17\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8626324534416199\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03055156022310257\n",
      "          model: {}\n",
      "          policy_loss: -0.005503037013113499\n",
      "          total_loss: 0.5983895659446716\n",
      "          vf_explained_var: -0.028895210474729538\n",
      "          vf_loss: 0.5947272181510925\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.21178571428572\n",
      "    ram_util_percent: 95.54964285714286\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07471023070301652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7804022662850163\n",
      "    mean_inference_ms: 1.2243051356576438\n",
      "    mean_raw_obs_processing_ms: 0.1302950052280572\n",
      "  time_since_restore: 421.37934494018555\n",
      "  time_this_iter_s: 209.9009599685669\n",
      "  time_total_s: 421.37934494018555\n",
      "  timers:\n",
      "    learn_throughput: 19.873\n",
      "    learn_time_ms: 201278.015\n",
      "    load_throughput: 7055179.142\n",
      "    load_time_ms: 0.567\n",
      "    sample_throughput: 36.179\n",
      "    sample_time_ms: 110562.87\n",
      "    update_time_ms: 28.558\n",
      "  timestamp: 1650188311\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-38-31\n",
      "  done: false\n",
      "  episode_len_mean: 216.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6666666666666666\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 3\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.021218996148491e-10\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.075472515545741e-11\n",
      "          model: {}\n",
      "          policy_loss: 0.014128318056464195\n",
      "          total_loss: 0.34568890929222107\n",
      "          vf_explained_var: 2.6341407277641338e-08\n",
      "          vf_loss: 0.33156058192253113\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.91648745519714\n",
      "    ram_util_percent: 95.53512544802867\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07464056818284442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7786089347261574\n",
      "    mean_inference_ms: 1.2387352804933838\n",
      "    mean_raw_obs_processing_ms: 0.10210983278274059\n",
      "  time_since_restore: 420.89257979393005\n",
      "  time_this_iter_s: 209.1551058292389\n",
      "  time_total_s: 420.89257979393005\n",
      "  timers:\n",
      "    learn_throughput: 19.864\n",
      "    learn_time_ms: 201368.824\n",
      "    load_throughput: 5265918.393\n",
      "    load_time_ms: 0.76\n",
      "    sample_throughput: 36.235\n",
      "    sample_time_ms: 110391.694\n",
      "    update_time_ms: 12.177\n",
      "  timestamp: 1650188311\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-42-03\n",
      "  done: false\n",
      "  episode_len_mean: 484.53846153846155\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.3076923076923077\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 26\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7682283520698547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03710649535059929\n",
      "          model: {}\n",
      "          policy_loss: -0.009728493168950081\n",
      "          total_loss: 0.11351777613162994\n",
      "          vf_explained_var: -0.696441650390625\n",
      "          vf_loss: 0.10654836148023605\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.68758865248226\n",
      "    ram_util_percent: 95.61879432624113\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07513135475415177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7802979754943116\n",
      "    mean_inference_ms: 1.2242693123053074\n",
      "    mean_raw_obs_processing_ms: 0.13155994075434518\n",
      "  time_since_restore: 633.0694808959961\n",
      "  time_this_iter_s: 211.69013595581055\n",
      "  time_total_s: 633.0694808959961\n",
      "  timers:\n",
      "    learn_throughput: 19.825\n",
      "    learn_time_ms: 201765.585\n",
      "    load_throughput: 4667685.06\n",
      "    load_time_ms: 0.857\n",
      "    sample_throughput: 27.873\n",
      "    sample_time_ms: 143506.287\n",
      "    update_time_ms: 23.308\n",
      "  timestamp: 1650188523\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-42-03\n",
      "  done: false\n",
      "  episode_len_mean: 1470.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.125\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 8\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.4795380682007817e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.5019925020686173e-10\n",
      "          model: {}\n",
      "          policy_loss: 0.0015769446035847068\n",
      "          total_loss: 0.2118334025144577\n",
      "          vf_explained_var: -0.06891388446092606\n",
      "          vf_loss: 0.21025647222995758\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.4918439716312\n",
      "    ram_util_percent: 95.61205673758866\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07364875665903253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7720034830923047\n",
      "    mean_inference_ms: 1.2306586954803165\n",
      "    mean_raw_obs_processing_ms: 0.09889230143754583\n",
      "  time_since_restore: 633.0982506275177\n",
      "  time_this_iter_s: 212.20567083358765\n",
      "  time_total_s: 633.0982506275177\n",
      "  timers:\n",
      "    learn_throughput: 19.796\n",
      "    learn_time_ms: 202065.669\n",
      "    load_throughput: 5979050.606\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 27.923\n",
      "    sample_time_ms: 143252.112\n",
      "    update_time_ms: 11.508\n",
      "  timestamp: 1650188523\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 503.1818181818182\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.5454545454545454\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 33\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9706208109855652\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0178748220205307\n",
      "          model: {}\n",
      "          policy_loss: -0.014952986501157284\n",
      "          total_loss: 0.1116725280880928\n",
      "          vf_explained_var: -0.2791014313697815\n",
      "          vf_loss: 0.11456001549959183\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.481654676259\n",
      "    ram_util_percent: 95.82410071942445\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07522897792504657\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7802881600053673\n",
      "    mean_inference_ms: 1.2234008694913574\n",
      "    mean_raw_obs_processing_ms: 0.13131095392969913\n",
      "  time_since_restore: 842.485090970993\n",
      "  time_this_iter_s: 209.41561007499695\n",
      "  time_total_s: 842.485090970993\n",
      "  timers:\n",
      "    learn_throughput: 19.857\n",
      "    learn_time_ms: 201435.835\n",
      "    load_throughput: 5239196.19\n",
      "    load_time_ms: 0.763\n",
      "    sample_throughput: 24.91\n",
      "    sample_time_ms: 160578.149\n",
      "    update_time_ms: 20.765\n",
      "  timestamp: 1650188732\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 1470.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.125\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 8\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 9.129413336417258e-10\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.486641329324708e-14\n",
      "          model: {}\n",
      "          policy_loss: 0.014128174632787704\n",
      "          total_loss: 0.08518557250499725\n",
      "          vf_explained_var: -3.582687924108541e-08\n",
      "          vf_loss: 0.07105740904808044\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.1359712230216\n",
      "    ram_util_percent: 95.82913669064747\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07364875665903253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7720034830923047\n",
      "    mean_inference_ms: 1.2306586954803165\n",
      "    mean_raw_obs_processing_ms: 0.09889230143754583\n",
      "  time_since_restore: 843.5480530261993\n",
      "  time_this_iter_s: 210.44980239868164\n",
      "  time_total_s: 843.5480530261993\n",
      "  timers:\n",
      "    learn_throughput: 19.799\n",
      "    learn_time_ms: 202033.046\n",
      "    load_throughput: 6480191.58\n",
      "    load_time_ms: 0.617\n",
      "    sample_throughput: 24.931\n",
      "    sample_time_ms: 160443.031\n",
      "    update_time_ms: 9.959\n",
      "  timestamp: 1650188734\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 10:47:50 (running for 00:16:40.27)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_10-31-10<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_1ddd7_00000</td><td>RUNNING </td><td>127.0.0.1:33070</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         843.548</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> 1.125  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">          1470    </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_1ddd7_00001</td><td>RUNNING </td><td>127.0.0.1:33071</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         842.485</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> 1.54545</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           503.182</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-49-02\n",
      "  done: false\n",
      "  episode_len_mean: 468.27906976744185\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.627906976744186\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 43\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7938090562820435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017194069921970367\n",
      "          model: {}\n",
      "          policy_loss: -0.003813857911154628\n",
      "          total_loss: 0.10486706346273422\n",
      "          vf_explained_var: 0.016636718064546585\n",
      "          vf_loss: 0.09707492589950562\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.49569892473119\n",
      "    ram_util_percent: 95.88709677419354\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07541440398837673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.781270676041321\n",
      "    mean_inference_ms: 1.2263623001067232\n",
      "    mean_raw_obs_processing_ms: 0.13167351239656555\n",
      "  time_since_restore: 1051.9215371608734\n",
      "  time_this_iter_s: 209.43644618988037\n",
      "  time_total_s: 1051.9215371608734\n",
      "  timers:\n",
      "    learn_throughput: 19.888\n",
      "    learn_time_ms: 201127.986\n",
      "    load_throughput: 5502530.666\n",
      "    load_time_ms: 0.727\n",
      "    sample_throughput: 23.462\n",
      "    sample_time_ms: 170489.078\n",
      "    update_time_ms: 18.058\n",
      "  timestamp: 1650188942\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-49-03\n",
      "  done: false\n",
      "  episode_len_mean: 1470.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.125\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 8\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 9.129284550546402e-10\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3835021908400975e-16\n",
      "          model: {}\n",
      "          policy_loss: 0.014128221198916435\n",
      "          total_loss: 0.059640977531671524\n",
      "          vf_explained_var: -7.787058109443024e-08\n",
      "          vf_loss: 0.04551275074481964\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.91335740072202\n",
      "    ram_util_percent: 95.89819494584836\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07364875665903253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7720034830923047\n",
      "    mean_inference_ms: 1.2306586954803165\n",
      "    mean_raw_obs_processing_ms: 0.09889230143754583\n",
      "  time_since_restore: 1052.3675589561462\n",
      "  time_this_iter_s: 208.8195059299469\n",
      "  time_total_s: 1052.3675589561462\n",
      "  timers:\n",
      "    learn_throughput: 19.837\n",
      "    learn_time_ms: 201645.144\n",
      "    load_throughput: 4834375.288\n",
      "    load_time_ms: 0.827\n",
      "    sample_throughput: 23.461\n",
      "    sample_time_ms: 170494.946\n",
      "    update_time_ms: 8.946\n",
      "  timestamp: 1650188943\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-52-33\n",
      "  done: false\n",
      "  episode_len_mean: 468.01851851851853\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.5555555555555556\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 54\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.701270580291748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012055598199367523\n",
      "          model: {}\n",
      "          policy_loss: -0.013657917268574238\n",
      "          total_loss: 0.06916701048612595\n",
      "          vf_explained_var: -0.2093951553106308\n",
      "          vf_loss: 0.0746874064207077\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.09139784946237\n",
      "    ram_util_percent: 95.88064516129032\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07547641753450611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.781878214253572\n",
      "    mean_inference_ms: 1.2297457741700362\n",
      "    mean_raw_obs_processing_ms: 0.1321404702468534\n",
      "  time_since_restore: 1262.999279975891\n",
      "  time_this_iter_s: 211.0777428150177\n",
      "  time_total_s: 1262.999279975891\n",
      "  timers:\n",
      "    learn_throughput: 19.877\n",
      "    learn_time_ms: 201233.337\n",
      "    load_throughput: 5782256.074\n",
      "    load_time_ms: 0.692\n",
      "    sample_throughput: 22.606\n",
      "    sample_time_ms: 176945.134\n",
      "    update_time_ms: 23.161\n",
      "  timestamp: 1650189153\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-52-33\n",
      "  done: false\n",
      "  episode_len_mean: 1425.8235294117646\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.5294117647058822\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 17\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.6023700999976427e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.172310692965084e-09\n",
      "          model: {}\n",
      "          policy_loss: -0.005221839062869549\n",
      "          total_loss: 0.08166030049324036\n",
      "          vf_explained_var: 0.1391790509223938\n",
      "          vf_loss: 0.08688214421272278\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.03669064748202\n",
      "    ram_util_percent: 95.87482014388488\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07340047598383481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7698146437709921\n",
      "    mean_inference_ms: 1.2212275091153355\n",
      "    mean_raw_obs_processing_ms: 0.0988966674916524\n",
      "  time_since_restore: 1262.651623725891\n",
      "  time_this_iter_s: 210.28406476974487\n",
      "  time_total_s: 1262.651623725891\n",
      "  timers:\n",
      "    learn_throughput: 19.844\n",
      "    learn_time_ms: 201570.421\n",
      "    load_throughput: 5122553.356\n",
      "    load_time_ms: 0.781\n",
      "    sample_throughput: 22.605\n",
      "    sample_time_ms: 176948.96\n",
      "    update_time_ms: 9.051\n",
      "  timestamp: 1650189153\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 435.29411764705884\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.5294117647058822\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 68\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8673811554908752\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007180201821029186\n",
      "          model: {}\n",
      "          policy_loss: -0.018525652587413788\n",
      "          total_loss: 0.06408266723155975\n",
      "          vf_explained_var: 0.02094348520040512\n",
      "          vf_loss: 0.0777616873383522\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.55627240143369\n",
      "    ram_util_percent: 95.85734767025089\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0755039474279424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7822891661720123\n",
      "    mean_inference_ms: 1.2311352538371103\n",
      "    mean_raw_obs_processing_ms: 0.13355219314690708\n",
      "  time_since_restore: 1473.7756960391998\n",
      "  time_this_iter_s: 210.77641606330872\n",
      "  time_total_s: 1473.7756960391998\n",
      "  timers:\n",
      "    learn_throughput: 19.873\n",
      "    learn_time_ms: 201280.451\n",
      "    load_throughput: 5987280.755\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 21.999\n",
      "    sample_time_ms: 181825.798\n",
      "    update_time_ms: 22.372\n",
      "  timestamp: 1650189364\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 1260.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.7\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 20\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3165885093258112e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.247463925716177e-12\n",
      "          model: {}\n",
      "          policy_loss: 0.00010148325236514211\n",
      "          total_loss: 0.008528183214366436\n",
      "          vf_explained_var: 0.03951852396130562\n",
      "          vf_loss: 0.008426706306636333\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.72652329749106\n",
      "    ram_util_percent: 95.83620071684588\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07334671577724068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.769397293228146\n",
      "    mean_inference_ms: 1.2196827394405512\n",
      "    mean_raw_obs_processing_ms: 0.09885465784851492\n",
      "  time_since_restore: 1473.4616894721985\n",
      "  time_this_iter_s: 210.81006574630737\n",
      "  time_total_s: 1473.4616894721985\n",
      "  timers:\n",
      "    learn_throughput: 19.838\n",
      "    learn_time_ms: 201629.519\n",
      "    load_throughput: 5407270.685\n",
      "    load_time_ms: 0.74\n",
      "    sample_throughput: 22.017\n",
      "    sample_time_ms: 181679.246\n",
      "    update_time_ms: 9.316\n",
      "  timestamp: 1650189364\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-59-32\n",
      "  done: false\n",
      "  episode_len_mean: 1260.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.7\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 20\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.755341675988234e-10\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.676496430361778e-15\n",
      "          model: {}\n",
      "          policy_loss: -0.014128286391496658\n",
      "          total_loss: -0.014073414728045464\n",
      "          vf_explained_var: -2.0887262053292943e-07\n",
      "          vf_loss: 5.486975351232104e-05\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.95398550724637\n",
      "    ram_util_percent: 95.76702898550724\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07334671577724068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.769397293228146\n",
      "    mean_inference_ms: 1.2196827394405512\n",
      "    mean_raw_obs_processing_ms: 0.09885465784851492\n",
      "  time_since_restore: 1681.2665824890137\n",
      "  time_this_iter_s: 207.80489301681519\n",
      "  time_total_s: 1681.2665824890137\n",
      "  timers:\n",
      "    learn_throughput: 19.868\n",
      "    learn_time_ms: 201324.541\n",
      "    load_throughput: 5622626.953\n",
      "    load_time_ms: 0.711\n",
      "    sample_throughput: 21.586\n",
      "    sample_time_ms: 185302.125\n",
      "    update_time_ms: 9.94\n",
      "  timestamp: 1650189572\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_10-59-34\n",
      "  done: false\n",
      "  episode_len_mean: 402.6470588235294\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.4941176470588236\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 85\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.841537356376648\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009392190724611282\n",
      "          model: {}\n",
      "          policy_loss: -0.001234284951351583\n",
      "          total_loss: 0.06271281838417053\n",
      "          vf_explained_var: 0.03850569203495979\n",
      "          vf_loss: 0.057607367634773254\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.1189964157706\n",
      "    ram_util_percent: 95.74659498207886\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07549977528486697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7825583447319644\n",
      "    mean_inference_ms: 1.2325989977574734\n",
      "    mean_raw_obs_processing_ms: 0.13565246541795184\n",
      "  time_since_restore: 1683.599730014801\n",
      "  time_this_iter_s: 209.8240339756012\n",
      "  time_total_s: 1683.599730014801\n",
      "  timers:\n",
      "    learn_throughput: 19.884\n",
      "    learn_time_ms: 201164.796\n",
      "    load_throughput: 5760170.293\n",
      "    load_time_ms: 0.694\n",
      "    sample_throughput: 21.565\n",
      "    sample_time_ms: 185486.416\n",
      "    update_time_ms: 21.051\n",
      "  timestamp: 1650189574\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_11-03-02\n",
      "  done: false\n",
      "  episode_len_mean: 1453.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.64\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 25\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.3279326910596865e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.7360722021363415e-10\n",
      "          model: {}\n",
      "          policy_loss: -0.002195490524172783\n",
      "          total_loss: 0.025121422484517097\n",
      "          vf_explained_var: 0.1179603636264801\n",
      "          vf_loss: 0.02731691300868988\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.60322580645162\n",
      "    ram_util_percent: 95.72544802867384\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07344398271668559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7695669004332706\n",
      "    mean_inference_ms: 1.220545811041533\n",
      "    mean_raw_obs_processing_ms: 0.0990257142518667\n",
      "  time_since_restore: 1891.6458494663239\n",
      "  time_this_iter_s: 210.37926697731018\n",
      "  time_total_s: 1891.6458494663239\n",
      "  timers:\n",
      "    learn_throughput: 19.876\n",
      "    learn_time_ms: 201244.811\n",
      "    load_throughput: 5814654.344\n",
      "    load_time_ms: 0.688\n",
      "    sample_throughput: 21.283\n",
      "    sample_time_ms: 187940.247\n",
      "    update_time_ms: 10.202\n",
      "  timestamp: 1650189782\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_11-03-04\n",
      "  done: false\n",
      "  episode_len_mean: 391.92857142857144\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.5510204081632653\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 98\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8439295887947083\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010213148780167103\n",
      "          model: {}\n",
      "          policy_loss: -0.012901403941214085\n",
      "          total_loss: 0.03907288983464241\n",
      "          vf_explained_var: 0.323587030172348\n",
      "          vf_loss: 0.04508041962981224\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.05878136200717\n",
      "    ram_util_percent: 95.71971326164875\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07555084523895379\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7830373763578397\n",
      "    mean_inference_ms: 1.2356766045237557\n",
      "    mean_raw_obs_processing_ms: 0.13708448061583278\n",
      "  time_since_restore: 1894.366094827652\n",
      "  time_this_iter_s: 210.76636481285095\n",
      "  time_total_s: 1894.366094827652\n",
      "  timers:\n",
      "    learn_throughput: 19.889\n",
      "    learn_time_ms: 201111.691\n",
      "    load_throughput: 5891565.96\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 21.247\n",
      "    sample_time_ms: 188264.069\n",
      "    update_time_ms: 19.458\n",
      "  timestamp: 1650189784\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 11:04:31 (running for 00:33:20.49)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_10-31-10<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_1ddd7_00000</td><td>RUNNING </td><td>127.0.0.1:33070</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         1891.65</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> 1.64   </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">          1453.08 </td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_1ddd7_00001</td><td>RUNNING </td><td>127.0.0.1:33071</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         1894.37</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> 1.55102</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           391.929</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_1ddd7_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_11-06-31\n",
      "  done: true\n",
      "  episode_len_mean: 1453.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.64\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 25\n",
      "  experiment_id: 991aab3184a04763aeee0c63d327af8c\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 7.744708763190999e-10\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.9416622647270947e-14\n",
      "          model: {}\n",
      "          policy_loss: 0.01412784680724144\n",
      "          total_loss: 0.01936306618154049\n",
      "          vf_explained_var: -1.6086845633367375e-08\n",
      "          vf_loss: 0.0052352165803313255\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.72527075812275\n",
      "    ram_util_percent: 95.78592057761733\n",
      "  pid: 33070\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07344398271668559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7695669004332706\n",
      "    mean_inference_ms: 1.220545811041533\n",
      "    mean_raw_obs_processing_ms: 0.0990257142518667\n",
      "  time_since_restore: 2100.3236615657806\n",
      "  time_this_iter_s: 208.6778120994568\n",
      "  time_total_s: 2100.3236615657806\n",
      "  timers:\n",
      "    learn_throughput: 19.9\n",
      "    learn_time_ms: 201003.777\n",
      "    load_throughput: 5983528.657\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 21.03\n",
      "    sample_time_ms: 190201.524\n",
      "    update_time_ms: 10.217\n",
      "  timestamp: 1650189991\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 1ddd7_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=33070)\u001b[0m 2022-04-17 11:06:31,472\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_1ddd7_00001:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_11-06-34\n",
      "  done: true\n",
      "  episode_len_mean: 364.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.61\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 111\n",
      "  experiment_id: e9b0d29523fe4a28b48fd7f645e04cef\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8821674585342407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008171937428414822\n",
      "          model: {}\n",
      "          policy_loss: -0.017674613744020462\n",
      "          total_loss: 0.024049077183008194\n",
      "          vf_explained_var: 0.2258344292640686\n",
      "          vf_loss: 0.03620763123035431\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.69964157706092\n",
      "    ram_util_percent: 95.78243727598567\n",
      "  pid: 33071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07585322784058177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.784539992602798\n",
      "    mean_inference_ms: 1.2434178187937142\n",
      "    mean_raw_obs_processing_ms: 0.1395868661593716\n",
      "  time_since_restore: 2104.0575737953186\n",
      "  time_this_iter_s: 209.69147896766663\n",
      "  time_total_s: 2104.0575737953186\n",
      "  timers:\n",
      "    learn_throughput: 19.908\n",
      "    learn_time_ms: 200924.884\n",
      "    load_throughput: 5785845.432\n",
      "    load_time_ms: 0.691\n",
      "    sample_throughput: 20.991\n",
      "    sample_time_ms: 190556.5\n",
      "    update_time_ms: 18.003\n",
      "  timestamp: 1650189994\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 1ddd7_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=33071)\u001b[0m 2022-04-17 11:06:34,665\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 11:06:34 (running for 00:35:24.24)<br>Memory usage on this node: 7.5/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_10-31-10<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_1ddd7_00000</td><td>TERMINATED</td><td>127.0.0.1:33070</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2100.32</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    1.64</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           1453.08</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_1ddd7_00001</td><td>TERMINATED</td><td>127.0.0.1:33071</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2104.06</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    1.61</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            364.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 11:06:35,402\tINFO tune.py:639 -- Total run time: 2124.87 seconds (2124.19 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No `mode` has been passed and  `default_mode` has not been set. Please specify the `mode` parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_1276/2849567667.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# 1st the path, 2nd the metric value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m checkpoints = analysis.get_trial_checkpoints_paths(\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode_reward_mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     metric=\"episode_reward_mean\")\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rosetta_rl/lib/python3.9/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36mget_best_trial\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \"\"\"\n\u001b[1;32m    495\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"avg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last-5-avg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last-10-avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rosetta_rl/lib/python3.9/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36m_validate_mode\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    735\u001b[0m                 \u001b[0;34m\"No `mode` has been passed and  `default_mode` has \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \"not been set. Please specify the `mode` parameter.\")\n",
      "\u001b[0;31mValueError\u001b[0m: No `mode` has been passed and  `default_mode` has not been set. Please specify the `mode` parameter."
     ]
    }
   ],
   "source": [
    "#Set hyperparameters to tune\n",
    "config_ppo_tune={\n",
    "        \"env\": \"Breakout-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"framework\": \"tf\",\n",
    "        \"clip_param\": 0.2,\n",
    "        \"lr\": tune.grid_search([5e-5, 1e-5]),\n",
    "    }\n",
    "#set stopping criteria & reporting style/frequency for tuning\n",
    "stop = tune.stopper.MaximumIterationStopper(max_iter=10)\n",
    "reporter = JupyterNotebookReporter(overwrite=False, max_report_frequency=1000)   #unit: seconds\n",
    "\n",
    "#tune hyperparameters and save checkpoints\n",
    "randomize()\n",
    "# tune.run() allows setting a custom log directory (other than ``~/ray-results``)\n",
    "# and automatically saving the trained agent\n",
    "analysis = tune.run(\n",
    "    ppo.PPOTrainer,\n",
    "    config=config_ppo_tune,\n",
    "    stop=stop,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=True,)\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "# if there are multiple trials, select a specific trial or automatically\n",
    "# choose the best one according to a given metric\n",
    "last_checkpoint = analysis.get_last_checkpoint(\n",
    "    metric=\"episode_reward_mean\", mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba718c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 12:04:47 (running for 00:00:00.13)<br>Memory usage on this node: 7.5/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_12-04-47<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_31d7f_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">5e-05</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_31d7f_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">1e-05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=47077)\u001b[0m 2022-04-17 12:04:58,312\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=47077)\u001b[0m 2022-04-17 12:04:58,313\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=47077)\u001b[0m 2022-04-17 12:04:58,313\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=47076)\u001b[0m 2022-04-17 12:04:58,312\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=47076)\u001b[0m 2022-04-17 12:04:58,313\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=47076)\u001b[0m 2022-04-17 12:04:58,313\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47075)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47075)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47104)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47104)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=47077)\u001b[0m 2022-04-17 12:05:13,378\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=47076)\u001b[0m 2022-04-17 12:05:15,312\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_31d7f_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-08-37\n",
      "  done: false\n",
      "  episode_len_mean: 234.55555555555554\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.2222222222222223\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 9\n",
      "  experiment_id: 253a94c4910048e9ad83a18b7d06bcff\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.0007191270124167204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00657288683578372\n",
      "          model: {}\n",
      "          policy_loss: -0.01444114837795496\n",
      "          total_loss: 0.6566326022148132\n",
      "          vf_explained_var: -0.020622221753001213\n",
      "          vf_loss: 0.6697592735290527\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.51894736842105\n",
      "    ram_util_percent: 95.59333333333333\n",
      "  pid: 47077\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07044527120096807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7688449162657456\n",
      "    mean_inference_ms: 1.1352174015707803\n",
      "    mean_raw_obs_processing_ms: 0.13393040032304546\n",
      "  time_since_restore: 213.10718774795532\n",
      "  time_this_iter_s: 213.10718774795532\n",
      "  time_total_s: 213.10718774795532\n",
      "  timers:\n",
      "    learn_throughput: 19.578\n",
      "    learn_time_ms: 204312.301\n",
      "    load_throughput: 7446611.629\n",
      "    load_time_ms: 0.537\n",
      "    sample_throughput: 455.305\n",
      "    sample_time_ms: 8785.324\n",
      "    update_time_ms: 20.945\n",
      "  timestamp: 1650193717\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 31d7f_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_31d7f_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-08-38\n",
      "  done: false\n",
      "  episode_len_mean: 291.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 2.1666666666666665\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 6\n",
      "  experiment_id: 936e2c16424d42389328068dfd7621be\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.0002848451549652964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00022102468938101083\n",
      "          model: {}\n",
      "          policy_loss: 0.014550336636602879\n",
      "          total_loss: 7.3353705406188965\n",
      "          vf_explained_var: -0.043530650436878204\n",
      "          vf_loss: 7.320775508880615\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.33473684210527\n",
      "    ram_util_percent: 95.60842105263158\n",
      "  pid: 47076\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0731283591646339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7981546698019404\n",
      "    mean_inference_ms: 1.3408162718384125\n",
      "    mean_raw_obs_processing_ms: 0.11741682995083748\n",
      "  time_since_restore: 212.8155002593994\n",
      "  time_this_iter_s: 212.8155002593994\n",
      "  time_total_s: 212.8155002593994\n",
      "  timers:\n",
      "    learn_throughput: 19.696\n",
      "    learn_time_ms: 203090.95\n",
      "    load_throughput: 5170174.422\n",
      "    load_time_ms: 0.774\n",
      "    sample_throughput: 411.737\n",
      "    sample_time_ms: 9714.932\n",
      "    update_time_ms: 13.699\n",
      "  timestamp: 1650193718\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 31d7f_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_31d7f_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-12-09\n",
      "  done: false\n",
      "  episode_len_mean: 234.55555555555554\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.2222222222222223\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 253a94c4910048e9ad83a18b7d06bcff\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 6.76177457125185e-14\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.8646930923508593e-19\n",
      "          model: {}\n",
      "          policy_loss: -0.014128007926046848\n",
      "          total_loss: 0.01846436969935894\n",
      "          vf_explained_var: 8.331831891084107e-10\n",
      "          vf_loss: 0.032592374831438065\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.53003533568905\n",
      "    ram_util_percent: 95.5303886925795\n",
      "  pid: 47077\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07044527120096807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7688449162657456\n",
      "    mean_inference_ms: 1.1352174015707803\n",
      "    mean_raw_obs_processing_ms: 0.13393040032304546\n",
      "  time_since_restore: 424.4130139350891\n",
      "  time_this_iter_s: 211.3058261871338\n",
      "  time_total_s: 424.4130139350891\n",
      "  timers:\n",
      "    learn_throughput: 19.682\n",
      "    learn_time_ms: 203228.018\n",
      "    load_throughput: 7374600.44\n",
      "    load_time_ms: 0.542\n",
      "    sample_throughput: 35.978\n",
      "    sample_time_ms: 111177.845\n",
      "    update_time_ms: 21.868\n",
      "  timestamp: 1650193929\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 31d7f_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_31d7f_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-12-10\n",
      "  done: false\n",
      "  episode_len_mean: 291.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 2.1666666666666665\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 6\n",
      "  experiment_id: 936e2c16424d42389328068dfd7621be\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 5.802449436487223e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.17808416589105e-07\n",
      "          model: {}\n",
      "          policy_loss: 0.014089231379330158\n",
      "          total_loss: 0.7292267680168152\n",
      "          vf_explained_var: -2.9667731382687634e-07\n",
      "          vf_loss: 0.7151374816894531\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.07208480565372\n",
      "    ram_util_percent: 95.52579505300352\n",
      "  pid: 47076\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0731283591646339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7981546698019404\n",
      "    mean_inference_ms: 1.3408162718384125\n",
      "    mean_raw_obs_processing_ms: 0.11741682995083748\n",
      "  time_since_restore: 424.3373453617096\n",
      "  time_this_iter_s: 211.52184510231018\n",
      "  time_total_s: 424.3373453617096\n",
      "  timers:\n",
      "    learn_throughput: 19.723\n",
      "    learn_time_ms: 202813.577\n",
      "    load_throughput: 5879521.991\n",
      "    load_time_ms: 0.68\n",
      "    sample_throughput: 36.059\n",
      "    sample_time_ms: 110929.416\n",
      "    update_time_ms: 10.163\n",
      "  timestamp: 1650193930\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 31d7f_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_31d7f_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-15-43\n",
      "  done: false\n",
      "  episode_len_mean: 234.55555555555554\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.2222222222222223\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 253a94c4910048e9ad83a18b7d06bcff\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 6.991798226540016e-14\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -9.580224101036604e-21\n",
      "          model: {}\n",
      "          policy_loss: -0.01412827055901289\n",
      "          total_loss: 0.0030857587698847055\n",
      "          vf_explained_var: 6.915420414088658e-08\n",
      "          vf_loss: 0.017214026302099228\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.36132404181186\n",
      "    ram_util_percent: 95.77073170731707\n",
      "  pid: 47077\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07044527120096807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7688449162657456\n",
      "    mean_inference_ms: 1.1352174015707803\n",
      "    mean_raw_obs_processing_ms: 0.13393040032304546\n",
      "  time_since_restore: 638.1968467235565\n",
      "  time_this_iter_s: 213.7838327884674\n",
      "  time_total_s: 638.1968467235565\n",
      "  timers:\n",
      "    learn_throughput: 19.636\n",
      "    learn_time_ms: 203703.087\n",
      "    load_throughput: 6678828.025\n",
      "    load_time_ms: 0.599\n",
      "    sample_throughput: 27.667\n",
      "    sample_time_ms: 144576.688\n",
      "    update_time_ms: 18.971\n",
      "  timestamp: 1650194143\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 31d7f_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_31d7f_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-15-44\n",
      "  done: false\n",
      "  episode_len_mean: 1504.125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 2.125\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 8\n",
      "  experiment_id: 936e2c16424d42389328068dfd7621be\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05000000074505806\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.703941378167656e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2175418834535776e-08\n",
      "          model: {}\n",
      "          policy_loss: 0.013624312356114388\n",
      "          total_loss: 0.25527867674827576\n",
      "          vf_explained_var: -0.030367545783519745\n",
      "          vf_loss: 0.2416543960571289\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.7773519163763\n",
      "    ram_util_percent: 95.76236933797908\n",
      "  pid: 47076\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07371496624088683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7950695242827688\n",
      "    mean_inference_ms: 1.325840822536872\n",
      "    mean_raw_obs_processing_ms: 0.11322441954503278\n",
      "  time_since_restore: 639.0551233291626\n",
      "  time_this_iter_s: 214.717777967453\n",
      "  time_total_s: 639.0551233291626\n",
      "  timers:\n",
      "    learn_throughput: 19.634\n",
      "    learn_time_ms: 203732.62\n",
      "    load_throughput: 5897779.236\n",
      "    load_time_ms: 0.678\n",
      "    sample_throughput: 27.677\n",
      "    sample_time_ms: 144524.366\n",
      "    update_time_ms: 9.789\n",
      "  timestamp: 1650194144\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 31d7f_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_31d7f_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-19-25\n",
      "  done: false\n",
      "  episode_len_mean: 695.6666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.2380952380952381\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 21\n",
      "  experiment_id: 253a94c4910048e9ad83a18b7d06bcff\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05000000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.431291827076446e-14\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.830350299306293e-21\n",
      "          model: {}\n",
      "          policy_loss: 0.00644528167322278\n",
      "          total_loss: 0.13421234488487244\n",
      "          vf_explained_var: 0.07093005627393723\n",
      "          vf_loss: 0.12776707112789154\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.87533783783785\n",
      "    ram_util_percent: 95.86824324324324\n",
      "  pid: 47077\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0737918918487535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7790604058397579\n",
      "    mean_inference_ms: 1.2113446119537241\n",
      "    mean_raw_obs_processing_ms: 0.12337072005557918\n",
      "  time_since_restore: 860.1983637809753\n",
      "  time_this_iter_s: 222.00151705741882\n",
      "  time_total_s: 860.1983637809753\n",
      "  timers:\n",
      "    learn_throughput: 19.439\n",
      "    learn_time_ms: 205772.667\n",
      "    load_throughput: 6748679.002\n",
      "    load_time_ms: 0.593\n",
      "    sample_throughput: 24.674\n",
      "    sample_time_ms: 162115.678\n",
      "    update_time_ms: 18.354\n",
      "  timestamp: 1650194365\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 31d7f_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_31d7f_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-19-26\n",
      "  done: false\n",
      "  episode_len_mean: 923.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.8\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 15\n",
      "  experiment_id: 936e2c16424d42389328068dfd7621be\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.116555593711382e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.544818382219319e-09\n",
      "          model: {}\n",
      "          policy_loss: -0.0006774211651645601\n",
      "          total_loss: 0.07917734980583191\n",
      "          vf_explained_var: -0.06332580745220184\n",
      "          vf_loss: 0.07985477149486542\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.33673469387755\n",
      "    ram_util_percent: 95.88503401360543\n",
      "  pid: 47076\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07527805222281118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.791402242568642\n",
      "    mean_inference_ms: 1.313787746449234\n",
      "    mean_raw_obs_processing_ms: 0.11019216266420409\n",
      "  time_since_restore: 860.8750715255737\n",
      "  time_this_iter_s: 221.81994819641113\n",
      "  time_total_s: 860.8750715255737\n",
      "  timers:\n",
      "    learn_throughput: 19.433\n",
      "    learn_time_ms: 205840.379\n",
      "    load_throughput: 6013877.946\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 24.659\n",
      "    sample_time_ms: 162209.625\n",
      "    update_time_ms: 8.908\n",
      "  timestamp: 1650194366\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 31d7f_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 12:21:27 (running for 00:16:40.21)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_12-04-47<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_31d7f_00000</td><td>RUNNING </td><td>127.0.0.1:47077</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         860.198</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  1.2381</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           695.667</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_31d7f_00001</td><td>RUNNING </td><td>127.0.0.1:47076</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         860.875</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  1.8   </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           923.4  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_31d7f_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-23-01\n",
      "  done: true\n",
      "  episode_len_mean: 695.6666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.2380952380952381\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 21\n",
      "  experiment_id: 253a94c4910048e9ad83a18b7d06bcff\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.5598887254901027e-14\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.896297758318223e-21\n",
      "          model: {}\n",
      "          policy_loss: 0.014128301292657852\n",
      "          total_loss: 0.0260055772960186\n",
      "          vf_explained_var: -9.677743051383914e-09\n",
      "          vf_loss: 0.011877271346747875\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.96885813148789\n",
      "    ram_util_percent: 95.7325259515571\n",
      "  pid: 47077\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0737918918487535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7790604058397579\n",
      "    mean_inference_ms: 1.2113446119537241\n",
      "    mean_raw_obs_processing_ms: 0.12337072005557918\n",
      "  time_since_restore: 1076.7637679576874\n",
      "  time_this_iter_s: 216.56540417671204\n",
      "  time_total_s: 1076.7637679576874\n",
      "  timers:\n",
      "    learn_throughput: 19.419\n",
      "    learn_time_ms: 205983.396\n",
      "    load_throughput: 6757377.155\n",
      "    load_time_ms: 0.592\n",
      "    sample_throughput: 22.981\n",
      "    sample_time_ms: 174058.321\n",
      "    update_time_ms: 16.421\n",
      "  timestamp: 1650194581\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 31d7f_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=47077)\u001b[0m 2022-04-17 12:23:01,807\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_31d7f_00001:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_12-23-03\n",
      "  done: true\n",
      "  episode_len_mean: 923.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.8\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 15\n",
      "  experiment_id: 936e2c16424d42389328068dfd7621be\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012500000186264515\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.8458469241977582e-07\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.0495277270347074e-11\n",
      "          model: {}\n",
      "          policy_loss: 0.014128322713077068\n",
      "          total_loss: 0.03527059406042099\n",
      "          vf_explained_var: -1.1149913916597143e-06\n",
      "          vf_loss: 0.021142270416021347\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.00311418685122\n",
      "    ram_util_percent: 95.72283737024222\n",
      "  pid: 47076\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07527805222281118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.791402242568642\n",
      "    mean_inference_ms: 1.313787746449234\n",
      "    mean_raw_obs_processing_ms: 0.11019216266420409\n",
      "  time_since_restore: 1077.216994524002\n",
      "  time_this_iter_s: 216.34192299842834\n",
      "  time_total_s: 1077.216994524002\n",
      "  timers:\n",
      "    learn_throughput: 19.418\n",
      "    learn_time_ms: 205995.428\n",
      "    load_throughput: 5958664.583\n",
      "    load_time_ms: 0.671\n",
      "    sample_throughput: 22.968\n",
      "    sample_time_ms: 174156.063\n",
      "    update_time_ms: 8.13\n",
      "  timestamp: 1650194583\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 31d7f_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=47076)\u001b[0m 2022-04-17 12:23:03,032\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 12:23:03 (running for 00:18:15.67)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_12-04-47<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_31d7f_00000</td><td>TERMINATED</td><td>127.0.0.1:47077</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         1076.76</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  1.2381</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           695.667</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_31d7f_00001</td><td>TERMINATED</td><td>127.0.0.1:47076</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         1077.22</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  1.8   </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           923.4  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 12:23:03,908\tINFO tune.py:639 -- Total run time: 1096.40 seconds (1095.60 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No `mode` has been passed and  `default_mode` has not been set. Please specify the `mode` parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_1276/3604045892.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# 1st the path, 2nd the metric value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m checkpoints = analysis.get_trial_checkpoints_paths(\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode_reward_mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     metric=\"episode_reward_mean\", mode=\"max\")\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rosetta_rl/lib/python3.9/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36mget_best_trial\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \"\"\"\n\u001b[1;32m    495\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"avg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last-5-avg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last-10-avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rosetta_rl/lib/python3.9/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36m_validate_mode\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    735\u001b[0m                 \u001b[0;34m\"No `mode` has been passed and  `default_mode` has \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \"not been set. Please specify the `mode` parameter.\")\n",
      "\u001b[0;31mValueError\u001b[0m: No `mode` has been passed and  `default_mode` has not been set. Please specify the `mode` parameter."
     ]
    }
   ],
   "source": [
    "#Set hyperparameters to tune\n",
    "config_ppo_tune={\n",
    "        \"env\": \"Breakout-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"framework\": \"tf\",\n",
    "        \"clip_param\": 0.2,\n",
    "        \"lr\": tune.grid_search([5e-5, 1e-5]),\n",
    "    }\n",
    "#set stopping criteria & reporting style/frequency for tuning\n",
    "stop = tune.stopper.MaximumIterationStopper(max_iter=5)\n",
    "reporter = JupyterNotebookReporter(overwrite=False, max_report_frequency=1000)   #unit: seconds\n",
    "\n",
    "#tune hyperparameters and save checkpoints\n",
    "randomize()\n",
    "# tune.run() allows setting a custom log directory (other than ``~/ray-results``)\n",
    "# and automatically saving the trained agent\n",
    "analysis = tune.run(\n",
    "    ppo.PPOTrainer,\n",
    "    config=config_ppo_tune,\n",
    "    stop=stop,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=True,)\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# if there are multiple trials, select a specific trial or automatically\n",
    "# choose the best one according to a given metric\n",
    "last_checkpoint = analysis.get_last_checkpoint(\n",
    "    metric=\"episode_reward_mean\", mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba1d85e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 18:58:40 (running for 00:00:00.13)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 PENDING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=98215)\u001b[0m 2022-04-17 18:58:50,292\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=98215)\u001b[0m 2022-04-17 18:58:50,293\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=98215)\u001b[0m 2022-04-17 18:58:50,293\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=98216)\u001b[0m 2022-04-17 18:58:50,292\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=98216)\u001b[0m 2022-04-17 18:58:50,293\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=98216)\u001b[0m 2022-04-17 18:58:50,293\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=98214)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=98214)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=98241)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=98241)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=98215)\u001b[0m 2022-04-17 18:59:06,545\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=98216)\u001b[0m 2022-04-17 18:59:07,743\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-02-29\n",
      "  done: false\n",
      "  episode_len_mean: 244.66666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.3333333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 3.807686198342708e-06\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.865408307821781e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.008732643909752369\n",
      "          total_loss: 0.17839957773685455\n",
      "          vf_explained_var: -0.010258889757096767\n",
      "          vf_loss: 0.16966655850410461\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.89788732394366\n",
      "    ram_util_percent: 95.68274647887324\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07966463937070542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7839199305474535\n",
      "    mean_inference_ms: 1.2531838873510688\n",
      "    mean_raw_obs_processing_ms: 0.10797715848518948\n",
      "  time_since_restore: 211.98220086097717\n",
      "  time_this_iter_s: 211.98220086097717\n",
      "  time_total_s: 211.98220086097717\n",
      "  timers:\n",
      "    learn_throughput: 19.727\n",
      "    learn_time_ms: 202764.203\n",
      "    load_throughput: 6770466.505\n",
      "    load_time_ms: 0.591\n",
      "    sample_throughput: 433.572\n",
      "    sample_time_ms: 9225.687\n",
      "    update_time_ms: 17.083\n",
      "  timestamp: 1650218549\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-02-30\n",
      "  done: false\n",
      "  episode_len_mean: 263.70588235294116\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.0588235294117647\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 17\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.07227671891450882\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01619725301861763\n",
      "          model: {}\n",
      "          policy_loss: -0.004157049115747213\n",
      "          total_loss: 12.667231559753418\n",
      "          vf_explained_var: -0.2017166018486023\n",
      "          vf_loss: 12.6681489944458\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.33146853146853\n",
      "    ram_util_percent: 95.66013986013986\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07978578532704471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8026013163380907\n",
      "    mean_inference_ms: 1.3697022230915354\n",
      "    mean_raw_obs_processing_ms: 0.18789118094850676\n",
      "  time_since_restore: 212.95370769500732\n",
      "  time_this_iter_s: 212.95370769500732\n",
      "  time_total_s: 212.95370769500732\n",
      "  timers:\n",
      "    learn_throughput: 19.722\n",
      "    learn_time_ms: 202816.066\n",
      "    load_throughput: 5413751.533\n",
      "    load_time_ms: 0.739\n",
      "    sample_throughput: 394.5\n",
      "    sample_time_ms: 10139.409\n",
      "    update_time_ms: 15.452\n",
      "  timestamp: 1650218550\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-05-59\n",
      "  done: false\n",
      "  episode_len_mean: 244.66666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.3333333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 3\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.828114199775075e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -5.001416004424665e-22\n",
      "          model: {}\n",
      "          policy_loss: 0.014128051698207855\n",
      "          total_loss: 0.04208771511912346\n",
      "          vf_explained_var: 2.0143806978012435e-07\n",
      "          vf_loss: 0.027959663420915604\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.45729537366547\n",
      "    ram_util_percent: 95.70462633451957\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07966463937070542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7839199305474535\n",
      "    mean_inference_ms: 1.2531838873510688\n",
      "    mean_raw_obs_processing_ms: 0.10797715848518948\n",
      "  time_since_restore: 421.71596574783325\n",
      "  time_this_iter_s: 209.73376488685608\n",
      "  time_total_s: 421.71596574783325\n",
      "  timers:\n",
      "    learn_throughput: 19.845\n",
      "    learn_time_ms: 201566.668\n",
      "    load_throughput: 6799277.001\n",
      "    load_time_ms: 0.588\n",
      "    sample_throughput: 36.123\n",
      "    sample_time_ms: 110732.962\n",
      "    update_time_ms: 18.6\n",
      "  timestamp: 1650218759\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-06-01\n",
      "  done: false\n",
      "  episode_len_mean: 263.2647058823529\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.1764705882352942\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 34\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.08281636983156204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01879277266561985\n",
      "          model: {}\n",
      "          policy_loss: -0.003660220419988036\n",
      "          total_loss: 0.2373788058757782\n",
      "          vf_explained_var: -0.07485437393188477\n",
      "          vf_loss: 0.23728047311306\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.43558718861209\n",
      "    ram_util_percent: 95.69608540925266\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07949829808029825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.807512309281092\n",
      "    mean_inference_ms: 1.3895835933154719\n",
      "    mean_raw_obs_processing_ms: 0.19109517194002992\n",
      "  time_since_restore: 423.627014875412\n",
      "  time_this_iter_s: 210.67330718040466\n",
      "  time_total_s: 423.627014875412\n",
      "  timers:\n",
      "    learn_throughput: 19.846\n",
      "    learn_time_ms: 201547.974\n",
      "    load_throughput: 6223002.967\n",
      "    load_time_ms: 0.643\n",
      "    sample_throughput: 35.807\n",
      "    sample_time_ms: 111710.799\n",
      "    update_time_ms: 12.179\n",
      "  timestamp: 1650218761\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-09-29\n",
      "  done: false\n",
      "  episode_len_mean: 1654.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2857142857142858\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 7\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05000000074505806\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.136507165415304e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.1147294073692987e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0019387829815968871\n",
      "          total_loss: 0.04182250797748566\n",
      "          vf_explained_var: -0.044892020523548126\n",
      "          vf_loss: 0.039883725345134735\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.15978647686833\n",
      "    ram_util_percent: 95.65622775800712\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07966355267480188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7904542916440309\n",
      "    mean_inference_ms: 1.3029019782259326\n",
      "    mean_raw_obs_processing_ms: 0.1038859729550287\n",
      "  time_since_restore: 631.6201777458191\n",
      "  time_this_iter_s: 209.90421199798584\n",
      "  time_total_s: 631.6201777458191\n",
      "  timers:\n",
      "    learn_throughput: 19.899\n",
      "    learn_time_ms: 201017.994\n",
      "    load_throughput: 7043331.654\n",
      "    load_time_ms: 0.568\n",
      "    sample_throughput: 27.786\n",
      "    sample_time_ms: 143956.461\n",
      "    update_time_ms: 21.913\n",
      "  timestamp: 1650218969\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-09-31\n",
      "  done: false\n",
      "  episode_len_mean: 261.7450980392157\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.196078431372549\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 51\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.15311545133590698\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010171827860176563\n",
      "          model: {}\n",
      "          policy_loss: -0.0023205713368952274\n",
      "          total_loss: 0.05984564498066902\n",
      "          vf_explained_var: 0.09455787390470505\n",
      "          vf_loss: 0.060131847858428955\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.86049822064057\n",
      "    ram_util_percent: 95.67686832740213\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07971747393908182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8129027872137742\n",
      "    mean_inference_ms: 1.4061298107772204\n",
      "    mean_raw_obs_processing_ms: 0.19244045296227508\n",
      "  time_since_restore: 633.9446187019348\n",
      "  time_this_iter_s: 210.31760382652283\n",
      "  time_total_s: 633.9446187019348\n",
      "  timers:\n",
      "    learn_throughput: 19.911\n",
      "    learn_time_ms: 200890.532\n",
      "    load_throughput: 6776847.718\n",
      "    load_time_ms: 0.59\n",
      "    sample_throughput: 27.617\n",
      "    sample_time_ms: 144836.377\n",
      "    update_time_ms: 11.232\n",
      "  timestamp: 1650218971\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-12-56\n",
      "  done: false\n",
      "  episode_len_mean: 1654.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2857142857142858\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 7\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128100126981735\n",
      "          total_loss: 0.018154747784137726\n",
      "          vf_explained_var: -1.082048584066797e-06\n",
      "          vf_loss: 0.004026648122817278\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.4373188405797\n",
      "    ram_util_percent: 95.55760869565218\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07966355267480188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7904542916440309\n",
      "    mean_inference_ms: 1.3029019782259326\n",
      "    mean_raw_obs_processing_ms: 0.1038859729550287\n",
      "  time_since_restore: 838.9461867809296\n",
      "  time_this_iter_s: 207.32600903511047\n",
      "  time_total_s: 838.9461867809296\n",
      "  timers:\n",
      "    learn_throughput: 19.986\n",
      "    learn_time_ms: 200138.635\n",
      "    load_throughput: 7074516.551\n",
      "    load_time_ms: 0.565\n",
      "    sample_throughput: 24.93\n",
      "    sample_time_ms: 160448.916\n",
      "    update_time_ms: 18.46\n",
      "  timestamp: 1650219176\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 252.74647887323943\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.1690140845070423\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 71\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.20404380559921265\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002404013415798545\n",
      "          model: {}\n",
      "          policy_loss: -0.007594067603349686\n",
      "          total_loss: 0.035038821399211884\n",
      "          vf_explained_var: 0.13859055936336517\n",
      "          vf_loss: 0.04215208441019058\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.80863309352519\n",
      "    ram_util_percent: 95.53848920863311\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08034482618409698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8185074471962751\n",
      "    mean_inference_ms: 1.4224810210104357\n",
      "    mean_raw_obs_processing_ms: 0.194503978083509\n",
      "  time_since_restore: 842.3954355716705\n",
      "  time_this_iter_s: 208.45081686973572\n",
      "  time_total_s: 842.3954355716705\n",
      "  timers:\n",
      "    learn_throughput: 19.999\n",
      "    learn_time_ms: 200012.069\n",
      "    load_throughput: 5448032.473\n",
      "    load_time_ms: 0.734\n",
      "    sample_throughput: 24.799\n",
      "    sample_time_ms: 161295.062\n",
      "    update_time_ms: 9.821\n",
      "  timestamp: 1650219180\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 19:15:20 (running for 00:16:40.47)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00000 with episode_reward_mean=1.2857142857142858 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-16-25\n",
      "  done: false\n",
      "  episode_len_mean: 1654.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2857142857142858\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 7\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012500000186264515\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128003269433975\n",
      "          total_loss: 0.015319050289690495\n",
      "          vf_explained_var: 7.146148561787413e-08\n",
      "          vf_loss: 0.0011910456232726574\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.94301075268817\n",
      "    ram_util_percent: 95.62903225806454\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07966355267480188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7904542916440309\n",
      "    mean_inference_ms: 1.3029019782259326\n",
      "    mean_raw_obs_processing_ms: 0.1038859729550287\n",
      "  time_since_restore: 1047.5311436653137\n",
      "  time_this_iter_s: 208.58495688438416\n",
      "  time_total_s: 1047.5311436653137\n",
      "  timers:\n",
      "    learn_throughput: 20.026\n",
      "    learn_time_ms: 199738.238\n",
      "    load_throughput: 7102970.364\n",
      "    load_time_ms: 0.563\n",
      "    sample_throughput: 23.534\n",
      "    sample_time_ms: 169963.726\n",
      "    update_time_ms: 17.172\n",
      "  timestamp: 1650219385\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-16-28\n",
      "  done: false\n",
      "  episode_len_mean: 247.63736263736263\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.1758241758241759\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 91\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.22508841753005981\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037811908405274153\n",
      "          model: {}\n",
      "          policy_loss: -0.005146475043147802\n",
      "          total_loss: 0.03413914144039154\n",
      "          vf_explained_var: 0.17722757160663605\n",
      "          vf_loss: 0.038907501846551895\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.92867383512544\n",
      "    ram_util_percent: 95.61111111111113\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0809921696691431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8229723657048086\n",
      "    mean_inference_ms: 1.4404974775925479\n",
      "    mean_raw_obs_processing_ms: 0.19682922614710935\n",
      "  time_since_restore: 1051.143299818039\n",
      "  time_this_iter_s: 208.7478642463684\n",
      "  time_total_s: 1051.143299818039\n",
      "  timers:\n",
      "    learn_throughput: 20.056\n",
      "    learn_time_ms: 199446.054\n",
      "    load_throughput: 5558682.659\n",
      "    load_time_ms: 0.72\n",
      "    sample_throughput: 23.414\n",
      "    sample_time_ms: 170838.927\n",
      "    update_time_ms: 9.712\n",
      "  timestamp: 1650219388\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-19-54\n",
      "  done: false\n",
      "  episode_len_mean: 2204.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 10\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0062500000931322575\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.011009681058709e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.6865755888604e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.002731625223532319\n",
      "          total_loss: 0.048115842044353485\n",
      "          vf_explained_var: -0.004828316625207663\n",
      "          vf_loss: 0.04538421332836151\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.35683453237412\n",
      "    ram_util_percent: 95.76079136690647\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026885161365638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7982291797999512\n",
      "    mean_inference_ms: 1.3378640927212389\n",
      "    mean_raw_obs_processing_ms: 0.10276715642139438\n",
      "  time_since_restore: 1256.7774436473846\n",
      "  time_this_iter_s: 209.24629998207092\n",
      "  time_total_s: 1256.7774436473846\n",
      "  timers:\n",
      "    learn_throughput: 20.046\n",
      "    learn_time_ms: 199540.395\n",
      "    load_throughput: 7118038.184\n",
      "    load_time_ms: 0.562\n",
      "    sample_throughput: 22.669\n",
      "    sample_time_ms: 176450.501\n",
      "    update_time_ms: 17.664\n",
      "  timestamp: 1650219594\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 245.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 109\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05000000074505806\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.21938881278038025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00514340540394187\n",
      "          model: {}\n",
      "          policy_loss: -0.009362142533063889\n",
      "          total_loss: 0.033628106117248535\n",
      "          vf_explained_var: 0.39281150698661804\n",
      "          vf_loss: 0.0427330806851387\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.01115107913668\n",
      "    ram_util_percent: 95.7593525179856\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.081576635115848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8286217790306839\n",
      "    mean_inference_ms: 1.4619919989378503\n",
      "    mean_raw_obs_processing_ms: 0.19934227452689535\n",
      "  time_since_restore: 1260.9182407855988\n",
      "  time_this_iter_s: 209.77494096755981\n",
      "  time_total_s: 1260.9182407855988\n",
      "  timers:\n",
      "    learn_throughput: 20.073\n",
      "    learn_time_ms: 199268.138\n",
      "    load_throughput: 5740052.232\n",
      "    load_time_ms: 0.697\n",
      "    sample_throughput: 22.582\n",
      "    sample_time_ms: 177135.414\n",
      "    update_time_ms: 9.118\n",
      "  timestamp: 1650219598\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-23-23\n",
      "  done: false\n",
      "  episode_len_mean: 2204.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 10\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031250000465661287\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128291048109531\n",
      "          total_loss: 0.016313573345541954\n",
      "          vf_explained_var: -4.6139120968291536e-07\n",
      "          vf_loss: 0.0021852743811905384\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.5794964028777\n",
      "    ram_util_percent: 95.85215827338129\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026885161365638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.7982291797999512\n",
      "    mean_inference_ms: 1.3378640927212389\n",
      "    mean_raw_obs_processing_ms: 0.10276715642139438\n",
      "  time_since_restore: 1465.2777516841888\n",
      "  time_this_iter_s: 208.5003080368042\n",
      "  time_total_s: 1465.2777516841888\n",
      "  timers:\n",
      "    learn_throughput: 20.072\n",
      "    learn_time_ms: 199282.637\n",
      "    load_throughput: 7163627.669\n",
      "    load_time_ms: 0.558\n",
      "    sample_throughput: 22.079\n",
      "    sample_time_ms: 181164.27\n",
      "    update_time_ms: 17.304\n",
      "  timestamp: 1650219803\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-23-27\n",
      "  done: false\n",
      "  episode_len_mean: 245.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 127\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05000000074505806\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.18538345396518707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0031078713946044445\n",
      "          model: {}\n",
      "          policy_loss: -0.0011692502303048968\n",
      "          total_loss: 0.03931841999292374\n",
      "          vf_explained_var: 0.45296621322631836\n",
      "          vf_loss: 0.04033227637410164\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.70611510791366\n",
      "    ram_util_percent: 95.86115107913668\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08243852724996997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8361701720179276\n",
      "    mean_inference_ms: 1.4887580740115365\n",
      "    mean_raw_obs_processing_ms: 0.2019051892109444\n",
      "  time_since_restore: 1470.0577476024628\n",
      "  time_this_iter_s: 209.139506816864\n",
      "  time_total_s: 1470.0577476024628\n",
      "  timers:\n",
      "    learn_throughput: 20.096\n",
      "    learn_time_ms: 199039.903\n",
      "    load_throughput: 5941842.246\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 22.0\n",
      "    sample_time_ms: 181815.685\n",
      "    update_time_ms: 8.557\n",
      "  timestamp: 1650219807\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-26-51\n",
      "  done: false\n",
      "  episode_len_mean: 2913.181818181818\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015625000232830644\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.279391047475431e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.538638066671398e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014791359193623066\n",
      "          total_loss: 0.02103659138083458\n",
      "          vf_explained_var: 0.041903670877218246\n",
      "          vf_loss: 0.006245225667953491\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.32482014388489\n",
      "    ram_util_percent: 95.87625899280575\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08048666485754669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.800806156240972\n",
      "    mean_inference_ms: 1.3504481064843674\n",
      "    mean_raw_obs_processing_ms: 0.10247934590762249\n",
      "  time_since_restore: 1673.837501525879\n",
      "  time_this_iter_s: 208.55974984169006\n",
      "  time_total_s: 1673.837501525879\n",
      "  timers:\n",
      "    learn_throughput: 20.096\n",
      "    learn_time_ms: 199047.305\n",
      "    load_throughput: 7232337.967\n",
      "    load_time_ms: 0.553\n",
      "    sample_throughput: 21.663\n",
      "    sample_time_ms: 184644.825\n",
      "    update_time_ms: 16.848\n",
      "  timestamp: 1650220011\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-26-56\n",
      "  done: false\n",
      "  episode_len_mean: 241.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 145\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.24716731905937195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004294135142117739\n",
      "          model: {}\n",
      "          policy_loss: 0.0003204708336852491\n",
      "          total_loss: 0.04054845869541168\n",
      "          vf_explained_var: 0.5446487069129944\n",
      "          vf_loss: 0.04012063518166542\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.85899280575539\n",
      "    ram_util_percent: 95.88597122302157\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08332484400627406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8425595353984636\n",
      "    mean_inference_ms: 1.5129864843774492\n",
      "    mean_raw_obs_processing_ms: 0.20405507830214667\n",
      "  time_since_restore: 1678.9599225521088\n",
      "  time_this_iter_s: 208.902174949646\n",
      "  time_total_s: 1678.9599225521088\n",
      "  timers:\n",
      "    learn_throughput: 20.121\n",
      "    learn_time_ms: 198793.588\n",
      "    load_throughput: 6197429.376\n",
      "    load_time_ms: 0.645\n",
      "    sample_throughput: 21.589\n",
      "    sample_time_ms: 185281.533\n",
      "    update_time_ms: 8.163\n",
      "  timestamp: 1650220016\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-30-21\n",
      "  done: false\n",
      "  episode_len_mean: 1711.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 20\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812500116415322\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.366461917635769e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.2795514822201317e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0005191044183447957\n",
      "          total_loss: 0.05658759921789169\n",
      "          vf_explained_var: -0.06502837687730789\n",
      "          vf_loss: 0.05606849119067192\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.22903225806452\n",
      "    ram_util_percent: 95.86379928315412\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08172875825491976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8148880075049361\n",
      "    mean_inference_ms: 1.41654330530844\n",
      "    mean_raw_obs_processing_ms: 0.10502114242121216\n",
      "  time_since_restore: 1883.1100375652313\n",
      "  time_this_iter_s: 209.27253603935242\n",
      "  time_total_s: 1883.1100375652313\n",
      "  timers:\n",
      "    learn_throughput: 20.109\n",
      "    learn_time_ms: 198912.042\n",
      "    load_throughput: 7287049.081\n",
      "    load_time_ms: 0.549\n",
      "    sample_throughput: 21.351\n",
      "    sample_time_ms: 187345.665\n",
      "    update_time_ms: 16.217\n",
      "  timestamp: 1650220221\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-30-26\n",
      "  done: false\n",
      "  episode_len_mean: 242.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 165\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012500000186264515\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.2712375223636627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01381409727036953\n",
      "          model: {}\n",
      "          policy_loss: 0.008266225457191467\n",
      "          total_loss: 0.04354849457740784\n",
      "          vf_explained_var: 0.39651697874069214\n",
      "          vf_loss: 0.03510959446430206\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.67813620071686\n",
      "    ram_util_percent: 95.86989247311827\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0840929473718334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8486300594509255\n",
      "    mean_inference_ms: 1.537793783721424\n",
      "    mean_raw_obs_processing_ms: 0.20627223960700608\n",
      "  time_since_restore: 1888.8209393024445\n",
      "  time_this_iter_s: 209.8610167503357\n",
      "  time_total_s: 1888.8209393024445\n",
      "  timers:\n",
      "    learn_throughput: 20.132\n",
      "    learn_time_ms: 198690.319\n",
      "    load_throughput: 6273419.918\n",
      "    load_time_ms: 0.638\n",
      "    sample_throughput: 21.285\n",
      "    sample_time_ms: 187930.026\n",
      "    update_time_ms: 7.867\n",
      "  timestamp: 1650220226\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 19:32:00 (running for 00:33:20.60)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.38 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 1711.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 20\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003906250058207661\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.01412796787917614\n",
      "          total_loss: 0.014183887280523777\n",
      "          vf_explained_var: -1.955545030796202e-06\n",
      "          vf_loss: 5.592373418039642e-05\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.16690647482014\n",
      "    ram_util_percent: 95.89424460431654\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08172875825491976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8148880075049361\n",
      "    mean_inference_ms: 1.41654330530844\n",
      "    mean_raw_obs_processing_ms: 0.10502114242121216\n",
      "  time_since_restore: 2093.0341806411743\n",
      "  time_this_iter_s: 209.924143075943\n",
      "  time_total_s: 2093.0341806411743\n",
      "  timers:\n",
      "    learn_throughput: 20.115\n",
      "    learn_time_ms: 198856.894\n",
      "    load_throughput: 7239672.046\n",
      "    load_time_ms: 0.553\n",
      "    sample_throughput: 21.102\n",
      "    sample_time_ms: 189558.237\n",
      "    update_time_ms: 16.161\n",
      "  timestamp: 1650220431\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-33-57\n",
      "  done: false\n",
      "  episode_len_mean: 244.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 184\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012500000186264515\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.3722148537635803\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022657882422208786\n",
      "          model: {}\n",
      "          policy_loss: 0.005577734671533108\n",
      "          total_loss: 0.054077256470918655\n",
      "          vf_explained_var: 0.6062136292457581\n",
      "          vf_loss: 0.04821629822254181\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.11857142857141\n",
      "    ram_util_percent: 95.88357142857143\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08462119390695735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.854131601019053\n",
      "    mean_inference_ms: 1.5584590585835825\n",
      "    mean_raw_obs_processing_ms: 0.2073661094423246\n",
      "  time_since_restore: 2099.412642478943\n",
      "  time_this_iter_s: 210.5917031764984\n",
      "  time_total_s: 2099.412642478943\n",
      "  timers:\n",
      "    learn_throughput: 20.135\n",
      "    learn_time_ms: 198659.487\n",
      "    load_throughput: 6353562.069\n",
      "    load_time_ms: 0.63\n",
      "    sample_throughput: 21.036\n",
      "    sample_time_ms: 190147.374\n",
      "    update_time_ms: 9.506\n",
      "  timestamp: 1650220437\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-37-20\n",
      "  done: false\n",
      "  episode_len_mean: 2019.090909090909\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 22\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00019531250291038305\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6000358798826938e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.081134607157181e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.013233378529548645\n",
      "          total_loss: -0.009580565616488457\n",
      "          vf_explained_var: 0.04830196872353554\n",
      "          vf_loss: 0.0036528129130601883\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.29856115107914\n",
      "    ram_util_percent: 95.86187050359712\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08195572702599407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8174418886987965\n",
      "    mean_inference_ms: 1.4287942283794886\n",
      "    mean_raw_obs_processing_ms: 0.10518372059602159\n",
      "  time_since_restore: 2301.967136621475\n",
      "  time_this_iter_s: 208.9329559803009\n",
      "  time_total_s: 2301.967136621475\n",
      "  timers:\n",
      "    learn_throughput: 20.173\n",
      "    learn_time_ms: 198283.891\n",
      "    load_throughput: 7271047.933\n",
      "    load_time_ms: 0.55\n",
      "    sample_throughput: 19.078\n",
      "    sample_time_ms: 209664.008\n",
      "    update_time_ms: 15.659\n",
      "  timestamp: 1650220640\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-37-26\n",
      "  done: false\n",
      "  episode_len_mean: 238.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 204\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4300917088985443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006237329449504614\n",
      "          model: {}\n",
      "          policy_loss: -0.014405746012926102\n",
      "          total_loss: 0.010775040835142136\n",
      "          vf_explained_var: 0.5034765601158142\n",
      "          vf_loss: 0.025063836947083473\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.74007220216606\n",
      "    ram_util_percent: 95.84584837545125\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08527797411366463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8598706698063181\n",
      "    mean_inference_ms: 1.5796480416352252\n",
      "    mean_raw_obs_processing_ms: 0.20840327556244043\n",
      "  time_since_restore: 2308.1396605968475\n",
      "  time_this_iter_s: 208.72701811790466\n",
      "  time_total_s: 2308.1396605968475\n",
      "  timers:\n",
      "    learn_throughput: 20.203\n",
      "    learn_time_ms: 197986.185\n",
      "    load_throughput: 6467451.525\n",
      "    load_time_ms: 0.618\n",
      "    sample_throughput: 19.026\n",
      "    sample_time_ms: 210237.436\n",
      "    update_time_ms: 8.543\n",
      "  timestamp: 1650220646\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-40-49\n",
      "  done: false\n",
      "  episode_len_mean: 1592.2413793103449\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.103448275862069\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 29\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.765625145519152e-05\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4195499816085102e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.597841307236257e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0010412478586658835\n",
      "          total_loss: 0.042385783046483994\n",
      "          vf_explained_var: 0.010814511217176914\n",
      "          vf_loss: 0.0413445383310318\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.70035842293908\n",
      "    ram_util_percent: 95.77598566308244\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08253939561965602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8243519841439956\n",
      "    mean_inference_ms: 1.4627946446511009\n",
      "    mean_raw_obs_processing_ms: 0.10635579561705379\n",
      "  time_since_restore: 2511.1868476867676\n",
      "  time_this_iter_s: 209.21971106529236\n",
      "  time_total_s: 2511.1868476867676\n",
      "  timers:\n",
      "    learn_throughput: 20.203\n",
      "    learn_time_ms: 197989.895\n",
      "    load_throughput: 7228754.363\n",
      "    load_time_ms: 0.553\n",
      "    sample_throughput: 19.108\n",
      "    sample_time_ms: 209336.149\n",
      "    update_time_ms: 14.999\n",
      "  timestamp: 1650220849\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-40-55\n",
      "  done: false\n",
      "  episode_len_mean: 238.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 224\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4571036994457245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004855033941566944\n",
      "          model: {}\n",
      "          policy_loss: -0.009542642161250114\n",
      "          total_loss: 0.01905636116862297\n",
      "          vf_explained_var: 0.33885109424591064\n",
      "          vf_loss: 0.02850797027349472\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.97230215827338\n",
      "    ram_util_percent: 95.74604316546763\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08585729614481954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8649811645662356\n",
      "    mean_inference_ms: 1.5982419197920117\n",
      "    mean_raw_obs_processing_ms: 0.20951856431097865\n",
      "  time_since_restore: 2517.0068333148956\n",
      "  time_this_iter_s: 208.8671727180481\n",
      "  time_total_s: 2517.0068333148956\n",
      "  timers:\n",
      "    learn_throughput: 20.238\n",
      "    learn_time_ms: 197652.193\n",
      "    load_throughput: 4390791.939\n",
      "    load_time_ms: 0.911\n",
      "    sample_throughput: 19.074\n",
      "    sample_time_ms: 209710.801\n",
      "    update_time_ms: 9.396\n",
      "  timestamp: 1650220855\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-44-19\n",
      "  done: false\n",
      "  episode_len_mean: 1592.2413793103449\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.103448275862069\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 29\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.882812572759576e-05\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6875004068904024e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.668840318176706e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128034934401512\n",
      "          total_loss: -0.012516677379608154\n",
      "          vf_explained_var: -2.4226403994020984e-08\n",
      "          vf_loss: 0.0016113542951643467\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.95071428571428\n",
      "    ram_util_percent: 95.69857142857144\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08253939561965602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8243519841439956\n",
      "    mean_inference_ms: 1.4627946446511009\n",
      "    mean_raw_obs_processing_ms: 0.10635579561705379\n",
      "  time_since_restore: 2721.247990846634\n",
      "  time_this_iter_s: 210.06114315986633\n",
      "  time_total_s: 2721.247990846634\n",
      "  timers:\n",
      "    learn_throughput: 20.217\n",
      "    learn_time_ms: 197855.278\n",
      "    load_throughput: 7308104.718\n",
      "    load_time_ms: 0.547\n",
      "    sample_throughput: 19.121\n",
      "    sample_time_ms: 209195.003\n",
      "    update_time_ms: 13.938\n",
      "  timestamp: 1650221059\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-44-26\n",
      "  done: false\n",
      "  episode_len_mean: 233.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 243\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4374522268772125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002989551518112421\n",
      "          model: {}\n",
      "          policy_loss: 0.005189146846532822\n",
      "          total_loss: 0.024585163220763206\n",
      "          vf_explained_var: 0.5966015458106995\n",
      "          vf_loss: 0.019367989152669907\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.27127659574468\n",
      "    ram_util_percent: 95.68687943262411\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08642055614670931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8693569533302106\n",
      "    mean_inference_ms: 1.6146098474582067\n",
      "    mean_raw_obs_processing_ms: 0.21064072716276067\n",
      "  time_since_restore: 2728.204694509506\n",
      "  time_this_iter_s: 211.1978611946106\n",
      "  time_total_s: 2728.204694509506\n",
      "  timers:\n",
      "    learn_throughput: 20.247\n",
      "    learn_time_ms: 197561.831\n",
      "    load_throughput: 3938960.862\n",
      "    load_time_ms: 1.015\n",
      "    sample_throughput: 19.088\n",
      "    sample_time_ms: 209555.822\n",
      "    update_time_ms: 9.917\n",
      "  timestamp: 1650221066\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-47-49\n",
      "  done: false\n",
      "  episode_len_mean: 1872.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1333333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.441406286379788e-05\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.7563435473458939e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.295215455643822e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0051959496922791\n",
      "          total_loss: 0.0002140375872841105\n",
      "          vf_explained_var: -0.05913614109158516\n",
      "          vf_loss: 0.005409976001828909\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.41423487544485\n",
      "    ram_util_percent: 95.57758007117438\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0826511402761713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8253795100276708\n",
      "    mean_inference_ms: 1.4681058567724141\n",
      "    mean_raw_obs_processing_ms: 0.10647598775951214\n",
      "  time_since_restore: 2931.007625579834\n",
      "  time_this_iter_s: 209.75963473320007\n",
      "  time_total_s: 2931.007625579834\n",
      "  timers:\n",
      "    learn_throughput: 20.23\n",
      "    learn_time_ms: 197726.91\n",
      "    load_throughput: 6999547.749\n",
      "    load_time_ms: 0.571\n",
      "    sample_throughput: 19.1\n",
      "    sample_time_ms: 209427.621\n",
      "    update_time_ms: 14.077\n",
      "  timestamp: 1650221269\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-47-55\n",
      "  done: false\n",
      "  episode_len_mean: 233.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 262\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4491066336631775\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005114683415740728\n",
      "          model: {}\n",
      "          policy_loss: -0.0069213793613016605\n",
      "          total_loss: 0.017861751839518547\n",
      "          vf_explained_var: 0.4451602101325989\n",
      "          vf_loss: 0.02475915662944317\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.25285714285714\n",
      "    ram_util_percent: 95.56\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08698000413277057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8734045874874852\n",
      "    mean_inference_ms: 1.631172202320502\n",
      "    mean_raw_obs_processing_ms: 0.21141072721661822\n",
      "  time_since_restore: 2937.186442375183\n",
      "  time_this_iter_s: 208.98174786567688\n",
      "  time_total_s: 2937.186442375183\n",
      "  timers:\n",
      "    learn_throughput: 20.262\n",
      "    learn_time_ms: 197413.218\n",
      "    load_throughput: 4191160.63\n",
      "    load_time_ms: 0.954\n",
      "    sample_throughput: 19.078\n",
      "    sample_time_ms: 209670.545\n",
      "    update_time_ms: 10.261\n",
      "  timestamp: 1650221275\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 19:48:41 (running for 00:50:01.61)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.29 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-51-17\n",
      "  done: false\n",
      "  episode_len_mean: 1872.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1333333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 30\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.220703143189894e-05\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128029346466064\n",
      "          total_loss: 0.016146788373589516\n",
      "          vf_explained_var: -1.495884305313666e-07\n",
      "          vf_loss: 0.0020187615882605314\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.97725631768952\n",
      "    ram_util_percent: 95.70108303249097\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0826511402761713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8253795100276708\n",
      "    mean_inference_ms: 1.4681058567724141\n",
      "    mean_raw_obs_processing_ms: 0.10647598775951214\n",
      "  time_since_restore: 3139.175045490265\n",
      "  time_this_iter_s: 208.1674199104309\n",
      "  time_total_s: 3139.175045490265\n",
      "  timers:\n",
      "    learn_throughput: 20.246\n",
      "    learn_time_ms: 197571.32\n",
      "    load_throughput: 7056662.881\n",
      "    load_time_ms: 0.567\n",
      "    sample_throughput: 19.101\n",
      "    sample_time_ms: 209411.004\n",
      "    update_time_ms: 14.705\n",
      "  timestamp: 1650221477\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-51-24\n",
      "  done: false\n",
      "  episode_len_mean: 236.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.25\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 280\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4471455216407776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003587853629142046\n",
      "          model: {}\n",
      "          policy_loss: -0.009571489877998829\n",
      "          total_loss: 0.01662680320441723\n",
      "          vf_explained_var: 0.2933219373226166\n",
      "          vf_loss: 0.026181479915976524\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.02805755395684\n",
      "    ram_util_percent: 95.7021582733813\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08748276348684766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8767625002822763\n",
      "    mean_inference_ms: 1.645596624940465\n",
      "    mean_raw_obs_processing_ms: 0.21218389267937962\n",
      "  time_since_restore: 3146.043712377548\n",
      "  time_this_iter_s: 208.8572700023651\n",
      "  time_total_s: 3146.043712377548\n",
      "  timers:\n",
      "    learn_throughput: 20.271\n",
      "    learn_time_ms: 197330.369\n",
      "    load_throughput: 4248256.862\n",
      "    load_time_ms: 0.942\n",
      "    sample_throughput: 19.083\n",
      "    sample_time_ms: 209611.918\n",
      "    update_time_ms: 9.847\n",
      "  timestamp: 1650221484\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-54-45\n",
      "  done: false\n",
      "  episode_len_mean: 1872.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1333333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 30\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.10351571594947e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014127976261079311\n",
      "          total_loss: 0.014909575693309307\n",
      "          vf_explained_var: -2.984077696055465e-07\n",
      "          vf_loss: 0.000781601294875145\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.5518115942029\n",
      "    ram_util_percent: 95.8427536231884\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0826511402761713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8253795100276708\n",
      "    mean_inference_ms: 1.4681058567724141\n",
      "    mean_raw_obs_processing_ms: 0.10647598775951214\n",
      "  time_since_restore: 3346.4198405742645\n",
      "  time_this_iter_s: 207.24479508399963\n",
      "  time_total_s: 3346.4198405742645\n",
      "  timers:\n",
      "    learn_throughput: 20.281\n",
      "    learn_time_ms: 197227.466\n",
      "    load_throughput: 6973073.982\n",
      "    load_time_ms: 0.574\n",
      "    sample_throughput: 19.102\n",
      "    sample_time_ms: 209401.256\n",
      "    update_time_ms: 13.53\n",
      "  timestamp: 1650221685\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-54-52\n",
      "  done: false\n",
      "  episode_len_mean: 236.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 300\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.39844393730163574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004063639789819717\n",
      "          model: {}\n",
      "          policy_loss: -0.0008334371377713978\n",
      "          total_loss: 0.013632015325129032\n",
      "          vf_explained_var: 0.4946739375591278\n",
      "          vf_loss: 0.014455928467214108\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.63176895306859\n",
      "    ram_util_percent: 95.85559566787002\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08794562096648782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8800443201320403\n",
      "    mean_inference_ms: 1.6602099907674943\n",
      "    mean_raw_obs_processing_ms: 0.2129377149112717\n",
      "  time_since_restore: 3354.320836544037\n",
      "  time_this_iter_s: 208.27712416648865\n",
      "  time_total_s: 3354.320836544037\n",
      "  timers:\n",
      "    learn_throughput: 20.301\n",
      "    learn_time_ms: 197031.393\n",
      "    load_throughput: 4255476.474\n",
      "    load_time_ms: 0.94\n",
      "    sample_throughput: 19.077\n",
      "    sample_time_ms: 209678.891\n",
      "    update_time_ms: 9.825\n",
      "  timestamp: 1650221692\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-58-22\n",
      "  done: false\n",
      "  episode_len_mean: 1924.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 35\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.051757857974735e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.682402647871603e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.1589129638406652e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0007094790926203132\n",
      "          total_loss: 0.0287373885512352\n",
      "          vf_explained_var: 0.08472982794046402\n",
      "          vf_loss: 0.02944687008857727\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.99792387543253\n",
      "    ram_util_percent: 95.90519031141868\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08333783217173245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8310776556356575\n",
      "    mean_inference_ms: 1.4958213531273092\n",
      "    mean_raw_obs_processing_ms: 0.10718724675592356\n",
      "  time_since_restore: 3564.0526666641235\n",
      "  time_this_iter_s: 217.632826089859\n",
      "  time_total_s: 3564.0526666641235\n",
      "  timers:\n",
      "    learn_throughput: 20.208\n",
      "    learn_time_ms: 197945.853\n",
      "    load_throughput: 6965835.998\n",
      "    load_time_ms: 0.574\n",
      "    sample_throughput: 19.117\n",
      "    sample_time_ms: 209241.844\n",
      "    update_time_ms: 14.232\n",
      "  timestamp: 1650221902\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_19-58-29\n",
      "  done: false\n",
      "  episode_len_mean: 236.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 320\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4285612106323242\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004472605884075165\n",
      "          model: {}\n",
      "          policy_loss: -0.008164044469594955\n",
      "          total_loss: 0.009782559238374233\n",
      "          vf_explained_var: 0.4328242838382721\n",
      "          vf_loss: 0.017941361293196678\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.3923875432526\n",
      "    ram_util_percent: 95.88408304498269\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840870608427938\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8830993165055037\n",
      "    mean_inference_ms: 1.6764684502321934\n",
      "    mean_raw_obs_processing_ms: 0.2137972074638803\n",
      "  time_since_restore: 3571.509085416794\n",
      "  time_this_iter_s: 217.18824887275696\n",
      "  time_total_s: 3571.509085416794\n",
      "  timers:\n",
      "    learn_throughput: 20.237\n",
      "    learn_time_ms: 197660.811\n",
      "    load_throughput: 4146105.523\n",
      "    load_time_ms: 0.965\n",
      "    sample_throughput: 19.088\n",
      "    sample_time_ms: 209553.486\n",
      "    update_time_ms: 9.959\n",
      "  timestamp: 1650221909\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 1924.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 35\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5258789289873675e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128098264336586\n",
      "          total_loss: -0.013390676118433475\n",
      "          vf_explained_var: -4.005688367669791e-07\n",
      "          vf_loss: 0.0007374253473244607\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.02474226804122\n",
      "    ram_util_percent: 95.92714776632303\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08333783217173245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8310776556356575\n",
      "    mean_inference_ms: 1.4958213531273092\n",
      "    mean_raw_obs_processing_ms: 0.10718724675592356\n",
      "  time_since_restore: 3782.7462165355682\n",
      "  time_this_iter_s: 218.6935498714447\n",
      "  time_total_s: 3782.7462165355682\n",
      "  timers:\n",
      "    learn_throughput: 20.121\n",
      "    learn_time_ms: 198796.834\n",
      "    load_throughput: 6917583.804\n",
      "    load_time_ms: 0.578\n",
      "    sample_throughput: 19.037\n",
      "    sample_time_ms: 210118.176\n",
      "    update_time_ms: 14.54\n",
      "  timestamp: 1650222121\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 240.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 338\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.41147899627685547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004310941323637962\n",
      "          model: {}\n",
      "          policy_loss: -0.01635880582034588\n",
      "          total_loss: -0.0024940152652561665\n",
      "          vf_explained_var: 0.5644102692604065\n",
      "          vf_loss: 0.013862265273928642\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.09041095890412\n",
      "    ram_util_percent: 95.91678082191783\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08886698322000391\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8860648032308512\n",
      "    mean_inference_ms: 1.6912402374797484\n",
      "    mean_raw_obs_processing_ms: 0.2147421002110538\n",
      "  time_since_restore: 3790.840919494629\n",
      "  time_this_iter_s: 219.33183407783508\n",
      "  time_total_s: 3790.840919494629\n",
      "  timers:\n",
      "    learn_throughput: 20.146\n",
      "    learn_time_ms: 198553.242\n",
      "    load_throughput: 4113776.819\n",
      "    load_time_ms: 0.972\n",
      "    sample_throughput: 19.017\n",
      "    sample_time_ms: 210333.129\n",
      "    update_time_ms: 10.078\n",
      "  timestamp: 1650222129\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 20:05:21 (running for 01:06:41.67)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.29 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-05-40\n",
      "  done: false\n",
      "  episode_len_mean: 1924.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 35\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.629394644936838e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128127135336399\n",
      "          total_loss: -0.013829635456204414\n",
      "          vf_explained_var: -2.5463361907895887e-07\n",
      "          vf_loss: 0.0002984911552630365\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.88698630136986\n",
      "    ram_util_percent: 95.87979452054795\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08333783217173245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8310776556356575\n",
      "    mean_inference_ms: 1.4958213531273092\n",
      "    mean_raw_obs_processing_ms: 0.10718724675592356\n",
      "  time_since_restore: 4001.325402736664\n",
      "  time_this_iter_s: 218.57918620109558\n",
      "  time_total_s: 4001.325402736664\n",
      "  timers:\n",
      "    learn_throughput: 20.043\n",
      "    learn_time_ms: 199569.849\n",
      "    load_throughput: 6786625.137\n",
      "    load_time_ms: 0.589\n",
      "    sample_throughput: 18.946\n",
      "    sample_time_ms: 211125.071\n",
      "    update_time_ms: 15.927\n",
      "  timestamp: 1650222340\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-05-47\n",
      "  done: false\n",
      "  episode_len_mean: 238.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 357\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0002929687616415322\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4512212872505188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037037821020931005\n",
      "          model: {}\n",
      "          policy_loss: -0.008971933275461197\n",
      "          total_loss: 0.012234928086400032\n",
      "          vf_explained_var: 0.30212974548339844\n",
      "          vf_loss: 0.021205777302384377\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.21643835616437\n",
      "    ram_util_percent: 95.83424657534248\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08938775401660315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8891965830300483\n",
      "    mean_inference_ms: 1.7072311695819922\n",
      "    mean_raw_obs_processing_ms: 0.2159706197184796\n",
      "  time_since_restore: 4009.501730442047\n",
      "  time_this_iter_s: 218.6608109474182\n",
      "  time_total_s: 4009.501730442047\n",
      "  timers:\n",
      "    learn_throughput: 20.077\n",
      "    learn_time_ms: 199231.52\n",
      "    load_throughput: 4048946.81\n",
      "    load_time_ms: 0.988\n",
      "    sample_throughput: 18.919\n",
      "    sample_time_ms: 211426.67\n",
      "    update_time_ms: 10.027\n",
      "  timestamp: 1650222347\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-09-17\n",
      "  done: false\n",
      "  episode_len_mean: 1838.953488372093\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1627906976744187\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 43\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.814697322468419e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0666978028776606e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.117427855059581e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0007762943278066814\n",
      "          total_loss: 0.024148914963006973\n",
      "          vf_explained_var: -0.07825837284326553\n",
      "          vf_loss: 0.023372618481516838\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.0993103448276\n",
      "    ram_util_percent: 95.63310344827588\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08444300170868489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8392669505602796\n",
      "    mean_inference_ms: 1.5365673248852307\n",
      "    mean_raw_obs_processing_ms: 0.1085737385874975\n",
      "  time_since_restore: 4218.5245525836945\n",
      "  time_this_iter_s: 217.19914984703064\n",
      "  time_total_s: 4218.5245525836945\n",
      "  timers:\n",
      "    learn_throughput: 19.986\n",
      "    learn_time_ms: 200137.888\n",
      "    load_throughput: 6935886.56\n",
      "    load_time_ms: 0.577\n",
      "    sample_throughput: 18.863\n",
      "    sample_time_ms: 212057.487\n",
      "    update_time_ms: 16.031\n",
      "  timestamp: 1650222557\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-09-25\n",
      "  done: false\n",
      "  episode_len_mean: 240.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 375\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.49288591742515564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012070715427398682\n",
      "          model: {}\n",
      "          policy_loss: -0.009590130299329758\n",
      "          total_loss: 0.04665760695934296\n",
      "          vf_explained_var: 0.43469226360321045\n",
      "          vf_loss: 0.05624597519636154\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.1319587628866\n",
      "    ram_util_percent: 95.64570446735395\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08999318698491159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.892287140656297\n",
      "    mean_inference_ms: 1.7221323756039268\n",
      "    mean_raw_obs_processing_ms: 0.21708567364728673\n",
      "  time_since_restore: 4226.820608377457\n",
      "  time_this_iter_s: 217.31887793540955\n",
      "  time_total_s: 4226.820608377457\n",
      "  timers:\n",
      "    learn_throughput: 20.022\n",
      "    learn_time_ms: 199784.555\n",
      "    load_throughput: 3739822.117\n",
      "    load_time_ms: 1.07\n",
      "    sample_throughput: 18.848\n",
      "    sample_time_ms: 212226.524\n",
      "    update_time_ms: 8.202\n",
      "  timestamp: 1650222565\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-12-54\n",
      "  done: false\n",
      "  episode_len_mean: 1838.953488372093\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1627906976744187\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 43\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9073486612342094e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412825845181942\n",
      "          total_loss: 0.014332721941173077\n",
      "          vf_explained_var: 6.805183829783346e-07\n",
      "          vf_loss: 0.00020445742120500654\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.62569444444445\n",
      "    ram_util_percent: 95.64930555555556\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08444300170868489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8392669505602796\n",
      "    mean_inference_ms: 1.5365673248852307\n",
      "    mean_raw_obs_processing_ms: 0.1085737385874975\n",
      "  time_since_restore: 4435.578573703766\n",
      "  time_this_iter_s: 217.0540211200714\n",
      "  time_total_s: 4435.578573703766\n",
      "  timers:\n",
      "    learn_throughput: 19.917\n",
      "    learn_time_ms: 200836.459\n",
      "    load_throughput: 6978294.651\n",
      "    load_time_ms: 0.573\n",
      "    sample_throughput: 18.802\n",
      "    sample_time_ms: 212746.833\n",
      "    update_time_ms: 16.614\n",
      "  timestamp: 1650222774\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 239.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 394\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4622003436088562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007927675731480122\n",
      "          model: {}\n",
      "          policy_loss: -0.012602384202182293\n",
      "          total_loss: 0.03445208817720413\n",
      "          vf_explained_var: 0.1175915002822876\n",
      "          vf_loss: 0.04705331474542618\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.16206896551724\n",
      "    ram_util_percent: 95.64448275862071\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907123454019316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8957336315613253\n",
      "    mean_inference_ms: 1.7379432010870597\n",
      "    mean_raw_obs_processing_ms: 0.21829582652168028\n",
      "  time_since_restore: 4444.94820022583\n",
      "  time_this_iter_s: 218.1275918483734\n",
      "  time_total_s: 4444.94820022583\n",
      "  timers:\n",
      "    learn_throughput: 19.939\n",
      "    learn_time_ms: 200610.678\n",
      "    load_throughput: 3780440.298\n",
      "    load_time_ms: 1.058\n",
      "    sample_throughput: 18.788\n",
      "    sample_time_ms: 212896.608\n",
      "    update_time_ms: 8.162\n",
      "  timestamp: 1650222783\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-16-32\n",
      "  done: false\n",
      "  episode_len_mean: 2024.4318181818182\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1363636363636365\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.536743306171047e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.741897454757092e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.6403135979148377e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0032356318552047014\n",
      "          total_loss: 0.0033045602031052113\n",
      "          vf_explained_var: -0.03225596621632576\n",
      "          vf_loss: 6.892864621477202e-05\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.55689655172414\n",
      "    ram_util_percent: 95.63517241379311\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08459174441157134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8402568708505407\n",
      "    mean_inference_ms: 1.5414089543730674\n",
      "    mean_raw_obs_processing_ms: 0.10871821566112164\n",
      "  time_since_restore: 4653.204869747162\n",
      "  time_this_iter_s: 217.626296043396\n",
      "  time_total_s: 4653.204869747162\n",
      "  timers:\n",
      "    learn_throughput: 19.855\n",
      "    learn_time_ms: 201464.58\n",
      "    load_throughput: 7007734.013\n",
      "    load_time_ms: 0.571\n",
      "    sample_throughput: 18.721\n",
      "    sample_time_ms: 213658.862\n",
      "    update_time_ms: 17.2\n",
      "  timestamp: 1650222992\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-16-40\n",
      "  done: false\n",
      "  episode_len_mean: 240.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 414\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.40345147252082825\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004605127964168787\n",
      "          model: {}\n",
      "          policy_loss: -0.009226405061781406\n",
      "          total_loss: 0.0037914810236543417\n",
      "          vf_explained_var: 0.28081169724464417\n",
      "          vf_loss: 0.013017211109399796\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.58689655172414\n",
      "    ram_util_percent: 95.62689655172414\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09151271706597779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8996520359103966\n",
      "    mean_inference_ms: 1.7545651151698558\n",
      "    mean_raw_obs_processing_ms: 0.21957454576878171\n",
      "  time_since_restore: 4662.085133552551\n",
      "  time_this_iter_s: 217.1369333267212\n",
      "  time_total_s: 4662.085133552551\n",
      "  timers:\n",
      "    learn_throughput: 19.879\n",
      "    learn_time_ms: 201214.231\n",
      "    load_throughput: 5213715.777\n",
      "    load_time_ms: 0.767\n",
      "    sample_throughput: 18.696\n",
      "    sample_time_ms: 213949.429\n",
      "    update_time_ms: 6.988\n",
      "  timestamp: 1650223000\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-20-12\n",
      "  done: false\n",
      "  episode_len_mean: 2024.4318181818182\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1363636363636365\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 44\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.7683716530855236e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128202572464943\n",
      "          total_loss: 0.01413037534803152\n",
      "          vf_explained_var: 1.755722041707486e-05\n",
      "          vf_loss: 2.1673929495591437e-06\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.12439862542955\n",
      "    ram_util_percent: 95.91065292096219\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08459174441157134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8402568708505407\n",
      "    mean_inference_ms: 1.5414089543730674\n",
      "    mean_raw_obs_processing_ms: 0.10871821566112164\n",
      "  time_since_restore: 4872.64767575264\n",
      "  time_this_iter_s: 219.4428060054779\n",
      "  time_total_s: 4872.64767575264\n",
      "  timers:\n",
      "    learn_throughput: 19.78\n",
      "    learn_time_ms: 202224.451\n",
      "    load_throughput: 6869714.192\n",
      "    load_time_ms: 0.582\n",
      "    sample_throughput: 18.651\n",
      "    sample_time_ms: 214466.549\n",
      "    update_time_ms: 18.436\n",
      "  timestamp: 1650223212\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-20-20\n",
      "  done: false\n",
      "  episode_len_mean: 240.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 433\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.37078824639320374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004277229309082031\n",
      "          model: {}\n",
      "          policy_loss: -0.011956365779042244\n",
      "          total_loss: -0.001817096839658916\n",
      "          vf_explained_var: 0.49144071340560913\n",
      "          vf_loss: 0.01013895682990551\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.21986301369863\n",
      "    ram_util_percent: 95.91883561643836\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09230777358300149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9035775650085466\n",
      "    mean_inference_ms: 1.7719959530339744\n",
      "    mean_raw_obs_processing_ms: 0.2205962193200309\n",
      "  time_since_restore: 4882.308479309082\n",
      "  time_this_iter_s: 220.22334575653076\n",
      "  time_total_s: 4882.308479309082\n",
      "  timers:\n",
      "    learn_throughput: 19.816\n",
      "    learn_time_ms: 201861.955\n",
      "    load_throughput: 5842055.853\n",
      "    load_time_ms: 0.685\n",
      "    sample_throughput: 18.622\n",
      "    sample_time_ms: 214803.263\n",
      "    update_time_ms: 6.672\n",
      "  timestamp: 1650223220\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 20:22:02 (running for 01:23:21.92)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.38 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-23-48\n",
      "  done: false\n",
      "  episode_len_mean: 2024.4318181818182\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1363636363636365\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 44\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3841858265427618e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014127985574305058\n",
      "          total_loss: 0.014129670336842537\n",
      "          vf_explained_var: 3.7852216337341815e-05\n",
      "          vf_loss: 1.6847171764311497e-06\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.28885017421604\n",
      "    ram_util_percent: 95.74843205574913\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08459174441157134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8402568708505407\n",
      "    mean_inference_ms: 1.5414089543730674\n",
      "    mean_raw_obs_processing_ms: 0.10871821566112164\n",
      "  time_since_restore: 5089.032785892487\n",
      "  time_this_iter_s: 216.3851101398468\n",
      "  time_total_s: 5089.032785892487\n",
      "  timers:\n",
      "    learn_throughput: 19.717\n",
      "    learn_time_ms: 202869.895\n",
      "    load_throughput: 7029461.6\n",
      "    load_time_ms: 0.569\n",
      "    sample_throughput: 18.584\n",
      "    sample_time_ms: 215241.519\n",
      "    update_time_ms: 19.006\n",
      "  timestamp: 1650223428\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-23-57\n",
      "  done: false\n",
      "  episode_len_mean: 238.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 452\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109520519152e-05\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.38071006536483765\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033838627859950066\n",
      "          model: {}\n",
      "          policy_loss: -0.01066039502620697\n",
      "          total_loss: 0.010650712065398693\n",
      "          vf_explained_var: 0.5531356334686279\n",
      "          vf_loss: 0.02131098322570324\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.31666666666666\n",
      "    ram_util_percent: 95.77604166666667\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09309222655404106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9075597859005964\n",
      "    mean_inference_ms: 1.7882846764105347\n",
      "    mean_raw_obs_processing_ms: 0.22145031077264488\n",
      "  time_since_restore: 5099.242604494095\n",
      "  time_this_iter_s: 216.93412518501282\n",
      "  time_total_s: 5099.242604494095\n",
      "  timers:\n",
      "    learn_throughput: 19.75\n",
      "    learn_time_ms: 202526.842\n",
      "    load_throughput: 5412005.161\n",
      "    load_time_ms: 0.739\n",
      "    sample_throughput: 18.555\n",
      "    sample_time_ms: 215579.78\n",
      "    update_time_ms: 6.402\n",
      "  timestamp: 1650223437\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-27-25\n",
      "  done: false\n",
      "  episode_len_mean: 2120.1702127659573\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.148936170212766\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 47\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1920929132713809e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0140050407648403e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -6.821597640700924e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0010417028097435832\n",
      "          total_loss: 0.00902296882122755\n",
      "          vf_explained_var: -0.011343516409397125\n",
      "          vf_loss: 0.01006467267870903\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.12110726643598\n",
      "    ram_util_percent: 95.66366782006921\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08511368641389688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8435862068074869\n",
      "    mean_inference_ms: 1.5577918987586483\n",
      "    mean_raw_obs_processing_ms: 0.1091766978140063\n",
      "  time_since_restore: 5305.757925033569\n",
      "  time_this_iter_s: 216.72513914108276\n",
      "  time_total_s: 5305.757925033569\n",
      "  timers:\n",
      "    learn_throughput: 19.658\n",
      "    learn_time_ms: 203479.796\n",
      "    load_throughput: 6958324.416\n",
      "    load_time_ms: 0.575\n",
      "    sample_throughput: 18.506\n",
      "    sample_time_ms: 216142.257\n",
      "    update_time_ms: 18.84\n",
      "  timestamp: 1650223645\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-27-34\n",
      "  done: false\n",
      "  episode_len_mean: 238.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 471\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.831054760259576e-05\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.438613623380661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004513404332101345\n",
      "          model: {}\n",
      "          policy_loss: 0.001096780295483768\n",
      "          total_loss: 0.01618606597185135\n",
      "          vf_explained_var: 0.7966788411140442\n",
      "          vf_loss: 0.015089203603565693\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.17569444444445\n",
      "    ram_util_percent: 95.684375\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09381097682682814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9115577987906281\n",
      "    mean_inference_ms: 1.8048636862619412\n",
      "    mean_raw_obs_processing_ms: 0.22236105670648904\n",
      "  time_since_restore: 5315.69632267952\n",
      "  time_this_iter_s: 216.4537181854248\n",
      "  time_total_s: 5315.69632267952\n",
      "  timers:\n",
      "    learn_throughput: 19.697\n",
      "    learn_time_ms: 203077.504\n",
      "    load_throughput: 5367678.526\n",
      "    load_time_ms: 0.745\n",
      "    sample_throughput: 18.48\n",
      "    sample_time_ms: 216456.063\n",
      "    update_time_ms: 6.486\n",
      "  timestamp: 1650223654\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-31-02\n",
      "  done: false\n",
      "  episode_len_mean: 2120.1702127659573\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.148936170212766\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 47\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.9604645663569045e-09\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.42071992518739e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.3927169799841857e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128126204013824\n",
      "          total_loss: 0.014513684436678886\n",
      "          vf_explained_var: -3.382723718914349e-07\n",
      "          vf_loss: 0.0003855627728626132\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.99198606271777\n",
      "    ram_util_percent: 95.68919860627177\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08511368641389688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8435862068074869\n",
      "    mean_inference_ms: 1.5577918987586483\n",
      "    mean_raw_obs_processing_ms: 0.1091766978140063\n",
      "  time_since_restore: 5522.3770270347595\n",
      "  time_this_iter_s: 216.61910200119019\n",
      "  time_total_s: 5522.3770270347595\n",
      "  timers:\n",
      "    learn_throughput: 19.583\n",
      "    learn_time_ms: 204258.493\n",
      "    load_throughput: 7043923.083\n",
      "    load_time_ms: 0.568\n",
      "    sample_throughput: 18.44\n",
      "    sample_time_ms: 216913.944\n",
      "    update_time_ms: 19.088\n",
      "  timestamp: 1650223862\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 235.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 491\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4442881643772125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005941014736890793\n",
      "          model: {}\n",
      "          policy_loss: -0.015180878341197968\n",
      "          total_loss: -0.00302065908908844\n",
      "          vf_explained_var: 0.5013502836227417\n",
      "          vf_loss: 0.01216016337275505\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.08611111111112\n",
      "    ram_util_percent: 95.71631944444445\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09447583995533929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9156716205149279\n",
      "    mean_inference_ms: 1.8223440780018547\n",
      "    mean_raw_obs_processing_ms: 0.22340911371271502\n",
      "  time_since_restore: 5532.951072692871\n",
      "  time_this_iter_s: 217.25475001335144\n",
      "  time_total_s: 5532.951072692871\n",
      "  timers:\n",
      "    learn_throughput: 19.627\n",
      "    learn_time_ms: 203800.517\n",
      "    load_throughput: 5341871.557\n",
      "    load_time_ms: 0.749\n",
      "    sample_throughput: 18.418\n",
      "    sample_time_ms: 217179.961\n",
      "    update_time_ms: 6.431\n",
      "  timestamp: 1650223871\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-34-40\n",
      "  done: false\n",
      "  episode_len_mean: 2120.1702127659573\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.148936170212766\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 47\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.9802322831784522e-09\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.42071992518739e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.3927169799841857e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128059148788452\n",
      "          total_loss: 0.014191527850925922\n",
      "          vf_explained_var: -1.4477519698630203e-06\n",
      "          vf_loss: 6.347271846607327e-05\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.47620689655172\n",
      "    ram_util_percent: 95.76965517241379\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08511368641389688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8435862068074869\n",
      "    mean_inference_ms: 1.5577918987586483\n",
      "    mean_raw_obs_processing_ms: 0.1091766978140063\n",
      "  time_since_restore: 5740.29638504982\n",
      "  time_this_iter_s: 217.91935801506042\n",
      "  time_total_s: 5740.29638504982\n",
      "  timers:\n",
      "    learn_throughput: 19.593\n",
      "    learn_time_ms: 204158.533\n",
      "    load_throughput: 6833061.54\n",
      "    load_time_ms: 0.585\n",
      "    sample_throughput: 18.364\n",
      "    sample_time_ms: 217823.251\n",
      "    update_time_ms: 18.693\n",
      "  timestamp: 1650224080\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-34-49\n",
      "  done: false\n",
      "  episode_len_mean: 240.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.33\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 509\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4361616373062134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008581685833632946\n",
      "          model: {}\n",
      "          policy_loss: -0.00067079474683851\n",
      "          total_loss: 0.013070518150925636\n",
      "          vf_explained_var: 0.3481530547142029\n",
      "          vf_loss: 0.013741235248744488\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.93780068728523\n",
      "    ram_util_percent: 95.78522336769758\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09505927640656932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9193005358236322\n",
      "    mean_inference_ms: 1.8379892486835143\n",
      "    mean_raw_obs_processing_ms: 0.2243376640585167\n",
      "  time_since_restore: 5751.250287532806\n",
      "  time_this_iter_s: 218.2992148399353\n",
      "  time_total_s: 5751.250287532806\n",
      "  timers:\n",
      "    learn_throughput: 19.633\n",
      "    learn_time_ms: 203741.778\n",
      "    load_throughput: 5050640.014\n",
      "    load_time_ms: 0.792\n",
      "    sample_throughput: 18.342\n",
      "    sample_time_ms: 218073.536\n",
      "    update_time_ms: 6.497\n",
      "  timestamp: 1650224089\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-38-15\n",
      "  done: false\n",
      "  episode_len_mean: 2093.811320754717\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.150943396226415\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 53\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4901161415892261e-09\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1083541661614336e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.2282393271712092e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0038090720772743225\n",
      "          total_loss: 0.015588512644171715\n",
      "          vf_explained_var: 0.04972337186336517\n",
      "          vf_loss: 0.019397588446736336\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.98124999999999\n",
      "    ram_util_percent: 95.70243055555554\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08617575997412343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8501802456673799\n",
      "    mean_inference_ms: 1.5900304880619076\n",
      "    mean_raw_obs_processing_ms: 0.11017374767882879\n",
      "  time_since_restore: 5955.235893249512\n",
      "  time_this_iter_s: 214.93950819969177\n",
      "  time_total_s: 5955.235893249512\n",
      "  timers:\n",
      "    learn_throughput: 19.643\n",
      "    learn_time_ms: 203633.499\n",
      "    load_throughput: 6863531.337\n",
      "    load_time_ms: 0.583\n",
      "    sample_throughput: 18.359\n",
      "    sample_time_ms: 217881.434\n",
      "    update_time_ms: 18.727\n",
      "  timestamp: 1650224295\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-38-25\n",
      "  done: false\n",
      "  episode_len_mean: 241.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 528\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.46228572726249695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008833395317196846\n",
      "          model: {}\n",
      "          policy_loss: -0.00712171383202076\n",
      "          total_loss: 0.02608870342373848\n",
      "          vf_explained_var: 0.5312267541885376\n",
      "          vf_loss: 0.03321033716201782\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.24166666666667\n",
      "    ram_util_percent: 95.7315972222222\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09561213607777672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9229571418151653\n",
      "    mean_inference_ms: 1.8529653149612237\n",
      "    mean_raw_obs_processing_ms: 0.22534052469903162\n",
      "  time_since_restore: 5966.6249186992645\n",
      "  time_this_iter_s: 215.37463116645813\n",
      "  time_total_s: 5966.6249186992645\n",
      "  timers:\n",
      "    learn_throughput: 19.683\n",
      "    learn_time_ms: 203223.64\n",
      "    load_throughput: 4953561.074\n",
      "    load_time_ms: 0.807\n",
      "    sample_throughput: 18.337\n",
      "    sample_time_ms: 218137.615\n",
      "    update_time_ms: 6.711\n",
      "  timestamp: 1650224305\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 20:38:42 (running for 01:40:02.31)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.31 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-41-54\n",
      "  done: false\n",
      "  episode_len_mean: 2093.811320754717\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.150943396226415\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 53\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.450580707946131e-10\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0326540861687897e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.689175684472504e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128250069916248\n",
      "          total_loss: -0.014011407271027565\n",
      "          vf_explained_var: -8.976587650977308e-07\n",
      "          vf_loss: 0.00011684624041663483\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.49212328767122\n",
      "    ram_util_percent: 95.69109589041096\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08617575997412343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8501802456673799\n",
      "    mean_inference_ms: 1.5900304880619076\n",
      "    mean_raw_obs_processing_ms: 0.11017374767882879\n",
      "  time_since_restore: 6174.233305215836\n",
      "  time_this_iter_s: 218.99741196632385\n",
      "  time_total_s: 6174.233305215836\n",
      "  timers:\n",
      "    learn_throughput: 19.657\n",
      "    learn_time_ms: 203487.94\n",
      "    load_throughput: 6890027.105\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 18.388\n",
      "    sample_time_ms: 217535.671\n",
      "    update_time_ms: 18.328\n",
      "  timestamp: 1650224514\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 239.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 547\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4466399550437927\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00418564910069108\n",
      "          model: {}\n",
      "          policy_loss: -0.0421772301197052\n",
      "          total_loss: -0.033545881509780884\n",
      "          vf_explained_var: 0.5258854031562805\n",
      "          vf_loss: 0.008631310425698757\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.01092150170648\n",
      "    ram_util_percent: 95.68259385665529\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09609030713523069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9262496709152646\n",
      "    mean_inference_ms: 1.8679271890666\n",
      "    mean_raw_obs_processing_ms: 0.22624307356150816\n",
      "  time_since_restore: 6185.567257881165\n",
      "  time_this_iter_s: 218.94233918190002\n",
      "  time_total_s: 6185.567257881165\n",
      "  timers:\n",
      "    learn_throughput: 19.691\n",
      "    learn_time_ms: 203143.156\n",
      "    load_throughput: 5028237.128\n",
      "    load_time_ms: 0.796\n",
      "    sample_throughput: 18.372\n",
      "    sample_time_ms: 217721.733\n",
      "    update_time_ms: 7.259\n",
      "  timestamp: 1650224524\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-45-29\n",
      "  done: false\n",
      "  episode_len_mean: 2131.9298245614036\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.0701754385964912\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 57\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7252903539730653e-10\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0554262391854038e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.4867156116683002e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.017856065183877945\n",
      "          total_loss: -0.016604142263531685\n",
      "          vf_explained_var: -0.13236725330352783\n",
      "          vf_loss: 0.0012519288575276732\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.01805555555555\n",
      "    ram_util_percent: 95.25937499999999\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08682117981507194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8542640818693646\n",
      "    mean_inference_ms: 1.610045294207466\n",
      "    mean_raw_obs_processing_ms: 0.11079008482898167\n",
      "  time_since_restore: 6389.030276298523\n",
      "  time_this_iter_s: 214.79697108268738\n",
      "  time_total_s: 6389.030276298523\n",
      "  timers:\n",
      "    learn_throughput: 19.692\n",
      "    learn_time_ms: 203127.046\n",
      "    load_throughput: 6660797.205\n",
      "    load_time_ms: 0.601\n",
      "    sample_throughput: 18.39\n",
      "    sample_time_ms: 217511.78\n",
      "    update_time_ms: 18.025\n",
      "  timestamp: 1650224729\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-45-39\n",
      "  done: false\n",
      "  episode_len_mean: 239.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 567\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.57763690064894e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5144514441490173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007793267257511616\n",
      "          model: {}\n",
      "          policy_loss: -0.011479570530354977\n",
      "          total_loss: 0.028072228655219078\n",
      "          vf_explained_var: 0.4006885886192322\n",
      "          vf_loss: 0.0395517572760582\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.5418118466899\n",
      "    ram_util_percent: 95.26968641114983\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09661013209380316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9296816965005698\n",
      "    mean_inference_ms: 1.882490589504772\n",
      "    mean_raw_obs_processing_ms: 0.22715034478289917\n",
      "  time_since_restore: 6400.315926790237\n",
      "  time_this_iter_s: 214.74866890907288\n",
      "  time_total_s: 6400.315926790237\n",
      "  timers:\n",
      "    learn_throughput: 19.728\n",
      "    learn_time_ms: 202755.27\n",
      "    load_throughput: 3438024.55\n",
      "    load_time_ms: 1.163\n",
      "    sample_throughput: 18.368\n",
      "    sample_time_ms: 217772.197\n",
      "    update_time_ms: 7.286\n",
      "  timestamp: 1650224739\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-49-09\n",
      "  done: false\n",
      "  episode_len_mean: 2008.5409836065573\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.0819672131147542\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 61\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8626451769865326e-10\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0495408173926116e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.8822217346713453e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.000760021386668086\n",
      "          total_loss: 0.009175428189337254\n",
      "          vf_explained_var: -0.055190254002809525\n",
      "          vf_loss: 0.009935448877513409\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.14493243243244\n",
      "    ram_util_percent: 95.39763513513513\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08741243377528894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8579469302175748\n",
      "    mean_inference_ms: 1.6282668371551383\n",
      "    mean_raw_obs_processing_ms: 0.11138514375631224\n",
      "  time_since_restore: 6609.63605427742\n",
      "  time_this_iter_s: 220.6057779788971\n",
      "  time_total_s: 6609.63605427742\n",
      "  timers:\n",
      "    learn_throughput: 19.67\n",
      "    learn_time_ms: 203357.658\n",
      "    load_throughput: 6584206.271\n",
      "    load_time_ms: 0.608\n",
      "    sample_throughput: 18.41\n",
      "    sample_time_ms: 217274.231\n",
      "    update_time_ms: 17.575\n",
      "  timestamp: 1650224949\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-49-20\n",
      "  done: false\n",
      "  episode_len_mean: 240.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 585\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.57763690064894e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.43262553215026855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037784313317388296\n",
      "          model: {}\n",
      "          policy_loss: -0.011487139388918877\n",
      "          total_loss: 0.009507105685770512\n",
      "          vf_explained_var: 0.5378456711769104\n",
      "          vf_loss: 0.02099423110485077\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.52094594594595\n",
      "    ram_util_percent: 95.42567567567568\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09704068556723426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9324203865611906\n",
      "    mean_inference_ms: 1.894325898533106\n",
      "    mean_raw_obs_processing_ms: 0.22783906388262865\n",
      "  time_since_restore: 6621.487445116043\n",
      "  time_this_iter_s: 221.17151832580566\n",
      "  time_total_s: 6621.487445116043\n",
      "  timers:\n",
      "    learn_throughput: 19.704\n",
      "    learn_time_ms: 203004.305\n",
      "    load_throughput: 3441056.69\n",
      "    load_time_ms: 1.162\n",
      "    sample_throughput: 18.396\n",
      "    sample_time_ms: 217437.861\n",
      "    update_time_ms: 7.334\n",
      "  timestamp: 1650224960\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-52-54\n",
      "  done: false\n",
      "  episode_len_mean: 2008.5409836065573\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.0819672131147542\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 61\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.313225884932663e-11\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412802841514349\n",
      "          total_loss: 0.014217372983694077\n",
      "          vf_explained_var: 1.945803234093546e-07\n",
      "          vf_loss: 8.934453217079863e-05\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.6887417218543\n",
      "    ram_util_percent: 95.39834437086093\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08741243377528894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8579469302175748\n",
      "    mean_inference_ms: 1.6282668371551383\n",
      "    mean_raw_obs_processing_ms: 0.11138514375631224\n",
      "  time_since_restore: 6833.984898328781\n",
      "  time_this_iter_s: 224.34884405136108\n",
      "  time_total_s: 6833.984898328781\n",
      "  timers:\n",
      "    learn_throughput: 19.618\n",
      "    learn_time_ms: 203890.763\n",
      "    load_throughput: 6597151.508\n",
      "    load_time_ms: 0.606\n",
      "    sample_throughput: 18.379\n",
      "    sample_time_ms: 217635.46\n",
      "    update_time_ms: 17.812\n",
      "  timestamp: 1650225174\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-53-05\n",
      "  done: false\n",
      "  episode_len_mean: 246.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 601\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.42693889141082764\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014795854687690735\n",
      "          model: {}\n",
      "          policy_loss: -0.013342748396098614\n",
      "          total_loss: 0.015748851001262665\n",
      "          vf_explained_var: 0.7325640320777893\n",
      "          vf_loss: 0.029091566801071167\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.10033112582781\n",
      "    ram_util_percent: 95.41456953642383\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09740078495869478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9347336478141584\n",
      "    mean_inference_ms: 1.9046464399827294\n",
      "    mean_raw_obs_processing_ms: 0.22842043080074653\n",
      "  time_since_restore: 6846.386367321014\n",
      "  time_this_iter_s: 224.8989222049713\n",
      "  time_total_s: 6846.386367321014\n",
      "  timers:\n",
      "    learn_throughput: 19.637\n",
      "    learn_time_ms: 203698.621\n",
      "    load_throughput: 3442610.087\n",
      "    load_time_ms: 1.162\n",
      "    sample_throughput: 18.368\n",
      "    sample_time_ms: 217769.141\n",
      "    update_time_ms: 8.536\n",
      "  timestamp: 1650225185\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 20:55:23 (running for 01:56:43.31)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.4 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-56-38\n",
      "  done: false\n",
      "  episode_len_mean: 2078.65625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.078125\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 64\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6566129424663316e-11\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0752290405624166e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.8466039714658366e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0004300154687371105\n",
      "          total_loss: 0.0022566088009625673\n",
      "          vf_explained_var: -0.03424545377492905\n",
      "          vf_loss: 0.0018265875987708569\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.18833333333333\n",
      "    ram_util_percent: 95.68066666666665\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08784603743624327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8606224019835202\n",
      "    mean_inference_ms: 1.6421313593370999\n",
      "    mean_raw_obs_processing_ms: 0.11182166010207287\n",
      "  time_since_restore: 7058.16418337822\n",
      "  time_this_iter_s: 224.17928504943848\n",
      "  time_total_s: 7058.16418337822\n",
      "  timers:\n",
      "    learn_throughput: 19.59\n",
      "    learn_time_ms: 204181.396\n",
      "    load_throughput: 6627381.394\n",
      "    load_time_ms: 0.604\n",
      "    sample_throughput: 18.319\n",
      "    sample_time_ms: 218348.764\n",
      "    update_time_ms: 18.12\n",
      "  timestamp: 1650225398\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_20-56-49\n",
      "  done: false\n",
      "  episode_len_mean: 249.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 619\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.49867162108421326\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010108726099133492\n",
      "          model: {}\n",
      "          policy_loss: -0.012334486469626427\n",
      "          total_loss: 0.04319699853658676\n",
      "          vf_explained_var: 0.49682044982910156\n",
      "          vf_loss: 0.055531468242406845\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.74006622516556\n",
      "    ram_util_percent: 95.6682119205298\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09780554376564027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9373333047511977\n",
      "    mean_inference_ms: 1.9167036528432992\n",
      "    mean_raw_obs_processing_ms: 0.22903008087600546\n",
      "  time_since_restore: 7070.651316165924\n",
      "  time_this_iter_s: 224.26494884490967\n",
      "  time_total_s: 7070.651316165924\n",
      "  timers:\n",
      "    learn_throughput: 19.603\n",
      "    learn_time_ms: 204047.294\n",
      "    load_throughput: 3448483.279\n",
      "    load_time_ms: 1.16\n",
      "    sample_throughput: 18.305\n",
      "    sample_time_ms: 218522.316\n",
      "    update_time_ms: 8.488\n",
      "  timestamp: 1650225409\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-00-18\n",
      "  done: false\n",
      "  episode_len_mean: 2078.65625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.078125\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 64\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3283064712331658e-11\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128174632787704\n",
      "          total_loss: -0.014083363115787506\n",
      "          vf_explained_var: -1.6118890755478787e-07\n",
      "          vf_loss: 4.4807300582760945e-05\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.12338983050849\n",
      "    ram_util_percent: 95.57322033898305\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08784603743624327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8606224019835202\n",
      "    mean_inference_ms: 1.6421313593370999\n",
      "    mean_raw_obs_processing_ms: 0.11182166010207287\n",
      "  time_since_restore: 7277.595011472702\n",
      "  time_this_iter_s: 219.43082809448242\n",
      "  time_total_s: 7277.595011472702\n",
      "  timers:\n",
      "    learn_throughput: 19.573\n",
      "    learn_time_ms: 204364.884\n",
      "    load_throughput: 6667149.897\n",
      "    load_time_ms: 0.6\n",
      "    sample_throughput: 18.285\n",
      "    sample_time_ms: 218763.786\n",
      "    update_time_ms: 17.827\n",
      "  timestamp: 1650225618\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-00-29\n",
      "  done: false\n",
      "  episode_len_mean: 250.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 637\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.544899582862854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010937489569187164\n",
      "          model: {}\n",
      "          policy_loss: -0.0180974118411541\n",
      "          total_loss: 0.030045541003346443\n",
      "          vf_explained_var: 0.2783472239971161\n",
      "          vf_loss: 0.0481429286301136\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.11122448979592\n",
      "    ram_util_percent: 95.59795918367347\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09821823802049699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9399146555637623\n",
      "    mean_inference_ms: 1.928468925277092\n",
      "    mean_raw_obs_processing_ms: 0.2296929525359773\n",
      "  time_since_restore: 7290.080706119537\n",
      "  time_this_iter_s: 219.42938995361328\n",
      "  time_total_s: 7290.080706119537\n",
      "  timers:\n",
      "    learn_throughput: 19.588\n",
      "    learn_time_ms: 204208.628\n",
      "    load_throughput: 3605832.187\n",
      "    load_time_ms: 1.109\n",
      "    sample_throughput: 18.268\n",
      "    sample_time_ms: 218958.715\n",
      "    update_time_ms: 8.879\n",
      "  timestamp: 1650225629\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-03-54\n",
      "  done: false\n",
      "  episode_len_mean: 2078.65625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.078125\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 64\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1641532356165829e-11\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014127977192401886\n",
      "          total_loss: -0.014117511920630932\n",
      "          vf_explained_var: -1.1585092352106585e-06\n",
      "          vf_loss: 1.0465319974173326e-05\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.0003448275862\n",
      "    ram_util_percent: 95.69241379310344\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08784603743624327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8606224019835202\n",
      "    mean_inference_ms: 1.6421313593370999\n",
      "    mean_raw_obs_processing_ms: 0.11182166010207287\n",
      "  time_since_restore: 7493.864523410797\n",
      "  time_this_iter_s: 216.2695119380951\n",
      "  time_total_s: 7493.864523410797\n",
      "  timers:\n",
      "    learn_throughput: 19.586\n",
      "    learn_time_ms: 204232.426\n",
      "    load_throughput: 6535474.271\n",
      "    load_time_ms: 0.612\n",
      "    sample_throughput: 18.264\n",
      "    sample_time_ms: 219014.457\n",
      "    update_time_ms: 32.738\n",
      "  timestamp: 1650225834\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-04-06\n",
      "  done: false\n",
      "  episode_len_mean: 252.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 656\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.45384374260902405\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004832169506698847\n",
      "          model: {}\n",
      "          policy_loss: -0.011434728279709816\n",
      "          total_loss: 0.008575133047997952\n",
      "          vf_explained_var: 0.5069461464881897\n",
      "          vf_loss: 0.020009851083159447\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.77568493150685\n",
      "    ram_util_percent: 95.71198630136986\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09861185846856411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9424948254664798\n",
      "    mean_inference_ms: 1.9405604815298787\n",
      "    mean_raw_obs_processing_ms: 0.23044404922189352\n",
      "  time_since_restore: 7507.748771190643\n",
      "  time_this_iter_s: 217.66806507110596\n",
      "  time_total_s: 7507.748771190643\n",
      "  timers:\n",
      "    learn_throughput: 19.582\n",
      "    learn_time_ms: 204269.463\n",
      "    load_throughput: 3630017.742\n",
      "    load_time_ms: 1.102\n",
      "    sample_throughput: 18.25\n",
      "    sample_time_ms: 219177.718\n",
      "    update_time_ms: 9.845\n",
      "  timestamp: 1650225846\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-07-33\n",
      "  done: false\n",
      "  episode_len_mean: 1989.6986301369864\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1369863013698631\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 73\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.8207661780829145e-12\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.183616393769906e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.6069266393639128e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.002489800099283457\n",
      "          total_loss: 0.031251437962055206\n",
      "          vf_explained_var: -0.040381744503974915\n",
      "          vf_loss: 0.033741239458322525\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.79387755102042\n",
      "    ram_util_percent: 95.38639455782312\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08911766053831335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8686824685508631\n",
      "    mean_inference_ms: 1.6821803038265186\n",
      "    mean_raw_obs_processing_ms: 0.11326473767925277\n",
      "  time_since_restore: 7712.839810371399\n",
      "  time_this_iter_s: 218.9752869606018\n",
      "  time_total_s: 7712.839810371399\n",
      "  timers:\n",
      "    learn_throughput: 19.59\n",
      "    learn_time_ms: 204187.831\n",
      "    load_throughput: 6540824.951\n",
      "    load_time_ms: 0.612\n",
      "    sample_throughput: 18.25\n",
      "    sample_time_ms: 219176.648\n",
      "    update_time_ms: 32.873\n",
      "  timestamp: 1650226053\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-07-44\n",
      "  done: false\n",
      "  episode_len_mean: 254.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.57\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 674\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.144409225162235e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.46219587326049805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0047789327800273895\n",
      "          model: {}\n",
      "          policy_loss: -0.018192676827311516\n",
      "          total_loss: 0.0022426494397222996\n",
      "          vf_explained_var: 0.6404403448104858\n",
      "          vf_loss: 0.02043532021343708\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.3541095890411\n",
      "    ram_util_percent: 95.36506849315067\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09899276867162494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9450086260352981\n",
      "    mean_inference_ms: 1.9528471187769798\n",
      "    mean_raw_obs_processing_ms: 0.2311844618632234\n",
      "  time_since_restore: 7725.065297365189\n",
      "  time_this_iter_s: 217.3165261745453\n",
      "  time_total_s: 7725.065297365189\n",
      "  timers:\n",
      "    learn_throughput: 19.593\n",
      "    learn_time_ms: 204153.098\n",
      "    load_throughput: 3502623.437\n",
      "    load_time_ms: 1.142\n",
      "    sample_throughput: 18.235\n",
      "    sample_time_ms: 219359.756\n",
      "    update_time_ms: 10.449\n",
      "  timestamp: 1650226064\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-11-12\n",
      "  done: false\n",
      "  episode_len_mean: 1989.6986301369864\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1369863013698631\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 73\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.9103830890414573e-12\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.686394281889638e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.041380143722529e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128269627690315\n",
      "          total_loss: 0.014159854501485825\n",
      "          vf_explained_var: -1.4897956361892284e-06\n",
      "          vf_loss: 3.159591506118886e-05\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.17201365187714\n",
      "    ram_util_percent: 95.56723549488053\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08911766053831335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8686824685508631\n",
      "    mean_inference_ms: 1.6821803038265186\n",
      "    mean_raw_obs_processing_ms: 0.11326473767925277\n",
      "  time_since_restore: 7931.737005233765\n",
      "  time_this_iter_s: 218.89719486236572\n",
      "  time_total_s: 7931.737005233765\n",
      "  timers:\n",
      "    learn_throughput: 19.585\n",
      "    learn_time_ms: 204233.364\n",
      "    load_throughput: 6608064.91\n",
      "    load_time_ms: 0.605\n",
      "    sample_throughput: 18.249\n",
      "    sample_time_ms: 219186.168\n",
      "    update_time_ms: 34.741\n",
      "  timestamp: 1650226272\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-11-24\n",
      "  done: false\n",
      "  episode_len_mean: 248.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.46\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 693\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.47287794947624207\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005339172203093767\n",
      "          model: {}\n",
      "          policy_loss: -0.02104744128882885\n",
      "          total_loss: 0.0007528869318775833\n",
      "          vf_explained_var: 0.5663810968399048\n",
      "          vf_loss: 0.0218003261834383\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.51694915254237\n",
      "    ram_util_percent: 95.59152542372881\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09937764756794315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9477033888198828\n",
      "    mean_inference_ms: 1.96580487508326\n",
      "    mean_raw_obs_processing_ms: 0.2318911726509449\n",
      "  time_since_restore: 7945.096251249313\n",
      "  time_this_iter_s: 220.03095388412476\n",
      "  time_total_s: 7945.096251249313\n",
      "  timers:\n",
      "    learn_throughput: 19.579\n",
      "    learn_time_ms: 204295.966\n",
      "    load_throughput: 3700149.089\n",
      "    load_time_ms: 1.081\n",
      "    sample_throughput: 18.242\n",
      "    sample_time_ms: 219277.256\n",
      "    update_time_ms: 10.967\n",
      "  timestamp: 1650226284\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 21:12:03 (running for 02:13:23.37)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.46 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-14-57\n",
      "  done: false\n",
      "  episode_len_mean: 1989.6986301369864\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1369863013698631\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 73\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4551915445207286e-12\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.686394281889638e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.041380143722529e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128055423498154\n",
      "          total_loss: 0.014134996570646763\n",
      "          vf_explained_var: -3.356702791279531e-06\n",
      "          vf_loss: 6.9491052272496745e-06\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.64086378737542\n",
      "    ram_util_percent: 95.60232558139536\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08911766053831335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8686824685508631\n",
      "    mean_inference_ms: 1.6821803038265186\n",
      "    mean_raw_obs_processing_ms: 0.11326473767925277\n",
      "  time_since_restore: 8156.691065311432\n",
      "  time_this_iter_s: 224.95406007766724\n",
      "  time_total_s: 8156.691065311432\n",
      "  timers:\n",
      "    learn_throughput: 19.506\n",
      "    learn_time_ms: 205060.694\n",
      "    load_throughput: 6555904.81\n",
      "    load_time_ms: 0.61\n",
      "    sample_throughput: 18.231\n",
      "    sample_time_ms: 219404.237\n",
      "    update_time_ms: 36.113\n",
      "  timestamp: 1650226497\n",
      "  timesteps_since_restore: 152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 249.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.5\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 711\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.45947226881980896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005160132423043251\n",
      "          model: {}\n",
      "          policy_loss: -0.015470972284674644\n",
      "          total_loss: 0.005215727724134922\n",
      "          vf_explained_var: 0.6996158957481384\n",
      "          vf_loss: 0.02068670094013214\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.67066666666668\n",
      "    ram_util_percent: 95.60599999999998\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0997512197361188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9503428358653658\n",
      "    mean_inference_ms: 1.9773250229068084\n",
      "    mean_raw_obs_processing_ms: 0.23253627531356372\n",
      "  time_since_restore: 8168.534482002258\n",
      "  time_this_iter_s: 223.43823075294495\n",
      "  time_total_s: 8168.534482002258\n",
      "  timers:\n",
      "    learn_throughput: 19.514\n",
      "    learn_time_ms: 204982.251\n",
      "    load_throughput: 3738238.859\n",
      "    load_time_ms: 1.07\n",
      "    sample_throughput: 18.22\n",
      "    sample_time_ms: 219537.762\n",
      "    update_time_ms: 11.213\n",
      "  timestamp: 1650226507\n",
      "  timesteps_since_restore: 152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-18-38\n",
      "  done: false\n",
      "  episode_len_mean: 2097.945945945946\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1216216216216217\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 74\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.275957722603643e-13\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.803463603020619e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.015290217596194e-28\n",
      "          model: {}\n",
      "          policy_loss: 0.004389157984405756\n",
      "          total_loss: 0.00464631337672472\n",
      "          vf_explained_var: -0.039631910622119904\n",
      "          vf_loss: 0.0002571571967564523\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.08479729729729\n",
      "    ram_util_percent: 95.66182432432433\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08925506936458526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8695431033699546\n",
      "    mean_inference_ms: 1.6865073573599287\n",
      "    mean_raw_obs_processing_ms: 0.11341194005363993\n",
      "  time_since_restore: 8377.386194229126\n",
      "  time_this_iter_s: 220.6951289176941\n",
      "  time_total_s: 8377.386194229126\n",
      "  timers:\n",
      "    learn_throughput: 19.488\n",
      "    learn_time_ms: 205253.492\n",
      "    load_throughput: 6579558.414\n",
      "    load_time_ms: 0.608\n",
      "    sample_throughput: 18.164\n",
      "    sample_time_ms: 220213.35\n",
      "    update_time_ms: 37.563\n",
      "  timestamp: 1650226718\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-18-49\n",
      "  done: false\n",
      "  episode_len_mean: 247.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.54\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 729\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5243110060691833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009399029426276684\n",
      "          model: {}\n",
      "          policy_loss: -0.016388677060604095\n",
      "          total_loss: 0.04696028679609299\n",
      "          vf_explained_var: 0.5321004986763\n",
      "          vf_loss: 0.06334896385669708\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.40979729729729\n",
      "    ram_util_percent: 95.66216216216216\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10013112561278616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.953031726224809\n",
      "    mean_inference_ms: 1.988840181333507\n",
      "    mean_raw_obs_processing_ms: 0.23327818911984471\n",
      "  time_since_restore: 8389.776293039322\n",
      "  time_this_iter_s: 221.2418110370636\n",
      "  time_total_s: 8389.776293039322\n",
      "  timers:\n",
      "    learn_throughput: 19.504\n",
      "    learn_time_ms: 205086.996\n",
      "    load_throughput: 2666054.76\n",
      "    load_time_ms: 1.5\n",
      "    sample_throughput: 18.153\n",
      "    sample_time_ms: 220352.252\n",
      "    update_time_ms: 11.25\n",
      "  timestamp: 1650226729\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-22-19\n",
      "  done: false\n",
      "  episode_len_mean: 2097.945945945946\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.1216216216216217\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 74\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.6379788613018216e-13\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412813551723957\n",
      "          total_loss: 0.014132257550954819\n",
      "          vf_explained_var: 1.420256978690304e-07\n",
      "          vf_loss: 4.131025889364537e-06\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.825\n",
      "    ram_util_percent: 95.80067567567568\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08925506936458526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8695431033699546\n",
      "    mean_inference_ms: 1.6865073573599287\n",
      "    mean_raw_obs_processing_ms: 0.11341194005363993\n",
      "  time_since_restore: 8598.000161409378\n",
      "  time_this_iter_s: 220.61396718025208\n",
      "  time_total_s: 8598.000161409378\n",
      "  timers:\n",
      "    learn_throughput: 19.438\n",
      "    learn_time_ms: 205781.449\n",
      "    load_throughput: 6691614.55\n",
      "    load_time_ms: 0.598\n",
      "    sample_throughput: 18.144\n",
      "    sample_time_ms: 220461.613\n",
      "    update_time_ms: 37.176\n",
      "  timestamp: 1650226939\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-22-31\n",
      "  done: false\n",
      "  episode_len_mean: 243.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.48\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 748\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.4768768548965454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008727526292204857\n",
      "          model: {}\n",
      "          policy_loss: -0.017187325283885002\n",
      "          total_loss: 0.013396178372204304\n",
      "          vf_explained_var: 0.6411821246147156\n",
      "          vf_loss: 0.03058350272476673\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.58148148148148\n",
      "    ram_util_percent: 95.78754208754208\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10051031982757641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9557127444141085\n",
      "    mean_inference_ms: 2.001310595154136\n",
      "    mean_raw_obs_processing_ms: 0.23395921322401547\n",
      "  time_since_restore: 8611.770111083984\n",
      "  time_this_iter_s: 221.99381804466248\n",
      "  time_total_s: 8611.770111083984\n",
      "  timers:\n",
      "    learn_throughput: 19.448\n",
      "    learn_time_ms: 205675.832\n",
      "    load_throughput: 3731587.189\n",
      "    load_time_ms: 1.072\n",
      "    sample_throughput: 18.133\n",
      "    sample_time_ms: 220593.499\n",
      "    update_time_ms: 11.195\n",
      "  timestamp: 1650226951\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-25-57\n",
      "  done: false\n",
      "  episode_len_mean: 2151.1688311688313\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.077922077922078\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 77\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8189894306509108e-13\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.853899616666855e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.8604265905148994e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0005480166873894632\n",
      "          total_loss: 0.0056757074780762196\n",
      "          vf_explained_var: 0.03896036744117737\n",
      "          vf_loss: 0.005127685610204935\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.97883959044368\n",
      "    ram_util_percent: 95.76348122866895\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08967633050229215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8721564087779691\n",
      "    mean_inference_ms: 1.69963137595997\n",
      "    mean_raw_obs_processing_ms: 0.1138763445417248\n",
      "  time_since_restore: 8816.3000934124\n",
      "  time_this_iter_s: 218.29993200302124\n",
      "  time_total_s: 8816.3000934124\n",
      "  timers:\n",
      "    learn_throughput: 19.471\n",
      "    learn_time_ms: 205430.038\n",
      "    load_throughput: 6659475.251\n",
      "    load_time_ms: 0.601\n",
      "    sample_throughput: 18.091\n",
      "    sample_time_ms: 221107.828\n",
      "    update_time_ms: 37.942\n",
      "  timestamp: 1650227157\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-26-08\n",
      "  done: false\n",
      "  episode_len_mean: 245.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.46\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 766\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5100319981575012\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007698372006416321\n",
      "          model: {}\n",
      "          policy_loss: -0.01735793985426426\n",
      "          total_loss: 0.018018480390310287\n",
      "          vf_explained_var: 0.48335105180740356\n",
      "          vf_loss: 0.0353764183819294\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.72020547945205\n",
      "    ram_util_percent: 95.79075342465754\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10087458288163476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9581348148962856\n",
      "    mean_inference_ms: 2.012460508688275\n",
      "    mean_raw_obs_processing_ms: 0.2345442862897243\n",
      "  time_since_restore: 8829.430051326752\n",
      "  time_this_iter_s: 217.65994024276733\n",
      "  time_total_s: 8829.430051326752\n",
      "  timers:\n",
      "    learn_throughput: 19.493\n",
      "    learn_time_ms: 205205.123\n",
      "    load_throughput: 3686085.027\n",
      "    load_time_ms: 1.085\n",
      "    sample_throughput: 18.075\n",
      "    sample_time_ms: 221304.93\n",
      "    update_time_ms: 11.537\n",
      "  timestamp: 1650227168\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 21:28:44 (running for 02:30:04.02)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.46 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-29-40\n",
      "  done: false\n",
      "  episode_len_mean: 2151.1688311688313\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.077922077922078\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 77\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.094947153254554e-14\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128148555755615\n",
      "          total_loss: -0.013552052900195122\n",
      "          vf_explained_var: -2.1214125567325937e-08\n",
      "          vf_loss: 0.000576101359911263\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.69397993311037\n",
      "    ram_util_percent: 95.71939799331103\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08967633050229215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8721564087779691\n",
      "    mean_inference_ms: 1.69963137595997\n",
      "    mean_raw_obs_processing_ms: 0.1138763445417248\n",
      "  time_since_restore: 9038.860264062881\n",
      "  time_this_iter_s: 222.56017065048218\n",
      "  time_total_s: 9038.860264062881\n",
      "  timers:\n",
      "    learn_throughput: 19.485\n",
      "    learn_time_ms: 205284.235\n",
      "    load_throughput: 6657361.216\n",
      "    load_time_ms: 0.601\n",
      "    sample_throughput: 18.122\n",
      "    sample_time_ms: 220724.92\n",
      "    update_time_ms: 38.509\n",
      "  timestamp: 1650227380\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-29-52\n",
      "  done: false\n",
      "  episode_len_mean: 249.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.51\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 784\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5811116695404053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007813052274286747\n",
      "          model: {}\n",
      "          policy_loss: -0.009239081293344498\n",
      "          total_loss: 0.022154835984110832\n",
      "          vf_explained_var: 0.4168151319026947\n",
      "          vf_loss: 0.03139391541481018\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.83766666666666\n",
      "    ram_util_percent: 95.69633333333334\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10123392354683113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9604820211308318\n",
      "    mean_inference_ms: 2.023638088986161\n",
      "    mean_raw_obs_processing_ms: 0.23522190608139545\n",
      "  time_since_restore: 9052.85326719284\n",
      "  time_this_iter_s: 223.42321586608887\n",
      "  time_total_s: 9052.85326719284\n",
      "  timers:\n",
      "    learn_throughput: 19.518\n",
      "    learn_time_ms: 204937.029\n",
      "    load_throughput: 3648093.24\n",
      "    load_time_ms: 1.096\n",
      "    sample_throughput: 18.103\n",
      "    sample_time_ms: 220953.7\n",
      "    update_time_ms: 10.416\n",
      "  timestamp: 1650227392\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-33-12\n",
      "  done: false\n",
      "  episode_len_mean: 2151.1688311688313\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.077922077922078\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 77\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.547473576627277e-14\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128075912594795\n",
      "          total_loss: -0.013883152045309544\n",
      "          vf_explained_var: 5.0888267821846966e-08\n",
      "          vf_loss: 0.00024492511874996126\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.12797202797204\n",
      "    ram_util_percent: 95.5437062937063\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08967633050229215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8721564087779691\n",
      "    mean_inference_ms: 1.69963137595997\n",
      "    mean_raw_obs_processing_ms: 0.1138763445417248\n",
      "  time_since_restore: 9251.320393800735\n",
      "  time_this_iter_s: 212.460129737854\n",
      "  time_total_s: 9251.320393800735\n",
      "  timers:\n",
      "    learn_throughput: 19.601\n",
      "    learn_time_ms: 204074.343\n",
      "    load_throughput: 6625549.325\n",
      "    load_time_ms: 0.604\n",
      "    sample_throughput: 18.131\n",
      "    sample_time_ms: 220620.948\n",
      "    update_time_ms: 37.693\n",
      "  timestamp: 1650227592\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-33-24\n",
      "  done: false\n",
      "  episode_len_mean: 247.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 803\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6360055208206177\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008291293866932392\n",
      "          model: {}\n",
      "          policy_loss: -0.022925136610865593\n",
      "          total_loss: 0.010124299675226212\n",
      "          vf_explained_var: 0.42819681763648987\n",
      "          vf_loss: 0.033049434423446655\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.05263157894737\n",
      "    ram_util_percent: 95.53368421052632\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10162865861549487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9629102297865092\n",
      "    mean_inference_ms: 2.035434574677695\n",
      "    mean_raw_obs_processing_ms: 0.23595621448702125\n",
      "  time_since_restore: 9264.735750198364\n",
      "  time_this_iter_s: 211.88248300552368\n",
      "  time_total_s: 9264.735750198364\n",
      "  timers:\n",
      "    learn_throughput: 19.641\n",
      "    learn_time_ms: 203658.142\n",
      "    load_throughput: 3503793.831\n",
      "    load_time_ms: 1.142\n",
      "    sample_throughput: 18.122\n",
      "    sample_time_ms: 220722.954\n",
      "    update_time_ms: 9.989\n",
      "  timestamp: 1650227604\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-36-42\n",
      "  done: false\n",
      "  episode_len_mean: 2180.4938271604938\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.123456790123457\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 81\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2737367883136385e-14\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.82224076159796e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.5024259614850513e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0017637076089158654\n",
      "          total_loss: 0.008634215220808983\n",
      "          vf_explained_var: 0.1438641995191574\n",
      "          vf_loss: 0.010397917591035366\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.81950354609928\n",
      "    ram_util_percent: 95.33297872340425\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09024011121257833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8756557185779046\n",
      "    mean_inference_ms: 1.7169898753331807\n",
      "    mean_raw_obs_processing_ms: 0.11449204043787937\n",
      "  time_since_restore: 9460.815469741821\n",
      "  time_this_iter_s: 209.49507594108582\n",
      "  time_total_s: 9460.815469741821\n",
      "  timers:\n",
      "    learn_throughput: 19.698\n",
      "    learn_time_ms: 203069.972\n",
      "    load_throughput: 6748407.546\n",
      "    load_time_ms: 0.593\n",
      "    sample_throughput: 18.23\n",
      "    sample_time_ms: 219422.585\n",
      "    update_time_ms: 37.94\n",
      "  timestamp: 1650227802\n",
      "  timesteps_since_restore: 176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-36-52\n",
      "  done: false\n",
      "  episode_len_mean: 256.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.57\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 817\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7655155062675476\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0104203000664711\n",
      "          model: {}\n",
      "          policy_loss: -0.029580170288681984\n",
      "          total_loss: 0.06680592149496078\n",
      "          vf_explained_var: 0.504045844078064\n",
      "          vf_loss: 0.09638609737157822\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.75357142857143\n",
      "    ram_util_percent: 95.34964285714285\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10189402011890655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9645327789922993\n",
      "    mean_inference_ms: 2.0432690822406916\n",
      "    mean_raw_obs_processing_ms: 0.2363789512434579\n",
      "  time_since_restore: 9473.378783226013\n",
      "  time_this_iter_s: 208.64303302764893\n",
      "  time_total_s: 9473.378783226013\n",
      "  timers:\n",
      "    learn_throughput: 19.742\n",
      "    learn_time_ms: 202618.7\n",
      "    load_throughput: 3535692.82\n",
      "    load_time_ms: 1.131\n",
      "    sample_throughput: 18.23\n",
      "    sample_time_ms: 219414.701\n",
      "    update_time_ms: 9.505\n",
      "  timestamp: 1650227812\n",
      "  timesteps_since_restore: 176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-40-12\n",
      "  done: false\n",
      "  episode_len_mean: 2180.4938271604938\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.123456790123457\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1368683941568192e-14\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014127994887530804\n",
      "          total_loss: 0.01416791696101427\n",
      "          vf_explained_var: -1.683670944885307e-07\n",
      "          vf_loss: 3.992470010416582e-05\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.62056737588652\n",
      "    ram_util_percent: 95.43085106382979\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09024011121257833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8756557185779046\n",
      "    mean_inference_ms: 1.7169898753331807\n",
      "    mean_raw_obs_processing_ms: 0.11449204043787937\n",
      "  time_since_restore: 9670.736499547958\n",
      "  time_this_iter_s: 209.92102980613708\n",
      "  time_total_s: 9670.736499547958\n",
      "  timers:\n",
      "    learn_throughput: 19.752\n",
      "    learn_time_ms: 202515.489\n",
      "    load_throughput: 6820561.021\n",
      "    load_time_ms: 0.586\n",
      "    sample_throughput: 18.319\n",
      "    sample_time_ms: 218353.446\n",
      "    update_time_ms: 22.617\n",
      "  timestamp: 1650228012\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-40-23\n",
      "  done: false\n",
      "  episode_len_mean: 258.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.66\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 833\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.760158896446228\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004588744603097439\n",
      "          model: {}\n",
      "          policy_loss: -0.028207838535308838\n",
      "          total_loss: 0.02980084717273712\n",
      "          vf_explained_var: 0.4859238862991333\n",
      "          vf_loss: 0.05800868198275566\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.06325088339223\n",
      "    ram_util_percent: 95.41413427561837\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1021935207610711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9662963651133171\n",
      "    mean_inference_ms: 2.0516116768525574\n",
      "    mean_raw_obs_processing_ms: 0.23679901551803278\n",
      "  time_since_restore: 9683.54120516777\n",
      "  time_this_iter_s: 210.1624219417572\n",
      "  time_total_s: 9683.54120516777\n",
      "  timers:\n",
      "    learn_throughput: 19.817\n",
      "    learn_time_ms: 201843.142\n",
      "    load_throughput: 3513111.651\n",
      "    load_time_ms: 1.139\n",
      "    sample_throughput: 18.315\n",
      "    sample_time_ms: 218402.027\n",
      "    update_time_ms: 8.417\n",
      "  timestamp: 1650228023\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-43-44\n",
      "  done: false\n",
      "  episode_len_mean: 2180.4938271604938\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.123456790123457\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.684341970784096e-15\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128133654594421\n",
      "          total_loss: 0.014137251302599907\n",
      "          vf_explained_var: -1.7183442651003134e-06\n",
      "          vf_loss: 9.122491974267177e-06\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.73936170212765\n",
      "    ram_util_percent: 95.4709219858156\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09024011121257833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8756557185779046\n",
      "    mean_inference_ms: 1.7169898753331807\n",
      "    mean_raw_obs_processing_ms: 0.11449204043787937\n",
      "  time_since_restore: 9882.828776597977\n",
      "  time_this_iter_s: 212.0922770500183\n",
      "  time_total_s: 9882.828776597977\n",
      "  timers:\n",
      "    learn_throughput: 19.799\n",
      "    learn_time_ms: 202035.157\n",
      "    load_throughput: 6812252.72\n",
      "    load_time_ms: 0.587\n",
      "    sample_throughput: 18.384\n",
      "    sample_time_ms: 217575.381\n",
      "    update_time_ms: 22.463\n",
      "  timestamp: 1650228224\n",
      "  timesteps_since_restore: 184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-43-55\n",
      "  done: false\n",
      "  episode_len_mean: 266.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.82\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 850\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8019448518753052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006587958429008722\n",
      "          model: {}\n",
      "          policy_loss: -0.031247597187757492\n",
      "          total_loss: 0.07312890142202377\n",
      "          vf_explained_var: 0.44590505957603455\n",
      "          vf_loss: 0.10437650233507156\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.84558303886925\n",
      "    ram_util_percent: 95.48869257950531\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10250872373805944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9681699148562172\n",
      "    mean_inference_ms: 2.0594061209125076\n",
      "    mean_raw_obs_processing_ms: 0.2371387962952911\n",
      "  time_since_restore: 9895.56337594986\n",
      "  time_this_iter_s: 212.02217078208923\n",
      "  time_total_s: 9895.56337594986\n",
      "  timers:\n",
      "    learn_throughput: 19.86\n",
      "    learn_time_ms: 201407.647\n",
      "    load_throughput: 3623197.495\n",
      "    load_time_ms: 1.104\n",
      "    sample_throughput: 18.388\n",
      "    sample_time_ms: 217534.263\n",
      "    update_time_ms: 7.926\n",
      "  timestamp: 1650228235\n",
      "  timesteps_since_restore: 184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 21:45:25 (running for 02:46:44.88)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.82 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-47-18\n",
      "  done: false\n",
      "  episode_len_mean: 2080.978021978022\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2197802197802199\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 91\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.842170985392048e-15\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.173900221012199e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -5.83373694248819e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.001369725214317441\n",
      "          total_loss: 0.019090596586465836\n",
      "          vf_explained_var: 0.3163853883743286\n",
      "          vf_loss: 0.017720872536301613\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.8048780487805\n",
      "    ram_util_percent: 95.36655052264807\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0915042094854643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8833930324540309\n",
      "    mean_inference_ms: 1.7552866027177942\n",
      "    mean_raw_obs_processing_ms: 0.11591776038069491\n",
      "  time_since_restore: 10097.214025497437\n",
      "  time_this_iter_s: 214.38524889945984\n",
      "  time_total_s: 10097.214025497437\n",
      "  timers:\n",
      "    learn_throughput: 19.843\n",
      "    learn_time_ms: 201583.001\n",
      "    load_throughput: 7017993.809\n",
      "    load_time_ms: 0.57\n",
      "    sample_throughput: 18.425\n",
      "    sample_time_ms: 217096.582\n",
      "    update_time_ms: 21.505\n",
      "  timestamp: 1650228438\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-47-29\n",
      "  done: false\n",
      "  episode_len_mean: 268.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.89\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 865\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8638232350349426\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00563056068494916\n",
      "          model: {}\n",
      "          policy_loss: -0.031279973685741425\n",
      "          total_loss: 0.06134263798594475\n",
      "          vf_explained_var: 0.33539801836013794\n",
      "          vf_loss: 0.09262261539697647\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.13588850174216\n",
      "    ram_util_percent: 95.35574912891988\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10274183698797373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9696513333771248\n",
      "    mean_inference_ms: 2.0655465495018706\n",
      "    mean_raw_obs_processing_ms: 0.23731302735957244\n",
      "  time_since_restore: 10109.974755048752\n",
      "  time_this_iter_s: 214.4113790988922\n",
      "  time_total_s: 10109.974755048752\n",
      "  timers:\n",
      "    learn_throughput: 19.907\n",
      "    learn_time_ms: 200939.365\n",
      "    load_throughput: 3667792.401\n",
      "    load_time_ms: 1.091\n",
      "    sample_throughput: 18.433\n",
      "    sample_time_ms: 217001.128\n",
      "    update_time_ms: 8.455\n",
      "  timestamp: 1650228449\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-50-48\n",
      "  done: false\n",
      "  episode_len_mean: 2080.978021978022\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2197802197802199\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 91\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.421085492696024e-15\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 5.738105380370897e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.428093780231864e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128043316304684\n",
      "          total_loss: 0.01531888172030449\n",
      "          vf_explained_var: 1.0985200304958198e-07\n",
      "          vf_loss: 0.0011908388696610928\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.79326241134753\n",
      "    ram_util_percent: 95.37943262411348\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0915042094854643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8833930324540309\n",
      "    mean_inference_ms: 1.7552866027177942\n",
      "    mean_raw_obs_processing_ms: 0.11591776038069491\n",
      "  time_since_restore: 10307.006520748138\n",
      "  time_this_iter_s: 209.7924952507019\n",
      "  time_total_s: 10307.006520748138\n",
      "  timers:\n",
      "    learn_throughput: 19.978\n",
      "    learn_time_ms: 200220.958\n",
      "    load_throughput: 6980036.612\n",
      "    load_time_ms: 0.573\n",
      "    sample_throughput: 18.477\n",
      "    sample_time_ms: 216486.198\n",
      "    update_time_ms: 19.382\n",
      "  timestamp: 1650228648\n",
      "  timesteps_since_restore: 192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 274.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.98\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 882\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7893518805503845\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005634818226099014\n",
      "          model: {}\n",
      "          policy_loss: -0.029208604246377945\n",
      "          total_loss: 0.040910493582487106\n",
      "          vf_explained_var: 0.4360443651676178\n",
      "          vf_loss: 0.07011909037828445\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.49752650176679\n",
      "    ram_util_percent: 95.38904593639576\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10297632668152883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9712153734888159\n",
      "    mean_inference_ms: 2.0714824478251477\n",
      "    mean_raw_obs_processing_ms: 0.23735655741154096\n",
      "  time_since_restore: 10320.60636806488\n",
      "  time_this_iter_s: 210.63161301612854\n",
      "  time_total_s: 10320.60636806488\n",
      "  timers:\n",
      "    learn_throughput: 20.028\n",
      "    learn_time_ms: 199725.075\n",
      "    load_throughput: 3694526.877\n",
      "    load_time_ms: 1.083\n",
      "    sample_throughput: 18.478\n",
      "    sample_time_ms: 216470.99\n",
      "    update_time_ms: 8.375\n",
      "  timestamp: 1650228660\n",
      "  timesteps_since_restore: 192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-54-18\n",
      "  done: false\n",
      "  episode_len_mean: 2080.978021978022\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2197802197802199\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 91\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.10542746348012e-16\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 5.738105380370897e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.428093780231864e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128156006336212\n",
      "          total_loss: 0.01416839100420475\n",
      "          vf_explained_var: -3.9390337747136073e-07\n",
      "          vf_loss: 4.02341247536242e-05\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.36014234875445\n",
      "    ram_util_percent: 95.37437722419928\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0915042094854643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8833930324540309\n",
      "    mean_inference_ms: 1.7552866027177942\n",
      "    mean_raw_obs_processing_ms: 0.11591776038069491\n",
      "  time_since_restore: 10516.193152666092\n",
      "  time_this_iter_s: 209.1866319179535\n",
      "  time_total_s: 10516.193152666092\n",
      "  timers:\n",
      "    learn_throughput: 20.094\n",
      "    learn_time_ms: 199062.094\n",
      "    load_throughput: 7043627.356\n",
      "    load_time_ms: 0.568\n",
      "    sample_throughput: 18.593\n",
      "    sample_time_ms: 215138.045\n",
      "    update_time_ms: 16.897\n",
      "  timestamp: 1650228858\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-54-28\n",
      "  done: false\n",
      "  episode_len_mean: 280.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 2.08\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 897\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8593539595603943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005834380630403757\n",
      "          model: {}\n",
      "          policy_loss: -0.03947439417243004\n",
      "          total_loss: 0.05154367908835411\n",
      "          vf_explained_var: 0.48260873556137085\n",
      "          vf_loss: 0.09101807326078415\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.40249999999999\n",
      "    ram_util_percent: 95.39392857142856\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10315170527421486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9724920566332044\n",
      "    mean_inference_ms: 2.0759496611628343\n",
      "    mean_raw_obs_processing_ms: 0.23729601896075178\n",
      "  time_since_restore: 10528.833719015121\n",
      "  time_this_iter_s: 208.2273509502411\n",
      "  time_total_s: 10528.833719015121\n",
      "  timers:\n",
      "    learn_throughput: 20.146\n",
      "    learn_time_ms: 198552.438\n",
      "    load_throughput: 6181502.524\n",
      "    load_time_ms: 0.647\n",
      "    sample_throughput: 18.593\n",
      "    sample_time_ms: 215132.799\n",
      "    update_time_ms: 7.91\n",
      "  timestamp: 1650228868\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-57-51\n",
      "  done: false\n",
      "  episode_len_mean: 2167.054347826087\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2065217391304348\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 92\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.55271373174006e-16\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.717613241430125e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.633337163774692e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.006357691250741482\n",
      "          total_loss: 0.006740303244441748\n",
      "          vf_explained_var: -0.011285140179097652\n",
      "          vf_loss: 0.0003826198517344892\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.74055944055944\n",
      "    ram_util_percent: 95.64335664335664\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0916201527396898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8841167006822642\n",
      "    mean_inference_ms: 1.7588177361910986\n",
      "    mean_raw_obs_processing_ms: 0.11604054698836098\n",
      "  time_since_restore: 10729.911380529404\n",
      "  time_this_iter_s: 213.71822786331177\n",
      "  time_total_s: 10729.911380529404\n",
      "  timers:\n",
      "    learn_throughput: 20.158\n",
      "    learn_time_ms: 198431.9\n",
      "    load_throughput: 6991963.326\n",
      "    load_time_ms: 0.572\n",
      "    sample_throughput: 18.699\n",
      "    sample_time_ms: 213916.735\n",
      "    update_time_ms: 17.194\n",
      "  timestamp: 1650229071\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_21-58-02\n",
      "  done: false\n",
      "  episode_len_mean: 272.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.92\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 915\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8579320311546326\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006683474872261286\n",
      "          model: {}\n",
      "          policy_loss: -0.03250369429588318\n",
      "          total_loss: 0.03024265170097351\n",
      "          vf_explained_var: 0.2539580166339874\n",
      "          vf_loss: 0.06274634599685669\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.62622377622378\n",
      "    ram_util_percent: 95.67132867132868\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10333786530413816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9739012587342621\n",
      "    mean_inference_ms: 2.08128293005033\n",
      "    mean_raw_obs_processing_ms: 0.23718990448405733\n",
      "  time_since_restore: 10743.012305259705\n",
      "  time_this_iter_s: 214.17858624458313\n",
      "  time_total_s: 10743.012305259705\n",
      "  timers:\n",
      "    learn_throughput: 20.213\n",
      "    learn_time_ms: 197890.42\n",
      "    load_throughput: 6392050.901\n",
      "    load_time_ms: 0.626\n",
      "    sample_throughput: 18.706\n",
      "    sample_time_ms: 213839.665\n",
      "    update_time_ms: 7.989\n",
      "  timestamp: 1650229082\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 2167.054347826087\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2065217391304348\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 92\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.77635686587003e-16\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412824634462595\n",
      "          total_loss: 0.014187094755470753\n",
      "          vf_explained_var: -1.9784896210239822e-07\n",
      "          vf_loss: 5.8862337027676404e-05\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.06551724137931\n",
      "    ram_util_percent: 95.69275862068966\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0916201527396898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8841167006822642\n",
      "    mean_inference_ms: 1.7588177361910986\n",
      "    mean_raw_obs_processing_ms: 0.11604054698836098\n",
      "  time_since_restore: 10946.804926395416\n",
      "  time_this_iter_s: 216.89354586601257\n",
      "  time_total_s: 10946.804926395416\n",
      "  timers:\n",
      "    learn_throughput: 20.162\n",
      "    learn_time_ms: 198397.009\n",
      "    load_throughput: 7208256.069\n",
      "    load_time_ms: 0.555\n",
      "    sample_throughput: 18.765\n",
      "    sample_time_ms: 213161.694\n",
      "    update_time_ms: 17.042\n",
      "  timestamp: 1650229288\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-01-40\n",
      "  done: false\n",
      "  episode_len_mean: 275.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.96\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 931\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.894080400466919\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006303939037024975\n",
      "          model: {}\n",
      "          policy_loss: -0.03355412557721138\n",
      "          total_loss: 0.044968683272600174\n",
      "          vf_explained_var: 0.5433045625686646\n",
      "          vf_loss: 0.07852280884981155\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.44109589041095\n",
      "    ram_util_percent: 95.71369863013699\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346631546447553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9749725040480203\n",
      "    mean_inference_ms: 2.0854344332556654\n",
      "    mean_raw_obs_processing_ms: 0.23699424837745806\n",
      "  time_since_restore: 10960.544404268265\n",
      "  time_this_iter_s: 217.53209900856018\n",
      "  time_total_s: 10960.544404268265\n",
      "  timers:\n",
      "    learn_throughput: 20.208\n",
      "    learn_time_ms: 197942.503\n",
      "    load_throughput: 4144466.787\n",
      "    load_time_ms: 0.965\n",
      "    sample_throughput: 18.771\n",
      "    sample_time_ms: 213094.968\n",
      "    update_time_ms: 7.994\n",
      "  timestamp: 1650229300\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 22:02:05 (running for 03:03:25.32)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.96 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-05-06\n",
      "  done: false\n",
      "  episode_len_mean: 2189.46875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2083333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 96\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.88178432935015e-17\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.36671476106308e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.1426565743660815e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0027716492768377066\n",
      "          total_loss: 0.0032569195609539747\n",
      "          vf_explained_var: 0.10942970216274261\n",
      "          vf_loss: 0.006028564181178808\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.23809523809524\n",
      "    ram_util_percent: 95.64149659863945\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09207084726919941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8869771199561071\n",
      "    mean_inference_ms: 1.772661533866435\n",
      "    mean_raw_obs_processing_ms: 0.11651665879631345\n",
      "  time_since_restore: 11164.856056451797\n",
      "  time_this_iter_s: 218.05113005638123\n",
      "  time_total_s: 11164.856056451797\n",
      "  timers:\n",
      "    learn_throughput: 20.209\n",
      "    learn_time_ms: 197926.861\n",
      "    load_throughput: 7201449.114\n",
      "    load_time_ms: 0.555\n",
      "    sample_throughput: 18.766\n",
      "    sample_time_ms: 213147.534\n",
      "    update_time_ms: 16.068\n",
      "  timestamp: 1650229506\n",
      "  timesteps_since_restore: 208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 275.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.92\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 947\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8398939967155457\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004881877917796373\n",
      "          model: {}\n",
      "          policy_loss: -0.03340890258550644\n",
      "          total_loss: 0.03599066659808159\n",
      "          vf_explained_var: 0.41143351793289185\n",
      "          vf_loss: 0.06939956545829773\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.66609589041097\n",
      "    ram_util_percent: 95.6431506849315\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10358277910736835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9760501169151544\n",
      "    mean_inference_ms: 2.089532430488928\n",
      "    mean_raw_obs_processing_ms: 0.23680463888406933\n",
      "  time_since_restore: 11178.36865735054\n",
      "  time_this_iter_s: 217.8242530822754\n",
      "  time_total_s: 11178.36865735054\n",
      "  timers:\n",
      "    learn_throughput: 20.255\n",
      "    learn_time_ms: 197482.102\n",
      "    load_throughput: 4238490.261\n",
      "    load_time_ms: 0.944\n",
      "    sample_throughput: 18.775\n",
      "    sample_time_ms: 213048.549\n",
      "    update_time_ms: 7.879\n",
      "  timestamp: 1650229518\n",
      "  timesteps_since_restore: 208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-08-45\n",
      "  done: false\n",
      "  episode_len_mean: 2189.46875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2083333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 96\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.440892164675075e-17\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128104783594608\n",
      "          total_loss: -0.013743119314312935\n",
      "          vf_explained_var: -1.1600473470707584e-08\n",
      "          vf_loss: 0.00038498788489960134\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.02559726962457\n",
      "    ram_util_percent: 95.75460750853242\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09207084726919941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8869771199561071\n",
      "    mean_inference_ms: 1.772661533866435\n",
      "    mean_raw_obs_processing_ms: 0.11651665879631345\n",
      "  time_since_restore: 11383.59369635582\n",
      "  time_this_iter_s: 218.73763990402222\n",
      "  time_total_s: 11383.59369635582\n",
      "  timers:\n",
      "    learn_throughput: 20.139\n",
      "    learn_time_ms: 198617.391\n",
      "    load_throughput: 7190029.999\n",
      "    load_time_ms: 0.556\n",
      "    sample_throughput: 18.814\n",
      "    sample_time_ms: 212610.42\n",
      "    update_time_ms: 14.731\n",
      "  timestamp: 1650229725\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-08-58\n",
      "  done: false\n",
      "  episode_len_mean: 278.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.98\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 962\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8628681898117065\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005293762777000666\n",
      "          model: {}\n",
      "          policy_loss: -0.04071332886815071\n",
      "          total_loss: 0.04462335258722305\n",
      "          vf_explained_var: 0.49830570816993713\n",
      "          vf_loss: 0.08533669263124466\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.8331081081081\n",
      "    ram_util_percent: 95.72601351351352\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10370875621880699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9771467055670595\n",
      "    mean_inference_ms: 2.094143968772319\n",
      "    mean_raw_obs_processing_ms: 0.23667761090526976\n",
      "  time_since_restore: 11398.436588287354\n",
      "  time_this_iter_s: 220.06793093681335\n",
      "  time_total_s: 11398.436588287354\n",
      "  timers:\n",
      "    learn_throughput: 20.17\n",
      "    learn_time_ms: 198318.611\n",
      "    load_throughput: 4531320.999\n",
      "    load_time_ms: 0.883\n",
      "    sample_throughput: 18.817\n",
      "    sample_time_ms: 212571.398\n",
      "    update_time_ms: 7.99\n",
      "  timestamp: 1650229738\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-12-21\n",
      "  done: false\n",
      "  episode_len_mean: 2189.46875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2083333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 96\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2204460823375376e-17\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128209091722965\n",
      "          total_loss: -0.014056526124477386\n",
      "          vf_explained_var: 3.9781292571205995e-07\n",
      "          vf_loss: 7.168265437940136e-05\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.02965517241378\n",
      "    ram_util_percent: 95.67241379310344\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09207084726919941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8869771199561071\n",
      "    mean_inference_ms: 1.772661533866435\n",
      "    mean_raw_obs_processing_ms: 0.11651665879631345\n",
      "  time_since_restore: 11599.685762405396\n",
      "  time_this_iter_s: 216.0920660495758\n",
      "  time_total_s: 11599.685762405396\n",
      "  timers:\n",
      "    learn_throughput: 20.078\n",
      "    learn_time_ms: 199218.211\n",
      "    load_throughput: 7120757.183\n",
      "    load_time_ms: 0.562\n",
      "    sample_throughput: 18.748\n",
      "    sample_time_ms: 213351.435\n",
      "    update_time_ms: 14.749\n",
      "  timestamp: 1650229941\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-12-34\n",
      "  done: false\n",
      "  episode_len_mean: 275.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.94\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 979\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.865484356880188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006500935181975365\n",
      "          model: {}\n",
      "          policy_loss: -0.03505650907754898\n",
      "          total_loss: 0.04069458693265915\n",
      "          vf_explained_var: 0.5722746253013611\n",
      "          vf_loss: 0.07575110346078873\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.93241379310345\n",
      "    ram_util_percent: 95.64862068965516\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10386536091439012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9784257476985609\n",
      "    mean_inference_ms: 2.099632018953334\n",
      "    mean_raw_obs_processing_ms: 0.23657339112598993\n",
      "  time_since_restore: 11614.217683315277\n",
      "  time_this_iter_s: 215.78109502792358\n",
      "  time_total_s: 11614.217683315277\n",
      "  timers:\n",
      "    learn_throughput: 20.106\n",
      "    learn_time_ms: 198945.582\n",
      "    load_throughput: 4462144.206\n",
      "    load_time_ms: 0.896\n",
      "    sample_throughput: 18.737\n",
      "    sample_time_ms: 213483.875\n",
      "    update_time_ms: 8.745\n",
      "  timestamp: 1650229954\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 2112.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 107\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1102230411687688e-17\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.643043972609652e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.1893874075879894e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.001063558622263372\n",
      "          total_loss: 0.016108820214867592\n",
      "          vf_explained_var: 0.2223730981349945\n",
      "          vf_loss: 0.015045266598463058\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.0809523809524\n",
      "    ram_util_percent: 95.67755102040816\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09415680914717851\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9014463015509119\n",
      "    mean_inference_ms: 1.8428651263903464\n",
      "    mean_raw_obs_processing_ms: 0.11869942517013633\n",
      "  time_since_restore: 11817.877908229828\n",
      "  time_this_iter_s: 218.19214582443237\n",
      "  time_total_s: 11817.877908229828\n",
      "  timers:\n",
      "    learn_throughput: 20.01\n",
      "    learn_time_ms: 199902.034\n",
      "    load_throughput: 7149280.266\n",
      "    load_time_ms: 0.559\n",
      "    sample_throughput: 18.683\n",
      "    sample_time_ms: 214092.685\n",
      "    update_time_ms: 16.0\n",
      "  timestamp: 1650230160\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 276.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.96\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 993\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9114307761192322\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006495847832411528\n",
      "          model: {}\n",
      "          policy_loss: -0.03767995536327362\n",
      "          total_loss: 0.056980714201927185\n",
      "          vf_explained_var: 0.4003326892852783\n",
      "          vf_loss: 0.0946606695652008\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.78430034129694\n",
      "    ram_util_percent: 95.66587030716722\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10400880464946897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9795829976819068\n",
      "    mean_inference_ms: 2.1046931610080826\n",
      "    mean_raw_obs_processing_ms: 0.2364954781753324\n",
      "  time_since_restore: 11832.399779319763\n",
      "  time_this_iter_s: 218.18209600448608\n",
      "  time_total_s: 11832.399779319763\n",
      "  timers:\n",
      "    learn_throughput: 20.035\n",
      "    learn_time_ms: 199647.526\n",
      "    load_throughput: 4405087.434\n",
      "    load_time_ms: 0.908\n",
      "    sample_throughput: 18.673\n",
      "    sample_time_ms: 214210.124\n",
      "    update_time_ms: 9.093\n",
      "  timestamp: 1650230172\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 22:18:45 (running for 03:20:05.35)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=1.96 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-19-40\n",
      "  done: false\n",
      "  episode_len_mean: 2016.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 110\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.551115205843844e-18\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.091653389593988e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.4996200548897741e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0043860687874257565\n",
      "          total_loss: 0.007900368422269821\n",
      "          vf_explained_var: 0.03089258261024952\n",
      "          vf_loss: 0.003514294046908617\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.12949152542372\n",
      "    ram_util_percent: 95.7677966101695\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09480738951316703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9057226540546806\n",
      "    mean_inference_ms: 1.8638401198914107\n",
      "    mean_raw_obs_processing_ms: 0.11955856086633725\n",
      "  time_since_restore: 12038.101151227951\n",
      "  time_this_iter_s: 220.22324299812317\n",
      "  time_total_s: 12038.101151227951\n",
      "  timers:\n",
      "    learn_throughput: 19.94\n",
      "    learn_time_ms: 200599.289\n",
      "    load_throughput: 7001008.179\n",
      "    load_time_ms: 0.571\n",
      "    sample_throughput: 18.615\n",
      "    sample_time_ms: 214885.783\n",
      "    update_time_ms: 16.722\n",
      "  timestamp: 1650230380\n",
      "  timesteps_since_restore: 224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-19-52\n",
      "  done: false\n",
      "  episode_len_mean: 286.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.17\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1008\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9529970288276672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007071098778396845\n",
      "          model: {}\n",
      "          policy_loss: -0.037975285202264786\n",
      "          total_loss: 0.04984859749674797\n",
      "          vf_explained_var: 0.5021094083786011\n",
      "          vf_loss: 0.08782388269901276\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.81389830508473\n",
      "    ram_util_percent: 95.78305084745763\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1041785716729126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9808763969341545\n",
      "    mean_inference_ms: 2.110376328764788\n",
      "    mean_raw_obs_processing_ms: 0.23641314155150275\n",
      "  time_since_restore: 12052.702195644379\n",
      "  time_this_iter_s: 220.30241632461548\n",
      "  time_total_s: 12052.702195644379\n",
      "  timers:\n",
      "    learn_throughput: 19.96\n",
      "    learn_time_ms: 200396.475\n",
      "    load_throughput: 4431031.878\n",
      "    load_time_ms: 0.903\n",
      "    sample_throughput: 18.605\n",
      "    sample_time_ms: 214990.997\n",
      "    update_time_ms: 9.338\n",
      "  timestamp: 1650230392\n",
      "  timesteps_since_restore: 224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-23-20\n",
      "  done: false\n",
      "  episode_len_mean: 2016.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 110\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.775557602921922e-18\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412825845181942\n",
      "          total_loss: 0.014223390258848667\n",
      "          vf_explained_var: -1.7407120367352036e-07\n",
      "          vf_loss: 9.512738324701786e-05\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.64169491525425\n",
      "    ram_util_percent: 95.78033898305085\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09480738951316703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9057226540546806\n",
      "    mean_inference_ms: 1.8638401198914107\n",
      "    mean_raw_obs_processing_ms: 0.11955856086633725\n",
      "  time_since_restore: 12258.041953325272\n",
      "  time_this_iter_s: 219.94080209732056\n",
      "  time_total_s: 12258.041953325272\n",
      "  timers:\n",
      "    learn_throughput: 19.897\n",
      "    learn_time_ms: 201039.712\n",
      "    load_throughput: 6843932.447\n",
      "    load_time_ms: 0.584\n",
      "    sample_throughput: 18.543\n",
      "    sample_time_ms: 215709.897\n",
      "    update_time_ms: 15.118\n",
      "  timestamp: 1650230600\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 289.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1021\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8742342591285706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00770309753715992\n",
      "          model: {}\n",
      "          policy_loss: -0.03367886692285538\n",
      "          total_loss: 0.03592212125658989\n",
      "          vf_explained_var: 0.6936516761779785\n",
      "          vf_loss: 0.06960099935531616\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.1\n",
      "    ram_util_percent: 95.7471186440678\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10433405821834991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9820988922791136\n",
      "    mean_inference_ms: 2.1158068015714235\n",
      "    mean_raw_obs_processing_ms: 0.23634301144095887\n",
      "  time_since_restore: 12272.91702580452\n",
      "  time_this_iter_s: 220.214830160141\n",
      "  time_total_s: 12272.91702580452\n",
      "  timers:\n",
      "    learn_throughput: 19.92\n",
      "    learn_time_ms: 200803.479\n",
      "    load_throughput: 4385512.338\n",
      "    load_time_ms: 0.912\n",
      "    sample_throughput: 18.525\n",
      "    sample_time_ms: 215923.756\n",
      "    update_time_ms: 8.195\n",
      "  timestamp: 1650230612\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 2016.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 113\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.387778801460961e-18\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1568693430634344e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.1684912457234886e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0001750412193359807\n",
      "          total_loss: 0.0014682863838970661\n",
      "          vf_explained_var: -0.0005852986359968781\n",
      "          vf_loss: 0.0012932521058246493\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.62199312714776\n",
      "    ram_util_percent: 95.46254295532646\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09543123757012996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9096862790710465\n",
      "    mean_inference_ms: 1.883133122175128\n",
      "    mean_raw_obs_processing_ms: 0.12027640164984672\n",
      "  time_since_restore: 12474.670852422714\n",
      "  time_this_iter_s: 216.62889909744263\n",
      "  time_total_s: 12474.670852422714\n",
      "  timers:\n",
      "    learn_throughput: 19.842\n",
      "    learn_time_ms: 201591.881\n",
      "    load_throughput: 6897391.876\n",
      "    load_time_ms: 0.58\n",
      "    sample_throughput: 18.495\n",
      "    sample_time_ms: 216276.836\n",
      "    update_time_ms: 15.952\n",
      "  timestamp: 1650230817\n",
      "  timesteps_since_restore: 232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 293.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1036\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8757921457290649\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00607033958658576\n",
      "          model: {}\n",
      "          policy_loss: -0.03653807193040848\n",
      "          total_loss: 0.09320281445980072\n",
      "          vf_explained_var: 0.3774176239967346\n",
      "          vf_loss: 0.1297408938407898\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.8195205479452\n",
      "    ram_util_percent: 95.44143835616438\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10452564134689847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.98350923227357\n",
      "    mean_inference_ms: 2.1225196186587114\n",
      "    mean_raw_obs_processing_ms: 0.23625964736262556\n",
      "  time_since_restore: 12489.800286769867\n",
      "  time_this_iter_s: 216.8832609653473\n",
      "  time_total_s: 12489.800286769867\n",
      "  timers:\n",
      "    learn_throughput: 19.866\n",
      "    learn_time_ms: 201350.313\n",
      "    load_throughput: 3929366.466\n",
      "    load_time_ms: 1.018\n",
      "    sample_throughput: 18.484\n",
      "    sample_time_ms: 216408.111\n",
      "    update_time_ms: 7.664\n",
      "  timestamp: 1650230829\n",
      "  timesteps_since_restore: 232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-30-34\n",
      "  done: false\n",
      "  episode_len_mean: 2016.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 113\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.938894007304805e-19\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.01412814762443304\n",
      "          total_loss: 0.014571044594049454\n",
      "          vf_explained_var: 3.2430055085796994e-08\n",
      "          vf_loss: 0.0004428975807968527\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.42611683848797\n",
      "    ram_util_percent: 95.53917525773197\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09543123757012996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9096862790710465\n",
      "    mean_inference_ms: 1.883133122175128\n",
      "    mean_raw_obs_processing_ms: 0.12027640164984672\n",
      "  time_since_restore: 12691.674463272095\n",
      "  time_this_iter_s: 217.0036108493805\n",
      "  time_total_s: 12691.674463272095\n",
      "  timers:\n",
      "    learn_throughput: 19.776\n",
      "    learn_time_ms: 202266.2\n",
      "    load_throughput: 6691080.801\n",
      "    load_time_ms: 0.598\n",
      "    sample_throughput: 18.439\n",
      "    sample_time_ms: 216937.214\n",
      "    update_time_ms: 15.472\n",
      "  timestamp: 1650231034\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-30-46\n",
      "  done: false\n",
      "  episode_len_mean: 295.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1051\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.935067892074585\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0065119918435812\n",
      "          model: {}\n",
      "          policy_loss: -0.03426196053624153\n",
      "          total_loss: 0.04868121072649956\n",
      "          vf_explained_var: 0.3806096315383911\n",
      "          vf_loss: 0.08294317126274109\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.2384879725086\n",
      "    ram_util_percent: 95.53883161512027\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1047408633380052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9849615305764374\n",
      "    mean_inference_ms: 2.129576373855476\n",
      "    mean_raw_obs_processing_ms: 0.23621641625592638\n",
      "  time_since_restore: 12706.616949558258\n",
      "  time_this_iter_s: 216.8166627883911\n",
      "  time_total_s: 12706.616949558258\n",
      "  timers:\n",
      "    learn_throughput: 19.794\n",
      "    learn_time_ms: 202079.456\n",
      "    load_throughput: 3914788.128\n",
      "    load_time_ms: 1.022\n",
      "    sample_throughput: 18.426\n",
      "    sample_time_ms: 217083.66\n",
      "    update_time_ms: 7.54\n",
      "  timestamp: 1650231046\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 2016.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 113\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4694470036524025e-19\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128189533948898\n",
      "          total_loss: 0.01436292752623558\n",
      "          vf_explained_var: 4.5055983832753554e-08\n",
      "          vf_loss: 0.0002347285917494446\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.75342465753425\n",
      "    ram_util_percent: 95.6708904109589\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09543123757012996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9096862790710465\n",
      "    mean_inference_ms: 1.883133122175128\n",
      "    mean_raw_obs_processing_ms: 0.12027640164984672\n",
      "  time_since_restore: 12908.911729574203\n",
      "  time_this_iter_s: 217.23726630210876\n",
      "  time_total_s: 12908.911729574203\n",
      "  timers:\n",
      "    learn_throughput: 19.753\n",
      "    learn_time_ms: 202504.102\n",
      "    load_throughput: 6712497.399\n",
      "    load_time_ms: 0.596\n",
      "    sample_throughput: 18.372\n",
      "    sample_time_ms: 217724.402\n",
      "    update_time_ms: 15.104\n",
      "  timestamp: 1650231251\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 300.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1065\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9578178524971008\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006320374086499214\n",
      "          model: {}\n",
      "          policy_loss: -0.04072797670960426\n",
      "          total_loss: 0.05566733703017235\n",
      "          vf_explained_var: 0.5002691745758057\n",
      "          vf_loss: 0.09639531373977661\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.85670103092785\n",
      "    ram_util_percent: 95.67594501718212\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1049391415482558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9862995593588121\n",
      "    mean_inference_ms: 2.1361658698516304\n",
      "    mean_raw_obs_processing_ms: 0.23616712130191544\n",
      "  time_since_restore: 12923.913428544998\n",
      "  time_this_iter_s: 217.2964789867401\n",
      "  time_total_s: 12923.913428544998\n",
      "  timers:\n",
      "    learn_throughput: 19.776\n",
      "    learn_time_ms: 202266.523\n",
      "    load_throughput: 3682605.909\n",
      "    load_time_ms: 1.086\n",
      "    sample_throughput: 18.354\n",
      "    sample_time_ms: 217937.151\n",
      "    update_time_ms: 7.548\n",
      "  timestamp: 1650231264\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 22:35:25 (running for 03:36:45.39)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=2.44 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-37-46\n",
      "  done: false\n",
      "  episode_len_mean: 2114.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 115\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7347235018262012e-19\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.466551583589109e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.024758267234903e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0027915050741285086\n",
      "          total_loss: -0.0017570741474628448\n",
      "          vf_explained_var: 0.03265037387609482\n",
      "          vf_loss: 0.0010344305774196982\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.79722222222223\n",
      "    ram_util_percent: 95.62673611111111\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09585676727007102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9124006658950133\n",
      "    mean_inference_ms: 1.8962231227511905\n",
      "    mean_raw_obs_processing_ms: 0.12069937480874483\n",
      "  time_since_restore: 13123.682570695877\n",
      "  time_this_iter_s: 214.77084112167358\n",
      "  time_total_s: 13123.682570695877\n",
      "  timers:\n",
      "    learn_throughput: 19.784\n",
      "    learn_time_ms: 202184.027\n",
      "    load_throughput: 6571311.739\n",
      "    load_time_ms: 0.609\n",
      "    sample_throughput: 18.341\n",
      "    sample_time_ms: 218091.485\n",
      "    update_time_ms: 14.105\n",
      "  timestamp: 1650231466\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-37-58\n",
      "  done: false\n",
      "  episode_len_mean: 308.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.58\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1079\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0067999362945557\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00626898230984807\n",
      "          model: {}\n",
      "          policy_loss: -0.04701756313443184\n",
      "          total_loss: 0.07189863175153732\n",
      "          vf_explained_var: 0.40669023990631104\n",
      "          vf_loss: 0.11891620606184006\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.01319444444444\n",
      "    ram_util_percent: 95.64583333333333\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10515348158454393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9877292876423976\n",
      "    mean_inference_ms: 2.1429125995048053\n",
      "    mean_raw_obs_processing_ms: 0.23610153156963484\n",
      "  time_since_restore: 13138.279306650162\n",
      "  time_this_iter_s: 214.36587810516357\n",
      "  time_total_s: 13138.279306650162\n",
      "  timers:\n",
      "    learn_throughput: 19.819\n",
      "    learn_time_ms: 201825.629\n",
      "    load_throughput: 5429519.741\n",
      "    load_time_ms: 0.737\n",
      "    sample_throughput: 18.326\n",
      "    sample_time_ms: 218264.521\n",
      "    update_time_ms: 7.68\n",
      "  timestamp: 1650231478\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-41-18\n",
      "  done: false\n",
      "  episode_len_mean: 2114.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 115\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.673617509131006e-20\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0141280023381114\n",
      "          total_loss: -0.014109415002167225\n",
      "          vf_explained_var: 2.52518610466268e-08\n",
      "          vf_loss: 1.8582977645564824e-05\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.06643109540637\n",
      "    ram_util_percent: 95.6713780918728\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09585676727007102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9124006658950133\n",
      "    mean_inference_ms: 1.8962231227511905\n",
      "    mean_raw_obs_processing_ms: 0.12069937480874483\n",
      "  time_since_restore: 13336.038546800613\n",
      "  time_this_iter_s: 212.35597610473633\n",
      "  time_total_s: 13336.038546800613\n",
      "  timers:\n",
      "    learn_throughput: 19.841\n",
      "    learn_time_ms: 201606.618\n",
      "    load_throughput: 6584464.678\n",
      "    load_time_ms: 0.607\n",
      "    sample_throughput: 18.368\n",
      "    sample_time_ms: 217774.281\n",
      "    update_time_ms: 13.966\n",
      "  timestamp: 1650231678\n",
      "  timesteps_since_restore: 248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-41-31\n",
      "  done: false\n",
      "  episode_len_mean: 311.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.63\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1092\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.961570143699646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006491522770375013\n",
      "          model: {}\n",
      "          policy_loss: -0.04615931212902069\n",
      "          total_loss: 0.03748105466365814\n",
      "          vf_explained_var: 0.5198001265525818\n",
      "          vf_loss: 0.08364037424325943\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.12112676056339\n",
      "    ram_util_percent: 95.7225352112676\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10532407177973217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9888744682481102\n",
      "    mean_inference_ms: 2.1486777320322976\n",
      "    mean_raw_obs_processing_ms: 0.23601022394572002\n",
      "  time_since_restore: 13351.363723754883\n",
      "  time_this_iter_s: 213.08441710472107\n",
      "  time_total_s: 13351.363723754883\n",
      "  timers:\n",
      "    learn_throughput: 19.869\n",
      "    learn_time_ms: 201321.585\n",
      "    load_throughput: 5402072.319\n",
      "    load_time_ms: 0.74\n",
      "    sample_throughput: 18.361\n",
      "    sample_time_ms: 217854.121\n",
      "    update_time_ms: 8.128\n",
      "  timestamp: 1650231691\n",
      "  timesteps_since_restore: 248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-44-56\n",
      "  done: false\n",
      "  episode_len_mean: 2212.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 119\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.336808754565503e-20\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.15394972208214e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.1755104355084e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0021996262948960066\n",
      "          total_loss: 0.0032007298432290554\n",
      "          vf_explained_var: 0.0034982094075530767\n",
      "          vf_loss: 0.0010011011036112905\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.56872852233677\n",
      "    ram_util_percent: 95.79072164948452\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09672152497820129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9179266685222504\n",
      "    mean_inference_ms: 1.922879495840553\n",
      "    mean_raw_obs_processing_ms: 0.12155559297054594\n",
      "  time_since_restore: 13554.03264594078\n",
      "  time_this_iter_s: 217.99409914016724\n",
      "  time_total_s: 13554.03264594078\n",
      "  timers:\n",
      "    learn_throughput: 19.859\n",
      "    learn_time_ms: 201421.352\n",
      "    load_throughput: 6512641.59\n",
      "    load_time_ms: 0.614\n",
      "    sample_throughput: 18.407\n",
      "    sample_time_ms: 217307.608\n",
      "    update_time_ms: 13.426\n",
      "  timestamp: 1650231896\n",
      "  timesteps_since_restore: 252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 313.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.66\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1106\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9856714010238647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006466224789619446\n",
      "          model: {}\n",
      "          policy_loss: -0.027748769149184227\n",
      "          total_loss: 0.11784171313047409\n",
      "          vf_explained_var: 0.41119706630706787\n",
      "          vf_loss: 0.14559048414230347\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.0152249134948\n",
      "    ram_util_percent: 95.77058823529411\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10551366375573501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9900296022292736\n",
      "    mean_inference_ms: 2.155000065203835\n",
      "    mean_raw_obs_processing_ms: 0.23589713422995925\n",
      "  time_since_restore: 13568.75452375412\n",
      "  time_this_iter_s: 217.39079999923706\n",
      "  time_total_s: 13568.75452375412\n",
      "  timers:\n",
      "    learn_throughput: 19.897\n",
      "    learn_time_ms: 201031.842\n",
      "    load_throughput: 5296841.574\n",
      "    load_time_ms: 0.755\n",
      "    sample_throughput: 18.402\n",
      "    sample_time_ms: 217370.726\n",
      "    update_time_ms: 8.348\n",
      "  timestamp: 1650231909\n",
      "  timesteps_since_restore: 252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-48-28\n",
      "  done: false\n",
      "  episode_len_mean: 2114.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 122\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1684043772827515e-20\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.3864425945789581e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.297373564428907e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.000934828829485923\n",
      "          total_loss: 0.001696153311058879\n",
      "          vf_explained_var: -0.001347344950772822\n",
      "          vf_loss: 0.000761323724873364\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.07526501766785\n",
      "    ram_util_percent: 95.63533568904593\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09735501030554831\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9218881995663888\n",
      "    mean_inference_ms: 1.941923539987298\n",
      "    mean_raw_obs_processing_ms: 0.12223008718060813\n",
      "  time_since_restore: 13765.227982282639\n",
      "  time_this_iter_s: 211.1953363418579\n",
      "  time_total_s: 13765.227982282639\n",
      "  timers:\n",
      "    learn_throughput: 19.904\n",
      "    learn_time_ms: 200963.102\n",
      "    load_throughput: 6564369.669\n",
      "    load_time_ms: 0.609\n",
      "    sample_throughput: 18.425\n",
      "    sample_time_ms: 217092.48\n",
      "    update_time_ms: 13.375\n",
      "  timestamp: 1650232108\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-48-40\n",
      "  done: false\n",
      "  episode_len_mean: 318.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.69\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1119\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9725609421730042\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007476589642465115\n",
      "          model: {}\n",
      "          policy_loss: -0.04045772925019264\n",
      "          total_loss: 0.08845297992229462\n",
      "          vf_explained_var: 0.5242409706115723\n",
      "          vf_loss: 0.12891069054603577\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.6148409893993\n",
      "    ram_util_percent: 95.65512367491166\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10568235874634403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9910147582044937\n",
      "    mean_inference_ms: 2.1606762493499527\n",
      "    mean_raw_obs_processing_ms: 0.23580197491088487\n",
      "  time_since_restore: 13780.575899839401\n",
      "  time_this_iter_s: 211.82137608528137\n",
      "  time_total_s: 13780.575899839401\n",
      "  timers:\n",
      "    learn_throughput: 19.936\n",
      "    learn_time_ms: 200642.057\n",
      "    load_throughput: 4090108.486\n",
      "    load_time_ms: 0.978\n",
      "    sample_throughput: 18.427\n",
      "    sample_time_ms: 217076.638\n",
      "    update_time_ms: 7.679\n",
      "  timestamp: 1650232120\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 22:52:05 (running for 03:53:25.57)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=2.69 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-52-05\n",
      "  done: false\n",
      "  episode_len_mean: 2114.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 122\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0842021886413758e-20\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128024689853191\n",
      "          total_loss: 0.014131629839539528\n",
      "          vf_explained_var: 4.845921921514673e-07\n",
      "          vf_loss: 3.5991358799947193e-06\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.54810996563575\n",
      "    ram_util_percent: 95.80274914089347\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09735501030554831\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9218881995663888\n",
      "    mean_inference_ms: 1.941923539987298\n",
      "    mean_raw_obs_processing_ms: 0.12223008718060813\n",
      "  time_since_restore: 13982.939736366272\n",
      "  time_this_iter_s: 217.71175408363342\n",
      "  time_total_s: 13982.939736366272\n",
      "  timers:\n",
      "    learn_throughput: 19.909\n",
      "    learn_time_ms: 200916.728\n",
      "    load_throughput: 6575947.948\n",
      "    load_time_ms: 0.608\n",
      "    sample_throughput: 18.464\n",
      "    sample_time_ms: 216633.699\n",
      "    update_time_ms: 11.687\n",
      "  timestamp: 1650232325\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-52-18\n",
      "  done: false\n",
      "  episode_len_mean: 324.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 2.81\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1131\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9942981600761414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007070763967931271\n",
      "          model: {}\n",
      "          policy_loss: -0.039793189615011215\n",
      "          total_loss: 0.10597982257604599\n",
      "          vf_explained_var: 0.5049830079078674\n",
      "          vf_loss: 0.1457730233669281\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.65910652920962\n",
      "    ram_util_percent: 95.79037800687286\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10583449265115757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9919689854571777\n",
      "    mean_inference_ms: 2.1656818275760195\n",
      "    mean_raw_obs_processing_ms: 0.2357158127355644\n",
      "  time_since_restore: 13997.841981887817\n",
      "  time_this_iter_s: 217.26608204841614\n",
      "  time_total_s: 13997.841981887817\n",
      "  timers:\n",
      "    learn_throughput: 19.939\n",
      "    learn_time_ms: 200607.082\n",
      "    load_throughput: 4160293.6\n",
      "    load_time_ms: 0.961\n",
      "    sample_throughput: 18.465\n",
      "    sample_time_ms: 216631.072\n",
      "    update_time_ms: 7.758\n",
      "  timestamp: 1650232338\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 2212.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.25\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 123\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.421010943206879e-21\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6763036125707022e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2549012737503262e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.00034556197351776063\n",
      "          total_loss: 0.0006921051535755396\n",
      "          vf_explained_var: 0.03071591816842556\n",
      "          vf_loss: 0.0010376612190157175\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.814\n",
      "    ram_util_percent: 95.81833333333333\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09756599339957134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9231696180602289\n",
      "    mean_inference_ms: 1.9480096390313753\n",
      "    mean_raw_obs_processing_ms: 0.12242960849769234\n",
      "  time_since_restore: 14206.475710391998\n",
      "  time_this_iter_s: 223.53597402572632\n",
      "  time_total_s: 14206.475710391998\n",
      "  timers:\n",
      "    learn_throughput: 19.877\n",
      "    learn_time_ms: 201239.562\n",
      "    load_throughput: 6637607.216\n",
      "    load_time_ms: 0.603\n",
      "    sample_throughput: 18.467\n",
      "    sample_time_ms: 216598.88\n",
      "    update_time_ms: 11.852\n",
      "  timestamp: 1650232549\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-56-01\n",
      "  done: false\n",
      "  episode_len_mean: 327.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 2.86\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1143\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9698800444602966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00653485395014286\n",
      "          model: {}\n",
      "          policy_loss: -0.03729655593633652\n",
      "          total_loss: 0.07223816961050034\n",
      "          vf_explained_var: 0.41576361656188965\n",
      "          vf_loss: 0.10953471809625626\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.71003344481606\n",
      "    ram_util_percent: 95.81204013377926\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10598604779123567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9928908555283971\n",
      "    mean_inference_ms: 2.170618010324738\n",
      "    mean_raw_obs_processing_ms: 0.2355893114978925\n",
      "  time_since_restore: 14221.37495970726\n",
      "  time_this_iter_s: 223.53297781944275\n",
      "  time_total_s: 14221.37495970726\n",
      "  timers:\n",
      "    learn_throughput: 19.909\n",
      "    learn_time_ms: 200916.297\n",
      "    load_throughput: 4147643.016\n",
      "    load_time_ms: 0.964\n",
      "    sample_throughput: 18.466\n",
      "    sample_time_ms: 216611.317\n",
      "    update_time_ms: 7.836\n",
      "  timestamp: 1650232561\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-59-33\n",
      "  done: false\n",
      "  episode_len_mean: 2212.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.25\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 123\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.7105054716034394e-21\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014127988368272781\n",
      "          total_loss: -0.01412725169211626\n",
      "          vf_explained_var: -2.215882886957843e-06\n",
      "          vf_loss: 7.35064475065883e-07\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.52866666666668\n",
      "    ram_util_percent: 95.74033333333333\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09756599339957134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9231696180602289\n",
      "    mean_inference_ms: 1.9480096390313753\n",
      "    mean_raw_obs_processing_ms: 0.12242960849769234\n",
      "  time_since_restore: 14430.62782216072\n",
      "  time_this_iter_s: 224.15211176872253\n",
      "  time_total_s: 14430.62782216072\n",
      "  timers:\n",
      "    learn_throughput: 19.836\n",
      "    learn_time_ms: 201657.649\n",
      "    load_throughput: 6723526.63\n",
      "    load_time_ms: 0.595\n",
      "    sample_throughput: 18.44\n",
      "    sample_time_ms: 216914.199\n",
      "    update_time_ms: 12.596\n",
      "  timestamp: 1650232773\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_22-59-45\n",
      "  done: false\n",
      "  episode_len_mean: 336.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1157\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9838929176330566\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0063950782641768456\n",
      "          model: {}\n",
      "          policy_loss: -0.04003961384296417\n",
      "          total_loss: 0.08775291591882706\n",
      "          vf_explained_var: 0.3733590841293335\n",
      "          vf_loss: 0.12779253721237183\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.83133333333333\n",
      "    ram_util_percent: 95.75\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10614963728232787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9938662755052995\n",
      "    mean_inference_ms: 2.1760518516710565\n",
      "    mean_raw_obs_processing_ms: 0.23539387589177974\n",
      "  time_since_restore: 14445.07788181305\n",
      "  time_this_iter_s: 223.70292210578918\n",
      "  time_total_s: 14445.07788181305\n",
      "  timers:\n",
      "    learn_throughput: 19.869\n",
      "    learn_time_ms: 201320.364\n",
      "    load_throughput: 4137720.67\n",
      "    load_time_ms: 0.967\n",
      "    sample_throughput: 18.445\n",
      "    sample_time_ms: 216856.348\n",
      "    update_time_ms: 8.155\n",
      "  timestamp: 1650232785\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 2212.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.25\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 123\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3552527358017197e-21\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128133654594421\n",
      "          total_loss: -0.014128082431852818\n",
      "          vf_explained_var: -8.082582098722924e-06\n",
      "          vf_loss: 5.208757158925437e-08\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.76912280701755\n",
      "    ram_util_percent: 95.6259649122807\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09756599339957134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9231696180602289\n",
      "    mean_inference_ms: 1.9480096390313753\n",
      "    mean_raw_obs_processing_ms: 0.12242960849769234\n",
      "  time_since_restore: 14642.941743135452\n",
      "  time_this_iter_s: 212.31392097473145\n",
      "  time_total_s: 14642.941743135452\n",
      "  timers:\n",
      "    learn_throughput: 19.874\n",
      "    learn_time_ms: 201271.293\n",
      "    load_throughput: 6672718.45\n",
      "    load_time_ms: 0.599\n",
      "    sample_throughput: 18.408\n",
      "    sample_time_ms: 217297.325\n",
      "    update_time_ms: 12.198\n",
      "  timestamp: 1650232986\n",
      "  timesteps_since_restore: 272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-03-18\n",
      "  done: false\n",
      "  episode_len_mean: 335.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1170\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9925853610038757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00663350336253643\n",
      "          model: {}\n",
      "          policy_loss: -0.04217292740941048\n",
      "          total_loss: 0.07834736257791519\n",
      "          vf_explained_var: 0.45419180393218994\n",
      "          vf_loss: 0.12052027881145477\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.28181818181818\n",
      "    ram_util_percent: 95.60629370629371\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10627879682816684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9946802097308128\n",
      "    mean_inference_ms: 2.1805816590408056\n",
      "    mean_raw_obs_processing_ms: 0.23516623125812872\n",
      "  time_since_restore: 14658.462847948074\n",
      "  time_this_iter_s: 213.38496613502502\n",
      "  time_total_s: 14658.462847948074\n",
      "  timers:\n",
      "    learn_throughput: 19.896\n",
      "    learn_time_ms: 201049.37\n",
      "    load_throughput: 4636511.262\n",
      "    load_time_ms: 0.863\n",
      "    sample_throughput: 18.418\n",
      "    sample_time_ms: 217181.941\n",
      "    update_time_ms: 8.305\n",
      "  timestamp: 1650232998\n",
      "  timesteps_since_restore: 272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 2114.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 136\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.776263679008599e-22\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4848614485893124e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.8440569291143845e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.003605287754908204\n",
      "          total_loss: 0.01025493536144495\n",
      "          vf_explained_var: 0.3423989713191986\n",
      "          vf_loss: 0.006649641785770655\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.94061433447098\n",
      "    ram_util_percent: 95.74368600682594\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10016053658740534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9388079237983719\n",
      "    mean_inference_ms: 2.0224929309378097\n",
      "    mean_raw_obs_processing_ms: 0.12508840912954072\n",
      "  time_since_restore: 14860.89244222641\n",
      "  time_this_iter_s: 217.95069909095764\n",
      "  time_total_s: 14860.89244222641\n",
      "  timers:\n",
      "    learn_throughput: 19.866\n",
      "    learn_time_ms: 201347.433\n",
      "    load_throughput: 6894557.409\n",
      "    load_time_ms: 0.58\n",
      "    sample_throughput: 18.439\n",
      "    sample_time_ms: 216926.04\n",
      "    update_time_ms: 12.991\n",
      "  timestamp: 1650233204\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-06-56\n",
      "  done: false\n",
      "  episode_len_mean: 337.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 2.99\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1183\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9730257391929626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009240948595106602\n",
      "          model: {}\n",
      "          policy_loss: -0.04381515458226204\n",
      "          total_loss: 0.08377984166145325\n",
      "          vf_explained_var: 0.3655433654785156\n",
      "          vf_loss: 0.127594992518425\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.58424657534246\n",
      "    ram_util_percent: 95.7664383561644\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10640072037494969\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9954529304885412\n",
      "    mean_inference_ms: 2.185077517705949\n",
      "    mean_raw_obs_processing_ms: 0.23494603259211416\n",
      "  time_since_restore: 14876.04375576973\n",
      "  time_this_iter_s: 217.58090782165527\n",
      "  time_total_s: 14876.04375576973\n",
      "  timers:\n",
      "    learn_throughput: 19.885\n",
      "    learn_time_ms: 201159.395\n",
      "    load_throughput: 4650133.319\n",
      "    load_time_ms: 0.86\n",
      "    sample_throughput: 18.444\n",
      "    sample_time_ms: 216875.282\n",
      "    update_time_ms: 9.985\n",
      "  timestamp: 1650233216\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 23:08:46 (running for 04:10:06.25)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=2.99 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-10-21\n",
      "  done: false\n",
      "  episode_len_mean: 2114.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 136\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3881318395042993e-22\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128096401691437\n",
      "          total_loss: 0.014162205159664154\n",
      "          vf_explained_var: 5.106772391627601e-07\n",
      "          vf_loss: 3.410472709219903e-05\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.93356401384084\n",
      "    ram_util_percent: 95.69377162629758\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10016053658740534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9388079237983719\n",
      "    mean_inference_ms: 2.0224929309378097\n",
      "    mean_raw_obs_processing_ms: 0.12508840912954072\n",
      "  time_since_restore: 15077.61583828926\n",
      "  time_this_iter_s: 216.72339606285095\n",
      "  time_total_s: 15077.61583828926\n",
      "  timers:\n",
      "    learn_throughput: 19.871\n",
      "    learn_time_ms: 201296.923\n",
      "    load_throughput: 6798450.442\n",
      "    load_time_ms: 0.588\n",
      "    sample_throughput: 18.433\n",
      "    sample_time_ms: 217004.865\n",
      "    update_time_ms: 13.405\n",
      "  timestamp: 1650233421\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-10-33\n",
      "  done: false\n",
      "  episode_len_mean: 343.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.09\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1195\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9495301246643066\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007584910839796066\n",
      "          model: {}\n",
      "          policy_loss: -0.04745848476886749\n",
      "          total_loss: 0.07491717487573624\n",
      "          vf_explained_var: 0.44149941205978394\n",
      "          vf_loss: 0.12237565219402313\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.0553633217993\n",
      "    ram_util_percent: 95.66678200692041\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10651915846666292\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9961890587524244\n",
      "    mean_inference_ms: 2.1891738630355797\n",
      "    mean_raw_obs_processing_ms: 0.23474370098197317\n",
      "  time_since_restore: 15092.88959980011\n",
      "  time_this_iter_s: 216.84584403038025\n",
      "  time_total_s: 15092.88959980011\n",
      "  timers:\n",
      "    learn_throughput: 19.886\n",
      "    learn_time_ms: 201145.438\n",
      "    load_throughput: 5064208.397\n",
      "    load_time_ms: 0.79\n",
      "    sample_throughput: 18.437\n",
      "    sample_time_ms: 216955.315\n",
      "    update_time_ms: 10.062\n",
      "  timestamp: 1650233433\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-13-59\n",
      "  done: false\n",
      "  episode_len_mean: 2114.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 136\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6940659197521496e-22\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128115028142929\n",
      "          total_loss: 0.014133375138044357\n",
      "          vf_explained_var: -3.6647242040999117e-07\n",
      "          vf_loss: 5.260972102405503e-06\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.17739726027396\n",
      "    ram_util_percent: 95.60958904109589\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10016053658740534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9388079237983719\n",
      "    mean_inference_ms: 2.0224929309378097\n",
      "    mean_raw_obs_processing_ms: 0.12508840912954072\n",
      "  time_since_restore: 15296.319731712341\n",
      "  time_this_iter_s: 218.70389342308044\n",
      "  time_total_s: 15296.319731712341\n",
      "  timers:\n",
      "    learn_throughput: 19.835\n",
      "    learn_time_ms: 201665.914\n",
      "    load_throughput: 6636294.45\n",
      "    load_time_ms: 0.603\n",
      "    sample_throughput: 18.435\n",
      "    sample_time_ms: 216978.595\n",
      "    update_time_ms: 13.483\n",
      "  timestamp: 1650233639\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-14-12\n",
      "  done: false\n",
      "  episode_len_mean: 343.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1207\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9548934698104858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006431555841118097\n",
      "          model: {}\n",
      "          policy_loss: -0.05034713074564934\n",
      "          total_loss: 0.07665789127349854\n",
      "          vf_explained_var: 0.45073631405830383\n",
      "          vf_loss: 0.12700502574443817\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.75445205479451\n",
      "    ram_util_percent: 95.62876712328767\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1066414527636054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9969893466611738\n",
      "    mean_inference_ms: 2.1931003650129397\n",
      "    mean_raw_obs_processing_ms: 0.23455697259407665\n",
      "  time_since_restore: 15312.262431144714\n",
      "  time_this_iter_s: 219.3728313446045\n",
      "  time_total_s: 15312.262431144714\n",
      "  timers:\n",
      "    learn_throughput: 19.834\n",
      "    learn_time_ms: 201677.752\n",
      "    load_throughput: 5040019.226\n",
      "    load_time_ms: 0.794\n",
      "    sample_throughput: 18.441\n",
      "    sample_time_ms: 216911.434\n",
      "    update_time_ms: 9.648\n",
      "  timestamp: 1650233652\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-17-41\n",
      "  done: false\n",
      "  episode_len_mean: 1921.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 145\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.470329598760748e-23\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.3344716121740757e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.530170007775226e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.0021659107878804207\n",
      "          total_loss: 0.0009411005303263664\n",
      "          vf_explained_var: 0.11816898733377457\n",
      "          vf_loss: 0.00310700130648911\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.75117845117846\n",
      "    ram_util_percent: 95.72558922558923\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10166121983009313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9480179709663268\n",
      "    mean_inference_ms: 2.065685212222811\n",
      "    mean_raw_obs_processing_ms: 0.1266556217576989\n",
      "  time_since_restore: 15518.384570598602\n",
      "  time_this_iter_s: 222.064838886261\n",
      "  time_total_s: 15518.384570598602\n",
      "  timers:\n",
      "    learn_throughput: 19.749\n",
      "    learn_time_ms: 202544.835\n",
      "    load_throughput: 6642600.467\n",
      "    load_time_ms: 0.602\n",
      "    sample_throughput: 18.396\n",
      "    sample_time_ms: 217437.688\n",
      "    update_time_ms: 15.476\n",
      "  timestamp: 1650233861\n",
      "  timesteps_since_restore: 288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-17-54\n",
      "  done: false\n",
      "  episode_len_mean: 336.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 2.99\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1222\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9520698189735413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006640208885073662\n",
      "          model: {}\n",
      "          policy_loss: -0.03965945541858673\n",
      "          total_loss: 0.048718009144067764\n",
      "          vf_explained_var: 0.4608459174633026\n",
      "          vf_loss: 0.0883774608373642\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.3543918918919\n",
      "    ram_util_percent: 95.72195945945946\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10679308054864294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9979829350137354\n",
      "    mean_inference_ms: 2.19775366698878\n",
      "    mean_raw_obs_processing_ms: 0.23434421019141502\n",
      "  time_since_restore: 15533.40059518814\n",
      "  time_this_iter_s: 221.1381640434265\n",
      "  time_total_s: 15533.40059518814\n",
      "  timers:\n",
      "    learn_throughput: 19.758\n",
      "    learn_time_ms: 202447.875\n",
      "    load_throughput: 4008701.137\n",
      "    load_time_ms: 0.998\n",
      "    sample_throughput: 18.393\n",
      "    sample_time_ms: 217478.149\n",
      "    update_time_ms: 9.593\n",
      "  timestamp: 1650233874\n",
      "  timesteps_since_restore: 288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-21-14\n",
      "  done: false\n",
      "  episode_len_mean: 1921.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 145\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.235164799380374e-23\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014127925969660282\n",
      "          total_loss: 0.014128070324659348\n",
      "          vf_explained_var: 6.9984826041036285e-06\n",
      "          vf_loss: 1.4032609385594697e-07\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.02631578947368\n",
      "    ram_util_percent: 95.65298245614035\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10166121983009313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9480179709663268\n",
      "    mean_inference_ms: 2.065685212222811\n",
      "    mean_raw_obs_processing_ms: 0.1266556217576989\n",
      "  time_since_restore: 15731.168301820755\n",
      "  time_this_iter_s: 212.7837312221527\n",
      "  time_total_s: 15731.168301820755\n",
      "  timers:\n",
      "    learn_throughput: 19.794\n",
      "    learn_time_ms: 202081.664\n",
      "    load_throughput: 6818897.74\n",
      "    load_time_ms: 0.587\n",
      "    sample_throughput: 18.326\n",
      "    sample_time_ms: 218264.341\n",
      "    update_time_ms: 16.97\n",
      "  timestamp: 1650234074\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-21-26\n",
      "  done: false\n",
      "  episode_len_mean: 335.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1235\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8970523476600647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006943554617464542\n",
      "          model: {}\n",
      "          policy_loss: -0.03692839667201042\n",
      "          total_loss: 0.054680731147527695\n",
      "          vf_explained_var: 0.594374418258667\n",
      "          vf_loss: 0.09160912781953812\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.04770318021201\n",
      "    ram_util_percent: 95.63144876325089\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10691707269994026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.998736913830131\n",
      "    mean_inference_ms: 2.2014517785590297\n",
      "    mean_raw_obs_processing_ms: 0.23415655667975885\n",
      "  time_since_restore: 15745.80242228508\n",
      "  time_this_iter_s: 212.4018270969391\n",
      "  time_total_s: 15745.80242228508\n",
      "  timers:\n",
      "    learn_throughput: 19.8\n",
      "    learn_time_ms: 202015.537\n",
      "    load_throughput: 4054132.373\n",
      "    load_time_ms: 0.987\n",
      "    sample_throughput: 18.333\n",
      "    sample_time_ms: 218185.305\n",
      "    update_time_ms: 9.143\n",
      "  timestamp: 1650234086\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-24-47\n",
      "  done: false\n",
      "  episode_len_mean: 1921.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 145\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.117582399690187e-23\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128287322819233\n",
      "          total_loss: 0.014128305949270725\n",
      "          vf_explained_var: 1.8847886167350225e-05\n",
      "          vf_loss: 2.4210800475543692e-08\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.2718309859155\n",
      "    ram_util_percent: 95.58485915492957\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10166121983009313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9480179709663268\n",
      "    mean_inference_ms: 2.065685212222811\n",
      "    mean_raw_obs_processing_ms: 0.1266556217576989\n",
      "  time_since_restore: 15943.617467880249\n",
      "  time_this_iter_s: 212.44916605949402\n",
      "  time_total_s: 15943.617467880249\n",
      "  timers:\n",
      "    learn_throughput: 19.783\n",
      "    learn_time_ms: 202197.455\n",
      "    load_throughput: 6747321.938\n",
      "    load_time_ms: 0.593\n",
      "    sample_throughput: 18.365\n",
      "    sample_time_ms: 217810.721\n",
      "    update_time_ms: 16.559\n",
      "  timestamp: 1650234287\n",
      "  timesteps_since_restore: 296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-24-59\n",
      "  done: false\n",
      "  episode_len_mean: 333.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 2.93\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1248\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9565943479537964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006944833789020777\n",
      "          model: {}\n",
      "          policy_loss: -0.04133846238255501\n",
      "          total_loss: 0.03316749259829521\n",
      "          vf_explained_var: 0.5981687903404236\n",
      "          vf_loss: 0.07450596243143082\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.58807017543859\n",
      "    ram_util_percent: 95.59824561403508\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1070282077576262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9994359897599563\n",
      "    mean_inference_ms: 2.2048077194253977\n",
      "    mean_raw_obs_processing_ms: 0.23398104052291566\n",
      "  time_since_restore: 15958.675122261047\n",
      "  time_this_iter_s: 212.8726999759674\n",
      "  time_total_s: 15958.675122261047\n",
      "  timers:\n",
      "    learn_throughput: 19.785\n",
      "    learn_time_ms: 202169.718\n",
      "    load_throughput: 5237315.352\n",
      "    load_time_ms: 0.764\n",
      "    sample_throughput: 18.373\n",
      "    sample_time_ms: 217708.429\n",
      "    update_time_ms: 9.025\n",
      "  timestamp: 1650234299\n",
      "  timesteps_since_restore: 296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 23:25:26 (running for 04:26:46.43)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=2.93 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-28-17\n",
      "  done: false\n",
      "  episode_len_mean: 1824.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 159\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0587911998450935e-23\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.079566734499061e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.275068673120682e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.0037508492823690176\n",
      "          total_loss: -0.0011221945751458406\n",
      "          vf_explained_var: 0.341686874628067\n",
      "          vf_loss: 0.002628649352118373\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.15567375886525\n",
      "    ram_util_percent: 95.53049645390071\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10337218023024977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9590481122447322\n",
      "    mean_inference_ms: 2.1168241254074176\n",
      "    mean_raw_obs_processing_ms: 0.12877929308852681\n",
      "  time_since_restore: 16153.33451795578\n",
      "  time_this_iter_s: 209.717050075531\n",
      "  time_total_s: 16153.33451795578\n",
      "  timers:\n",
      "    learn_throughput: 19.859\n",
      "    learn_time_ms: 201418.157\n",
      "    load_throughput: 6758738.267\n",
      "    load_time_ms: 0.592\n",
      "    sample_throughput: 18.357\n",
      "    sample_time_ms: 217902.925\n",
      "    update_time_ms: 17.129\n",
      "  timestamp: 1650234497\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 343.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.11\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1260\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0043463706970215\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0068761738948524\n",
      "          model: {}\n",
      "          policy_loss: -0.05374912545084953\n",
      "          total_loss: 0.04846912622451782\n",
      "          vf_explained_var: 0.5955669283866882\n",
      "          vf_loss: 0.10221824795007706\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.2595744680851\n",
      "    ram_util_percent: 95.54184397163121\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10712987185299448\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0000835954301732\n",
      "    mean_inference_ms: 2.207805097298402\n",
      "    mean_raw_obs_processing_ms: 0.23383587438716355\n",
      "  time_since_restore: 16168.575951099396\n",
      "  time_this_iter_s: 209.9008288383484\n",
      "  time_total_s: 16168.575951099396\n",
      "  timers:\n",
      "    learn_throughput: 19.851\n",
      "    learn_time_ms: 201502.328\n",
      "    load_throughput: 5241242.112\n",
      "    load_time_ms: 0.763\n",
      "    sample_throughput: 18.366\n",
      "    sample_time_ms: 217792.706\n",
      "    update_time_ms: 9.019\n",
      "  timestamp: 1650234509\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-31-52\n",
      "  done: false\n",
      "  episode_len_mean: 1626.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 167\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.293955999225468e-24\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.7939739321154918e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.8675392151473148e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.00020936013606842607\n",
      "          total_loss: 0.000821391586214304\n",
      "          vf_explained_var: -0.004451356362551451\n",
      "          vf_loss: 0.0010307560442015529\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.18850174216027\n",
      "    ram_util_percent: 95.67839721254354\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10416710033384263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9642643137639355\n",
      "    mean_inference_ms: 2.139945823145417\n",
      "    mean_raw_obs_processing_ms: 0.12978363258903103\n",
      "  time_since_restore: 16369.05449104309\n",
      "  time_this_iter_s: 215.7199730873108\n",
      "  time_total_s: 16369.05449104309\n",
      "  timers:\n",
      "    learn_throughput: 19.934\n",
      "    learn_time_ms: 200660.474\n",
      "    load_throughput: 6666620.043\n",
      "    load_time_ms: 0.6\n",
      "    sample_throughput: 18.425\n",
      "    sample_time_ms: 217096.35\n",
      "    update_time_ms: 20.825\n",
      "  timestamp: 1650234712\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-32-05\n",
      "  done: false\n",
      "  episode_len_mean: 343.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1271\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9551234841346741\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006735638715326786\n",
      "          model: {}\n",
      "          policy_loss: -0.040456850081682205\n",
      "          total_loss: 0.050029974430799484\n",
      "          vf_explained_var: 0.600799560546875\n",
      "          vf_loss: 0.09048683196306229\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.63611111111112\n",
      "    ram_util_percent: 95.65972222222221\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10722455232627227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0006986630800345\n",
      "    mean_inference_ms: 2.2107376819642184\n",
      "    mean_raw_obs_processing_ms: 0.2336977745533251\n",
      "  time_since_restore: 16384.7472512722\n",
      "  time_this_iter_s: 216.1713001728058\n",
      "  time_total_s: 16384.7472512722\n",
      "  timers:\n",
      "    learn_throughput: 19.923\n",
      "    learn_time_ms: 200773.072\n",
      "    load_throughput: 4779288.97\n",
      "    load_time_ms: 0.837\n",
      "    sample_throughput: 18.423\n",
      "    sample_time_ms: 217117.815\n",
      "    update_time_ms: 8.801\n",
      "  timestamp: 1650234725\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-35-21\n",
      "  done: false\n",
      "  episode_len_mean: 1626.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 167\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.646977999612734e-24\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128144830465317\n",
      "          total_loss: 0.014128444716334343\n",
      "          vf_explained_var: 4.936161985824583e-06\n",
      "          vf_loss: 3.0405644224629214e-07\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.33594306049822\n",
      "    ram_util_percent: 95.43701067615658\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10416710033384263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9642643137639355\n",
      "    mean_inference_ms: 2.139945823145417\n",
      "    mean_raw_obs_processing_ms: 0.12978363258903103\n",
      "  time_since_restore: 16577.81476688385\n",
      "  time_this_iter_s: 208.76027584075928\n",
      "  time_total_s: 16577.81476688385\n",
      "  timers:\n",
      "    learn_throughput: 20.085\n",
      "    learn_time_ms: 199154.136\n",
      "    load_throughput: 6601564.492\n",
      "    load_time_ms: 0.606\n",
      "    sample_throughput: 18.491\n",
      "    sample_time_ms: 216315.737\n",
      "    update_time_ms: 19.746\n",
      "  timestamp: 1650234921\n",
      "  timesteps_since_restore: 308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-35-35\n",
      "  done: false\n",
      "  episode_len_mean: 343.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.19\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1284\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9541370272636414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006513635627925396\n",
      "          model: {}\n",
      "          policy_loss: -0.04360446333885193\n",
      "          total_loss: 0.05398591235280037\n",
      "          vf_explained_var: 0.5636221766471863\n",
      "          vf_loss: 0.0975903794169426\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.5153024911032\n",
      "    ram_util_percent: 95.41743772241992\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10732831226245412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0014127669039694\n",
      "    mean_inference_ms: 2.2140548282635155\n",
      "    mean_raw_obs_processing_ms: 0.23354673792547687\n",
      "  time_since_restore: 16594.116436958313\n",
      "  time_this_iter_s: 209.36918568611145\n",
      "  time_total_s: 16594.116436958313\n",
      "  timers:\n",
      "    learn_throughput: 20.071\n",
      "    learn_time_ms: 199290.924\n",
      "    load_throughput: 4828253.712\n",
      "    load_time_ms: 0.828\n",
      "    sample_throughput: 18.481\n",
      "    sample_time_ms: 216437.144\n",
      "    update_time_ms: 8.337\n",
      "  timestamp: 1650234935\n",
      "  timesteps_since_restore: 308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-38-51\n",
      "  done: false\n",
      "  episode_len_mean: 1722.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 170\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.323488999806367e-24\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.408696952496425e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.1820664663868874e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0038990702014416456\n",
      "          total_loss: 0.004025625996291637\n",
      "          vf_explained_var: 0.01091249193996191\n",
      "          vf_loss: 0.00012655511090997607\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.68892857142858\n",
      "    ram_util_percent: 95.53571428571429\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1044388026512354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9660490768604248\n",
      "    mean_inference_ms: 2.1477329824226126\n",
      "    mean_raw_obs_processing_ms: 0.1301008573982145\n",
      "  time_since_restore: 16787.077806949615\n",
      "  time_this_iter_s: 209.26304006576538\n",
      "  time_total_s: 16787.077806949615\n",
      "  timers:\n",
      "    learn_throughput: 20.118\n",
      "    learn_time_ms: 198828.02\n",
      "    load_throughput: 6761462.137\n",
      "    load_time_ms: 0.592\n",
      "    sample_throughput: 18.62\n",
      "    sample_time_ms: 214825.215\n",
      "    update_time_ms: 19.486\n",
      "  timestamp: 1650235131\n",
      "  timesteps_since_restore: 312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-39-05\n",
      "  done: false\n",
      "  episode_len_mean: 339.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 3.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1297\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9561980962753296\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007551001384854317\n",
      "          model: {}\n",
      "          policy_loss: -0.04042477533221245\n",
      "          total_loss: 0.04637109115719795\n",
      "          vf_explained_var: 0.480091392993927\n",
      "          vf_loss: 0.0867958590388298\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.67686832740212\n",
      "    ram_util_percent: 95.55017793594305\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1074242730088721\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.002101034824485\n",
      "    mean_inference_ms: 2.2173462003362543\n",
      "    mean_raw_obs_processing_ms: 0.23339011605690455\n",
      "  time_since_restore: 16804.448469161987\n",
      "  time_this_iter_s: 210.33203220367432\n",
      "  time_total_s: 16804.448469161987\n",
      "  timers:\n",
      "    learn_throughput: 20.113\n",
      "    learn_time_ms: 198876.008\n",
      "    load_throughput: 4818269.96\n",
      "    load_time_ms: 0.83\n",
      "    sample_throughput: 18.599\n",
      "    sample_time_ms: 215063.004\n",
      "    update_time_ms: 8.331\n",
      "  timestamp: 1650235145\n",
      "  timesteps_since_restore: 312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 23:42:07 (running for 04:43:26.77)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=3.1 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-42-20\n",
      "  done: false\n",
      "  episode_len_mean: 1722.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 170\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.617444999031835e-25\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.8260260176291852e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.109649298342799e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128084294497967\n",
      "          total_loss: -0.01409254688769579\n",
      "          vf_explained_var: 4.050552249168504e-08\n",
      "          vf_loss: 3.553957139956765e-05\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.56227758007117\n",
      "    ram_util_percent: 95.37722419928825\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1044388026512354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9660490768604248\n",
      "    mean_inference_ms: 2.1477329824226126\n",
      "    mean_raw_obs_processing_ms: 0.1301008573982145\n",
      "  time_since_restore: 16996.4425137043\n",
      "  time_this_iter_s: 209.36470675468445\n",
      "  time_total_s: 16996.4425137043\n",
      "  timers:\n",
      "    learn_throughput: 20.208\n",
      "    learn_time_ms: 197945.293\n",
      "    load_throughput: 6800655.047\n",
      "    load_time_ms: 0.588\n",
      "    sample_throughput: 18.646\n",
      "    sample_time_ms: 214523.404\n",
      "    update_time_ms: 19.177\n",
      "  timestamp: 1650235340\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 344.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.2\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1309\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9858977198600769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007601273246109486\n",
      "          model: {}\n",
      "          policy_loss: -0.051431234925985336\n",
      "          total_loss: 0.04573703184723854\n",
      "          vf_explained_var: 0.4913887679576874\n",
      "          vf_loss: 0.09716826677322388\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.48967971530249\n",
      "    ram_util_percent: 95.36120996441281\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10750087508714434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0028931986773737\n",
      "    mean_inference_ms: 2.220614989410181\n",
      "    mean_raw_obs_processing_ms: 0.23324454528289987\n",
      "  time_since_restore: 17013.684766054153\n",
      "  time_this_iter_s: 209.23629689216614\n",
      "  time_total_s: 17013.684766054153\n",
      "  timers:\n",
      "    learn_throughput: 20.209\n",
      "    learn_time_ms: 197932.569\n",
      "    load_throughput: 4829782.653\n",
      "    load_time_ms: 0.828\n",
      "    sample_throughput: 18.626\n",
      "    sample_time_ms: 214758.263\n",
      "    update_time_ms: 7.036\n",
      "  timestamp: 1650235354\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 1722.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 170\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3087224995159173e-25\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.8260260176291852e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.109649298342799e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014127840287983418\n",
      "          total_loss: -0.014121361076831818\n",
      "          vf_explained_var: -3.992870389879499e-08\n",
      "          vf_loss: 6.482475328084547e-06\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.0643109540636\n",
      "    ram_util_percent: 95.30883392226147\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1044388026512354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9660490768604248\n",
      "    mean_inference_ms: 2.1477329824226126\n",
      "    mean_raw_obs_processing_ms: 0.1301008573982145\n",
      "  time_since_restore: 17208.557174921036\n",
      "  time_this_iter_s: 212.11466121673584\n",
      "  time_total_s: 17208.557174921036\n",
      "  timers:\n",
      "    learn_throughput: 20.263\n",
      "    learn_time_ms: 197404.403\n",
      "    load_throughput: 6727301.014\n",
      "    load_time_ms: 0.595\n",
      "    sample_throughput: 18.716\n",
      "    sample_time_ms: 213717.445\n",
      "    update_time_ms: 18.708\n",
      "  timestamp: 1650235552\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-46-07\n",
      "  done: false\n",
      "  episode_len_mean: 351.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1321\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9979168176651001\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006660467479377985\n",
      "          model: {}\n",
      "          policy_loss: -0.040880002081394196\n",
      "          total_loss: 0.053519513458013535\n",
      "          vf_explained_var: 0.6244426965713501\n",
      "          vf_loss: 0.09439951926469803\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.28978873239437\n",
      "    ram_util_percent: 95.32077464788732\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10758783370780968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0037875693362763\n",
      "    mean_inference_ms: 2.224040476975203\n",
      "    mean_raw_obs_processing_ms: 0.2330993770077433\n",
      "  time_since_restore: 17226.684376955032\n",
      "  time_this_iter_s: 212.9996109008789\n",
      "  time_total_s: 17226.684376955032\n",
      "  timers:\n",
      "    learn_throughput: 20.256\n",
      "    learn_time_ms: 197475.58\n",
      "    load_throughput: 4803646.567\n",
      "    load_time_ms: 0.833\n",
      "    sample_throughput: 18.702\n",
      "    sample_time_ms: 213885.821\n",
      "    update_time_ms: 7.222\n",
      "  timestamp: 1650235567\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-49-23\n",
      "  done: false\n",
      "  episode_len_mean: 1526.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 180\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6543612497579586e-25\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.450299128410462e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.3392271148817502e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0035600883420556784\n",
      "          total_loss: 0.006303096655756235\n",
      "          vf_explained_var: 0.25292062759399414\n",
      "          vf_loss: 0.002743000164628029\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.21535714285716\n",
      "    ram_util_percent: 95.70464285714287\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10523339561417963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.971490142429254\n",
      "    mean_inference_ms: 2.1707442259926615\n",
      "    mean_raw_obs_processing_ms: 0.13108538370272996\n",
      "  time_since_restore: 17419.364020109177\n",
      "  time_this_iter_s: 210.80684518814087\n",
      "  time_total_s: 17419.364020109177\n",
      "  timers:\n",
      "    learn_throughput: 20.355\n",
      "    learn_time_ms: 196509.034\n",
      "    load_throughput: 6879009.389\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 18.756\n",
      "    sample_time_ms: 213264.312\n",
      "    update_time_ms: 19.364\n",
      "  timestamp: 1650235763\n",
      "  timesteps_since_restore: 324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 356.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1332\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.990699291229248\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00844452902674675\n",
      "          model: {}\n",
      "          policy_loss: -0.04462961107492447\n",
      "          total_loss: 0.046505093574523926\n",
      "          vf_explained_var: 0.6304334402084351\n",
      "          vf_loss: 0.09113471210002899\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.13936170212767\n",
      "    ram_util_percent: 95.70567375886525\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1076808677430207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0047276167123027\n",
      "    mean_inference_ms: 2.2274563908938796\n",
      "    mean_raw_obs_processing_ms: 0.23298776200713434\n",
      "  time_since_restore: 17438.55997276306\n",
      "  time_this_iter_s: 211.87559580802917\n",
      "  time_total_s: 17438.55997276306\n",
      "  timers:\n",
      "    learn_throughput: 20.343\n",
      "    learn_time_ms: 196625.164\n",
      "    load_throughput: 3406748.837\n",
      "    load_time_ms: 1.174\n",
      "    sample_throughput: 18.734\n",
      "    sample_time_ms: 213511.905\n",
      "    update_time_ms: 7.976\n",
      "  timestamp: 1650235779\n",
      "  timesteps_since_restore: 324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-52-55\n",
      "  done: false\n",
      "  episode_len_mean: 1429.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 183\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.271806248789793e-26\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0522375871709826e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.9672435073006776e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.002445037942379713\n",
      "          total_loss: 0.0027654608711600304\n",
      "          vf_explained_var: 0.14513073861598969\n",
      "          vf_loss: 0.005210507661104202\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.45845070422534\n",
      "    ram_util_percent: 95.58661971830986\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10542667124251767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9728763606103922\n",
      "    mean_inference_ms: 2.176462853642158\n",
      "    mean_raw_obs_processing_ms: 0.13133530095815268\n",
      "  time_since_restore: 17630.990747213364\n",
      "  time_this_iter_s: 211.626727104187\n",
      "  time_total_s: 17630.990747213364\n",
      "  timers:\n",
      "    learn_throughput: 20.483\n",
      "    learn_time_ms: 195279.349\n",
      "    load_throughput: 6879291.455\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 18.818\n",
      "    sample_time_ms: 212559.408\n",
      "    update_time_ms: 18.049\n",
      "  timestamp: 1650235975\n",
      "  timesteps_since_restore: 328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-53-09\n",
      "  done: false\n",
      "  episode_len_mean: 361.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1344\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0052107572555542\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006206912454217672\n",
      "          model: {}\n",
      "          policy_loss: -0.040784772485494614\n",
      "          total_loss: 0.05584097281098366\n",
      "          vf_explained_var: 0.5921141505241394\n",
      "          vf_loss: 0.09662573784589767\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.07224199288255\n",
      "    ram_util_percent: 95.59786476868328\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10779333069788744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0058974307840949\n",
      "    mean_inference_ms: 2.2316354819946027\n",
      "    mean_raw_obs_processing_ms: 0.23287490361875585\n",
      "  time_since_restore: 17648.150765180588\n",
      "  time_this_iter_s: 209.59079241752625\n",
      "  time_total_s: 17648.150765180588\n",
      "  timers:\n",
      "    learn_throughput: 20.476\n",
      "    learn_time_ms: 195346.912\n",
      "    load_throughput: 4011960.4\n",
      "    load_time_ms: 0.997\n",
      "    sample_throughput: 18.798\n",
      "    sample_time_ms: 212785.741\n",
      "    update_time_ms: 7.678\n",
      "  timestamp: 1650235989\n",
      "  timesteps_since_restore: 328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 1429.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 183\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.1359031243948966e-26\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0420779351677561e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.433347277443761e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128156006336212\n",
      "          total_loss: -0.01412754226475954\n",
      "          vf_explained_var: -1.4681329503218876e-06\n",
      "          vf_loss: 6.129879466243437e-07\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.12892857142856\n",
      "    ram_util_percent: 95.55642857142858\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10542667124251767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9728763606103922\n",
      "    mean_inference_ms: 2.176462853642158\n",
      "    mean_raw_obs_processing_ms: 0.13133530095815268\n",
      "  time_since_restore: 17839.93099117279\n",
      "  time_this_iter_s: 208.94024395942688\n",
      "  time_total_s: 17839.93099117279\n",
      "  timers:\n",
      "    learn_throughput: 20.531\n",
      "    learn_time_ms: 194827.621\n",
      "    load_throughput: 6739191.002\n",
      "    load_time_ms: 0.594\n",
      "    sample_throughput: 18.922\n",
      "    sample_time_ms: 211395.982\n",
      "    update_time_ms: 17.513\n",
      "  timestamp: 1650236184\n",
      "  timesteps_since_restore: 332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-56-38\n",
      "  done: false\n",
      "  episode_len_mean: 360.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.48\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1356\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9727231860160828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007841546088457108\n",
      "          model: {}\n",
      "          policy_loss: -0.04551554098725319\n",
      "          total_loss: 0.055620551109313965\n",
      "          vf_explained_var: 0.585191011428833\n",
      "          vf_loss: 0.10113608837127686\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.57571428571428\n",
      "    ram_util_percent: 95.58428571428571\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10790625390710253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.007120889251765\n",
      "    mean_inference_ms: 2.2360408508047813\n",
      "    mean_raw_obs_processing_ms: 0.23275416928877551\n",
      "  time_since_restore: 17857.34756207466\n",
      "  time_this_iter_s: 209.1967968940735\n",
      "  time_total_s: 17857.34756207466\n",
      "  timers:\n",
      "    learn_throughput: 20.518\n",
      "    learn_time_ms: 194949.246\n",
      "    load_throughput: 4031143.468\n",
      "    load_time_ms: 0.992\n",
      "    sample_throughput: 18.905\n",
      "    sample_time_ms: 211582.244\n",
      "    update_time_ms: 7.659\n",
      "  timestamp: 1650236198\n",
      "  timesteps_since_restore: 332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-17 23:58:47 (running for 05:00:07.27)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=3.48 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-17_23-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 1521.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 188\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0679515621974483e-26\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0499254234884409e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0211380985950244e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0011545683955773711\n",
      "          total_loss: 0.00240046507678926\n",
      "          vf_explained_var: 0.020736459642648697\n",
      "          vf_loss: 0.0012459084391593933\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.95658362989323\n",
      "    ram_util_percent: 95.6508896797153\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10574926380564804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9752589629359311\n",
      "    mean_inference_ms: 2.18617689046871\n",
      "    mean_raw_obs_processing_ms: 0.13175227969843978\n",
      "  time_since_restore: 18049.688652276993\n",
      "  time_this_iter_s: 209.75766110420227\n",
      "  time_total_s: 18049.688652276993\n",
      "  timers:\n",
      "    learn_throughput: 20.569\n",
      "    learn_time_ms: 194470.968\n",
      "    load_throughput: 6639445.961\n",
      "    load_time_ms: 0.602\n",
      "    sample_throughput: 18.955\n",
      "    sample_time_ms: 211031.263\n",
      "    update_time_ms: 17.512\n",
      "  timestamp: 1650236394\n",
      "  timesteps_since_restore: 336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-00-09\n",
      "  done: false\n",
      "  episode_len_mean: 365.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.51\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1367\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.027572751045227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007430088706314564\n",
      "          model: {}\n",
      "          policy_loss: -0.04859093576669693\n",
      "          total_loss: 0.06632134318351746\n",
      "          vf_explained_var: 0.46045053005218506\n",
      "          vf_loss: 0.11491228640079498\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.73758865248227\n",
      "    ram_util_percent: 95.6421985815603\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10802079323626199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0082973043055654\n",
      "    mean_inference_ms: 2.2401697113343384\n",
      "    mean_raw_obs_processing_ms: 0.23265990249383306\n",
      "  time_since_restore: 18067.99147295952\n",
      "  time_this_iter_s: 210.64391088485718\n",
      "  time_total_s: 18067.99147295952\n",
      "  timers:\n",
      "    learn_throughput: 20.554\n",
      "    learn_time_ms: 194609.232\n",
      "    load_throughput: 3787609.437\n",
      "    load_time_ms: 1.056\n",
      "    sample_throughput: 18.931\n",
      "    sample_time_ms: 211296.766\n",
      "    update_time_ms: 7.859\n",
      "  timestamp: 1650236409\n",
      "  timesteps_since_restore: 336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-03-30\n",
      "  done: false\n",
      "  episode_len_mean: 1521.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 188\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0339757810987241e-26\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.965147622502517e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.5000877284581697e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014127954840660095\n",
      "          total_loss: 0.014169949106872082\n",
      "          vf_explained_var: -8.331832113128712e-09\n",
      "          vf_loss: 4.199082468403503e-05\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.55310344827586\n",
      "    ram_util_percent: 95.76\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10574926380564804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9752589629359311\n",
      "    mean_inference_ms: 2.18617689046871\n",
      "    mean_raw_obs_processing_ms: 0.13175227969843978\n",
      "  time_since_restore: 18265.760459184647\n",
      "  time_this_iter_s: 216.0718069076538\n",
      "  time_total_s: 18265.760459184647\n",
      "  timers:\n",
      "    learn_throughput: 20.521\n",
      "    learn_time_ms: 194925.868\n",
      "    load_throughput: 6634982.204\n",
      "    load_time_ms: 0.603\n",
      "    sample_throughput: 18.97\n",
      "    sample_time_ms: 210859.252\n",
      "    update_time_ms: 17.968\n",
      "  timestamp: 1650236610\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-03-46\n",
      "  done: false\n",
      "  episode_len_mean: 369.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.55\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1378\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0308036804199219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006583849433809519\n",
      "          model: {}\n",
      "          policy_loss: -0.049358490854501724\n",
      "          total_loss: 0.030851084738969803\n",
      "          vf_explained_var: 0.6696889996528625\n",
      "          vf_loss: 0.08020957559347153\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.40481099656357\n",
      "    ram_util_percent: 95.74707903780069\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10815622830318844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0095253728628975\n",
      "    mean_inference_ms: 2.245053158690519\n",
      "    mean_raw_obs_processing_ms: 0.2325938106658346\n",
      "  time_since_restore: 18285.152657985687\n",
      "  time_this_iter_s: 217.16118502616882\n",
      "  time_total_s: 18285.152657985687\n",
      "  timers:\n",
      "    learn_throughput: 20.517\n",
      "    learn_time_ms: 194962.412\n",
      "    load_throughput: 3735242.675\n",
      "    load_time_ms: 1.071\n",
      "    sample_throughput: 18.928\n",
      "    sample_time_ms: 211330.171\n",
      "    update_time_ms: 7.482\n",
      "  timestamp: 1650236626\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-07-05\n",
      "  done: false\n",
      "  episode_len_mean: 1521.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 188\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.169878905493621e-27\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.965147622502517e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.5000877284581697e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014127980917692184\n",
      "          total_loss: 0.014138132333755493\n",
      "          vf_explained_var: 4.661339687572763e-07\n",
      "          vf_loss: 1.0153069524676539e-05\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.97951388888889\n",
      "    ram_util_percent: 95.75312500000001\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10574926380564804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9752589629359311\n",
      "    mean_inference_ms: 2.18617689046871\n",
      "    mean_raw_obs_processing_ms: 0.13175227969843978\n",
      "  time_since_restore: 18480.76921224594\n",
      "  time_this_iter_s: 215.00875306129456\n",
      "  time_total_s: 18480.76921224594\n",
      "  timers:\n",
      "    learn_throughput: 20.551\n",
      "    learn_time_ms: 194639.643\n",
      "    load_throughput: 6850919.188\n",
      "    load_time_ms: 0.584\n",
      "    sample_throughput: 18.909\n",
      "    sample_time_ms: 211533.917\n",
      "    update_time_ms: 13.218\n",
      "  timestamp: 1650236825\n",
      "  timesteps_since_restore: 344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-07-21\n",
      "  done: false\n",
      "  episode_len_mean: 375.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.63\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1389\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0103977918624878\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007118542678654194\n",
      "          model: {}\n",
      "          policy_loss: -0.04701979085803032\n",
      "          total_loss: 0.09081849455833435\n",
      "          vf_explained_var: 0.4843822121620178\n",
      "          vf_loss: 0.13783828914165497\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.39860627177701\n",
      "    ram_util_percent: 95.7651567944251\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10830813650460644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.010871697873609\n",
      "    mean_inference_ms: 2.2502781939288377\n",
      "    mean_raw_obs_processing_ms: 0.2325280016416526\n",
      "  time_since_restore: 18499.997472047806\n",
      "  time_this_iter_s: 214.84481406211853\n",
      "  time_total_s: 18499.997472047806\n",
      "  timers:\n",
      "    learn_throughput: 20.55\n",
      "    learn_time_ms: 194646.801\n",
      "    load_throughput: 4057662.225\n",
      "    load_time_ms: 0.986\n",
      "    sample_throughput: 18.88\n",
      "    sample_time_ms: 211865.796\n",
      "    update_time_ms: 7.242\n",
      "  timestamp: 1650236841\n",
      "  timesteps_since_restore: 344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 1424.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 194\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.5849394527468104e-27\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.83900968767157e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.919009362215477e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0008968326146714389\n",
      "          total_loss: 0.0027864088770002127\n",
      "          vf_explained_var: 0.22035755217075348\n",
      "          vf_loss: 0.001889580162242055\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.53437500000001\n",
      "    ram_util_percent: 95.65833333333333\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10616399876024801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9782242706953466\n",
      "    mean_inference_ms: 2.1984253192082193\n",
      "    mean_raw_obs_processing_ms: 0.13230925440384617\n",
      "  time_since_restore: 18696.839074373245\n",
      "  time_this_iter_s: 216.06986212730408\n",
      "  time_total_s: 18696.839074373245\n",
      "  timers:\n",
      "    learn_throughput: 20.497\n",
      "    learn_time_ms: 195147.051\n",
      "    load_throughput: 6784978.364\n",
      "    load_time_ms: 0.59\n",
      "    sample_throughput: 18.916\n",
      "    sample_time_ms: 211458.939\n",
      "    update_time_ms: 16.163\n",
      "  timestamp: 1650237041\n",
      "  timesteps_since_restore: 348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-10-56\n",
      "  done: false\n",
      "  episode_len_mean: 382.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.74\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9900176525115967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00853465311229229\n",
      "          model: {}\n",
      "          policy_loss: -0.046114709228277206\n",
      "          total_loss: 0.06548566371202469\n",
      "          vf_explained_var: 0.5349306464195251\n",
      "          vf_loss: 0.1116003692150116\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.49163763066201\n",
      "    ram_util_percent: 95.66620209059235\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10847091872847038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0122682612461642\n",
      "    mean_inference_ms: 2.255764425516456\n",
      "    mean_raw_obs_processing_ms: 0.23247719544446574\n",
      "  time_since_restore: 18715.427545785904\n",
      "  time_this_iter_s: 215.43007373809814\n",
      "  time_total_s: 18715.427545785904\n",
      "  timers:\n",
      "    learn_throughput: 20.503\n",
      "    learn_time_ms: 195092.743\n",
      "    load_throughput: 4061788.161\n",
      "    load_time_ms: 0.985\n",
      "    sample_throughput: 18.894\n",
      "    sample_time_ms: 211710.877\n",
      "    update_time_ms: 7.289\n",
      "  timestamp: 1650237056\n",
      "  timesteps_since_restore: 348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 1424.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 194\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2924697263734052e-27\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014127899892628193\n",
      "          total_loss: 0.014137567952275276\n",
      "          vf_explained_var: 1.5131888631003676e-07\n",
      "          vf_loss: 9.659311217546929e-06\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.28397212543554\n",
      "    ram_util_percent: 95.53623693379791\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10616399876024801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9782242706953466\n",
      "    mean_inference_ms: 2.1984253192082193\n",
      "    mean_raw_obs_processing_ms: 0.13230925440384617\n",
      "  time_since_restore: 18912.148482322693\n",
      "  time_this_iter_s: 215.30940794944763\n",
      "  time_total_s: 18912.148482322693\n",
      "  timers:\n",
      "    learn_throughput: 20.45\n",
      "    learn_time_ms: 195596.991\n",
      "    load_throughput: 6561032.42\n",
      "    load_time_ms: 0.61\n",
      "    sample_throughput: 18.857\n",
      "    sample_time_ms: 212121.05\n",
      "    update_time_ms: 16.059\n",
      "  timestamp: 1650237257\n",
      "  timesteps_since_restore: 352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-14-33\n",
      "  done: false\n",
      "  episode_len_mean: 384.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.74\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1412\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0353840589523315\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007284778635948896\n",
      "          model: {}\n",
      "          policy_loss: -0.05019629746675491\n",
      "          total_loss: 0.07764069736003876\n",
      "          vf_explained_var: 0.49371179938316345\n",
      "          vf_loss: 0.12783698737621307\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.77395833333334\n",
      "    ram_util_percent: 95.52777777777777\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10866045198147402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.013675654923959\n",
      "    mean_inference_ms: 2.261906991521217\n",
      "    mean_raw_obs_processing_ms: 0.23242302329971906\n",
      "  time_since_restore: 18932.00119662285\n",
      "  time_this_iter_s: 216.57365083694458\n",
      "  time_total_s: 18932.00119662285\n",
      "  timers:\n",
      "    learn_throughput: 20.452\n",
      "    learn_time_ms: 195576.549\n",
      "    load_throughput: 2500926.599\n",
      "    load_time_ms: 1.599\n",
      "    sample_throughput: 18.842\n",
      "    sample_time_ms: 212295.832\n",
      "    update_time_ms: 7.167\n",
      "  timestamp: 1650237273\n",
      "  timesteps_since_restore: 352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 00:15:28 (running for 05:16:47.73)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=3.74 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-17-53\n",
      "  done: false\n",
      "  episode_len_mean: 1424.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 194\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.462348631867026e-28\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128034934401512\n",
      "          total_loss: 0.014129728078842163\n",
      "          vf_explained_var: 3.2744100053605507e-07\n",
      "          vf_loss: 1.6940111891017295e-06\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.64221453287196\n",
      "    ram_util_percent: 95.51003460207613\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10616399876024801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9782242706953466\n",
      "    mean_inference_ms: 2.1984253192082193\n",
      "    mean_raw_obs_processing_ms: 0.13230925440384617\n",
      "  time_since_restore: 19128.79155921936\n",
      "  time_this_iter_s: 216.64307689666748\n",
      "  time_total_s: 19128.79155921936\n",
      "  timers:\n",
      "    learn_throughput: 20.391\n",
      "    learn_time_ms: 196161.303\n",
      "    load_throughput: 6524545.384\n",
      "    load_time_ms: 0.613\n",
      "    sample_throughput: 18.803\n",
      "    sample_time_ms: 212729.379\n",
      "    update_time_ms: 16.155\n",
      "  timestamp: 1650237473\n",
      "  timesteps_since_restore: 356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-18-08\n",
      "  done: false\n",
      "  episode_len_mean: 379.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.63\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1424\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.038583755493164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006547943688929081\n",
      "          model: {}\n",
      "          policy_loss: -0.04711374267935753\n",
      "          total_loss: 0.02816743589937687\n",
      "          vf_explained_var: 0.6210095286369324\n",
      "          vf_loss: 0.07528118789196014\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.27118055555555\n",
      "    ram_util_percent: 95.55312500000001\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10885479780067968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.015114247716054\n",
      "    mean_inference_ms: 2.2682156638667776\n",
      "    mean_raw_obs_processing_ms: 0.2323941035729261\n",
      "  time_since_restore: 19147.594072580338\n",
      "  time_this_iter_s: 215.592875957489\n",
      "  time_total_s: 19147.594072580338\n",
      "  timers:\n",
      "    learn_throughput: 20.393\n",
      "    learn_time_ms: 196149.817\n",
      "    load_throughput: 2487245.341\n",
      "    load_time_ms: 1.608\n",
      "    sample_throughput: 18.793\n",
      "    sample_time_ms: 212841.879\n",
      "    update_time_ms: 6.87\n",
      "  timestamp: 1650237488\n",
      "  timesteps_since_restore: 356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 1522.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 195\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.231174315933513e-28\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1333548929595471e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -9.026270314834241e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.0020589237101376057\n",
      "          total_loss: -0.001590161700733006\n",
      "          vf_explained_var: 0.03226199373602867\n",
      "          vf_loss: 0.0004687634063884616\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.84773519163764\n",
      "    ram_util_percent: 95.5637630662021\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10623626837780613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.978721306938545\n",
      "    mean_inference_ms: 2.200518538134156\n",
      "    mean_raw_obs_processing_ms: 0.1324050304654793\n",
      "  time_since_restore: 19344.25060915947\n",
      "  time_this_iter_s: 215.45904994010925\n",
      "  time_total_s: 19344.25060915947\n",
      "  timers:\n",
      "    learn_throughput: 20.367\n",
      "    learn_time_ms: 196395.2\n",
      "    load_throughput: 6681487.853\n",
      "    load_time_ms: 0.599\n",
      "    sample_throughput: 18.744\n",
      "    sample_time_ms: 213396.369\n",
      "    update_time_ms: 16.937\n",
      "  timestamp: 1650237689\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-21-45\n",
      "  done: false\n",
      "  episode_len_mean: 383.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.69\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1434\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9915379285812378\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009071098640561104\n",
      "          model: {}\n",
      "          policy_loss: -0.04798884317278862\n",
      "          total_loss: 0.04020354896783829\n",
      "          vf_explained_var: 0.6919304132461548\n",
      "          vf_loss: 0.0881923958659172\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.64409722222223\n",
      "    ram_util_percent: 95.56319444444446\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1090212026775616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0162835173360778\n",
      "    mean_inference_ms: 2.273604625471192\n",
      "    mean_raw_obs_processing_ms: 0.2323714664822\n",
      "  time_since_restore: 19363.758382558823\n",
      "  time_this_iter_s: 216.1643099784851\n",
      "  time_total_s: 19363.758382558823\n",
      "  timers:\n",
      "    learn_throughput: 20.371\n",
      "    learn_time_ms: 196356.029\n",
      "    load_throughput: 2481359.502\n",
      "    load_time_ms: 1.612\n",
      "    sample_throughput: 18.733\n",
      "    sample_time_ms: 213525.966\n",
      "    update_time_ms: 6.437\n",
      "  timestamp: 1650237705\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-25-05\n",
      "  done: false\n",
      "  episode_len_mean: 1522.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 195\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6155871579667565e-28\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128084294497967\n",
      "          total_loss: -0.014115826226770878\n",
      "          vf_explained_var: -3.27505098596248e-08\n",
      "          vf_loss: 1.2259461982466746e-05\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.88083623693379\n",
      "    ram_util_percent: 95.74634146341462\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10623626837780613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.978721306938545\n",
      "    mean_inference_ms: 2.200518538134156\n",
      "    mean_raw_obs_processing_ms: 0.1324050304654793\n",
      "  time_since_restore: 19560.066862344742\n",
      "  time_this_iter_s: 215.81625318527222\n",
      "  time_total_s: 19560.066862344742\n",
      "  timers:\n",
      "    learn_throughput: 20.323\n",
      "    learn_time_ms: 196820.673\n",
      "    load_throughput: 6633408.192\n",
      "    load_time_ms: 0.603\n",
      "    sample_throughput: 18.716\n",
      "    sample_time_ms: 213720.06\n",
      "    update_time_ms: 18.927\n",
      "  timestamp: 1650237905\n",
      "  timesteps_since_restore: 364000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-25-21\n",
      "  done: false\n",
      "  episode_len_mean: 380.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.65\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1447\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.018671989440918\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0073395986109972\n",
      "          model: {}\n",
      "          policy_loss: -0.03455045074224472\n",
      "          total_loss: 0.029147544875741005\n",
      "          vf_explained_var: 0.7514909505844116\n",
      "          vf_loss: 0.06369799375534058\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.41631944444445\n",
      "    ram_util_percent: 95.70833333333333\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10924384148376642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.017761719661296\n",
      "    mean_inference_ms: 2.280751451774736\n",
      "    mean_raw_obs_processing_ms: 0.23236080260191236\n",
      "  time_since_restore: 19579.817830324173\n",
      "  time_this_iter_s: 216.05944776535034\n",
      "  time_total_s: 19579.817830324173\n",
      "  timers:\n",
      "    learn_throughput: 20.338\n",
      "    learn_time_ms: 196676.603\n",
      "    load_throughput: 3114562.905\n",
      "    load_time_ms: 1.284\n",
      "    sample_throughput: 18.705\n",
      "    sample_time_ms: 213845.103\n",
      "    update_time_ms: 5.754\n",
      "  timestamp: 1650237921\n",
      "  timesteps_since_restore: 364000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-28-41\n",
      "  done: false\n",
      "  episode_len_mean: 1521.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 199\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.077935789833782e-29\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0588886045493144e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.314881346144156e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.00021426113380584866\n",
      "          total_loss: 0.0005491233896464109\n",
      "          vf_explained_var: 0.0429091602563858\n",
      "          vf_loss: 0.0007633818313479424\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.82906574394462\n",
      "    ram_util_percent: 95.62595155709343\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1065200990789182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9806863966895761\n",
      "    mean_inference_ms: 2.2088092089260676\n",
      "    mean_raw_obs_processing_ms: 0.1327832265165773\n",
      "  time_since_restore: 19776.400712251663\n",
      "  time_this_iter_s: 216.3338499069214\n",
      "  time_total_s: 19776.400712251663\n",
      "  timers:\n",
      "    learn_throughput: 20.274\n",
      "    learn_time_ms: 197300.107\n",
      "    load_throughput: 6475439.423\n",
      "    load_time_ms: 0.618\n",
      "    sample_throughput: 18.679\n",
      "    sample_time_ms: 214142.37\n",
      "    update_time_ms: 18.772\n",
      "  timestamp: 1650238121\n",
      "  timesteps_since_restore: 368000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-28-56\n",
      "  done: false\n",
      "  episode_len_mean: 379.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.64\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1459\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9979662895202637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007962417788803577\n",
      "          model: {}\n",
      "          policy_loss: -0.04311241954565048\n",
      "          total_loss: 0.0426039919257164\n",
      "          vf_explained_var: 0.6414578557014465\n",
      "          vf_loss: 0.08571641147136688\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.2954861111111\n",
      "    ram_util_percent: 95.63159722222223\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10946147605853404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.019167261617719\n",
      "    mean_inference_ms: 2.2875952772666226\n",
      "    mean_raw_obs_processing_ms: 0.2323694081582293\n",
      "  time_since_restore: 19794.985750436783\n",
      "  time_this_iter_s: 215.16792011260986\n",
      "  time_total_s: 19794.985750436783\n",
      "  timers:\n",
      "    learn_throughput: 20.286\n",
      "    learn_time_ms: 197175.724\n",
      "    load_throughput: 3145865.632\n",
      "    load_time_ms: 1.272\n",
      "    sample_throughput: 18.672\n",
      "    sample_time_ms: 214224.065\n",
      "    update_time_ms: 5.686\n",
      "  timestamp: 1650238136\n",
      "  timesteps_since_restore: 368000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 00:32:08 (running for 05:33:28.04)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=3.64 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-32-19\n",
      "  done: false\n",
      "  episode_len_mean: 1521.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 199\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.038967894916891e-29\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.01412812527269125\n",
      "          total_loss: 0.014264886267483234\n",
      "          vf_explained_var: 8.97274199385123e-10\n",
      "          vf_loss: 0.00013676438538823277\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.32560553633218\n",
      "    ram_util_percent: 95.85536332179932\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1065200990789182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9806863966895761\n",
      "    mean_inference_ms: 2.2088092089260676\n",
      "    mean_raw_obs_processing_ms: 0.1327832265165773\n",
      "  time_since_restore: 19993.97701025009\n",
      "  time_this_iter_s: 217.57629799842834\n",
      "  time_total_s: 19993.97701025009\n",
      "  timers:\n",
      "    learn_throughput: 20.194\n",
      "    learn_time_ms: 198076.159\n",
      "    load_throughput: 6575690.209\n",
      "    load_time_ms: 0.608\n",
      "    sample_throughput: 18.63\n",
      "    sample_time_ms: 214707.286\n",
      "    update_time_ms: 19.096\n",
      "  timestamp: 1650238339\n",
      "  timesteps_since_restore: 372000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-32-34\n",
      "  done: false\n",
      "  episode_len_mean: 377.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.62\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1471\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0446116924285889\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006645025219768286\n",
      "          model: {}\n",
      "          policy_loss: -0.049935635179281235\n",
      "          total_loss: 0.03944633901119232\n",
      "          vf_explained_var: 0.47006189823150635\n",
      "          vf_loss: 0.08938198536634445\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.01875000000001\n",
      "    ram_util_percent: 95.84479166666668\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10968058905454303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0205697190414307\n",
      "    mean_inference_ms: 2.2944219343267656\n",
      "    mean_raw_obs_processing_ms: 0.23237376544110958\n",
      "  time_since_restore: 20013.260795354843\n",
      "  time_this_iter_s: 218.2750449180603\n",
      "  time_total_s: 20013.260795354843\n",
      "  timers:\n",
      "    learn_throughput: 20.21\n",
      "    learn_time_ms: 197920.876\n",
      "    load_throughput: 3135693.78\n",
      "    load_time_ms: 1.276\n",
      "    sample_throughput: 18.615\n",
      "    sample_time_ms: 214885.563\n",
      "    update_time_ms: 5.786\n",
      "  timestamp: 1650238354\n",
      "  timesteps_since_restore: 372000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-35-55\n",
      "  done: false\n",
      "  episode_len_mean: 1521.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 199\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0194839474584456e-29\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128031209111214\n",
      "          total_loss: 0.014142717234790325\n",
      "          vf_explained_var: 2.0316852200608082e-08\n",
      "          vf_loss: 1.4685021596960723e-05\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.92604166666666\n",
      "    ram_util_percent: 95.75381944444445\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1065200990789182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9806863966895761\n",
      "    mean_inference_ms: 2.2088092089260676\n",
      "    mean_raw_obs_processing_ms: 0.1327832265165773\n",
      "  time_since_restore: 20210.39378118515\n",
      "  time_this_iter_s: 216.4167709350586\n",
      "  time_total_s: 20210.39378118515\n",
      "  timers:\n",
      "    learn_throughput: 20.138\n",
      "    learn_time_ms: 198625.047\n",
      "    load_throughput: 6705790.0\n",
      "    load_time_ms: 0.596\n",
      "    sample_throughput: 18.553\n",
      "    sample_time_ms: 215600.436\n",
      "    update_time_ms: 19.266\n",
      "  timestamp: 1650238555\n",
      "  timesteps_since_restore: 376000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-36-10\n",
      "  done: false\n",
      "  episode_len_mean: 376.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.64\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1481\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9830000400543213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00768272252753377\n",
      "          model: {}\n",
      "          policy_loss: -0.04989677667617798\n",
      "          total_loss: 0.040595732629299164\n",
      "          vf_explained_var: 0.6143497228622437\n",
      "          vf_loss: 0.09049250930547714\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.86306620209058\n",
      "    ram_util_percent: 95.77944250871079\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10985626254338726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0217173569330091\n",
      "    mean_inference_ms: 2.2996307287981\n",
      "    mean_raw_obs_processing_ms: 0.23235411341022136\n",
      "  time_since_restore: 20228.71483540535\n",
      "  time_this_iter_s: 215.4540400505066\n",
      "  time_total_s: 20228.71483540535\n",
      "  timers:\n",
      "    learn_throughput: 20.167\n",
      "    learn_time_ms: 198339.438\n",
      "    load_throughput: 3302081.562\n",
      "    load_time_ms: 1.211\n",
      "    sample_throughput: 18.545\n",
      "    sample_time_ms: 215692.765\n",
      "    update_time_ms: 5.566\n",
      "  timestamp: 1650238570\n",
      "  timesteps_since_restore: 376000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-39-30\n",
      "  done: false\n",
      "  episode_len_mean: 1521.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 199\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0097419737292228e-29\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.118379046463216e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.299683722095605e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.008528472855687141\n",
      "          total_loss: 0.008649464696645737\n",
      "          vf_explained_var: 0.016731904819607735\n",
      "          vf_loss: 0.00012098857405362651\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.89372822299651\n",
      "    ram_util_percent: 95.7630662020906\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1065200990789182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9806863966895761\n",
      "    mean_inference_ms: 2.2088092089260676\n",
      "    mean_raw_obs_processing_ms: 0.1327832265165773\n",
      "  time_since_restore: 20425.134036064148\n",
      "  time_this_iter_s: 214.7402548789978\n",
      "  time_total_s: 20425.134036064148\n",
      "  timers:\n",
      "    learn_throughput: 20.148\n",
      "    learn_time_ms: 198531.415\n",
      "    load_throughput: 6667679.835\n",
      "    load_time_ms: 0.6\n",
      "    sample_throughput: 18.509\n",
      "    sample_time_ms: 216106.28\n",
      "    update_time_ms: 19.163\n",
      "  timestamp: 1650238770\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-39-46\n",
      "  done: false\n",
      "  episode_len_mean: 381.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.74\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1492\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9531301259994507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008100079372525215\n",
      "          model: {}\n",
      "          policy_loss: -0.048399776220321655\n",
      "          total_loss: 0.034690164029598236\n",
      "          vf_explained_var: 0.6862667202949524\n",
      "          vf_loss: 0.0830899327993393\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.23958333333333\n",
      "    ram_util_percent: 95.7767361111111\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11004529303120343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0229245998017233\n",
      "    mean_inference_ms: 2.3052128783050274\n",
      "    mean_raw_obs_processing_ms: 0.2323226251775207\n",
      "  time_since_restore: 20444.565544366837\n",
      "  time_this_iter_s: 215.85070896148682\n",
      "  time_total_s: 20444.565544366837\n",
      "  timers:\n",
      "    learn_throughput: 20.165\n",
      "    learn_time_ms: 198367.066\n",
      "    load_throughput: 3313559.804\n",
      "    load_time_ms: 1.207\n",
      "    sample_throughput: 18.523\n",
      "    sample_time_ms: 215951.502\n",
      "    update_time_ms: 5.59\n",
      "  timestamp: 1650238786\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-43-08\n",
      "  done: false\n",
      "  episode_len_mean: 1521.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 199\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.048709868646114e-30\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412828266620636\n",
      "          total_loss: 0.014145081862807274\n",
      "          vf_explained_var: -5.056781304801916e-08\n",
      "          vf_loss: 1.6796473573776893e-05\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.33079584775086\n",
      "    ram_util_percent: 95.7674740484429\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1065200990789182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9806863966895761\n",
      "    mean_inference_ms: 2.2088092089260676\n",
      "    mean_raw_obs_processing_ms: 0.1327832265165773\n",
      "  time_since_restore: 20642.55241203308\n",
      "  time_this_iter_s: 217.4183759689331\n",
      "  time_total_s: 20642.55241203308\n",
      "  timers:\n",
      "    learn_throughput: 20.124\n",
      "    learn_time_ms: 198768.413\n",
      "    load_throughput: 6620320.417\n",
      "    load_time_ms: 0.604\n",
      "    sample_throughput: 18.517\n",
      "    sample_time_ms: 216012.655\n",
      "    update_time_ms: 20.747\n",
      "  timestamp: 1650238988\n",
      "  timesteps_since_restore: 384000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-43-23\n",
      "  done: false\n",
      "  episode_len_mean: 379.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.73\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1503\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0072616338729858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007472481112927198\n",
      "          model: {}\n",
      "          policy_loss: -0.04177137091755867\n",
      "          total_loss: 0.021244406700134277\n",
      "          vf_explained_var: 0.7318110466003418\n",
      "          vf_loss: 0.06301577389240265\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.49896193771626\n",
      "    ram_util_percent: 95.77716262975778\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11022874264106268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.024115064499719\n",
      "    mean_inference_ms: 2.310736368676262\n",
      "    mean_raw_obs_processing_ms: 0.23228151370492928\n",
      "  time_since_restore: 20662.193222284317\n",
      "  time_this_iter_s: 217.62767791748047\n",
      "  time_total_s: 20662.193222284317\n",
      "  timers:\n",
      "    learn_throughput: 20.138\n",
      "    learn_time_ms: 198628.604\n",
      "    load_throughput: 3293072.409\n",
      "    load_time_ms: 1.215\n",
      "    sample_throughput: 18.519\n",
      "    sample_time_ms: 215995.034\n",
      "    update_time_ms: 6.104\n",
      "  timestamp: 1650239003\n",
      "  timesteps_since_restore: 384000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 1714.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 201\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.524354934323057e-30\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0790353638498676e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.95571599172114e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.001968681113794446\n",
      "          total_loss: 0.0024633919820189476\n",
      "          vf_explained_var: 0.020547330379486084\n",
      "          vf_loss: 0.0004947206471115351\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.5752613240418\n",
      "    ram_util_percent: 95.69581881533101\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10668083469388473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9817617388108569\n",
      "    mean_inference_ms: 2.2133991953840426\n",
      "    mean_raw_obs_processing_ms: 0.13297204161390677\n",
      "  time_since_restore: 20859.139642953873\n",
      "  time_this_iter_s: 216.58723092079163\n",
      "  time_total_s: 20859.139642953873\n",
      "  timers:\n",
      "    learn_throughput: 20.118\n",
      "    learn_time_ms: 198826.082\n",
      "    load_throughput: 6524799.129\n",
      "    load_time_ms: 0.613\n",
      "    sample_throughput: 18.496\n",
      "    sample_time_ms: 216258.29\n",
      "    update_time_ms: 17.922\n",
      "  timestamp: 1650239204\n",
      "  timesteps_since_restore: 388000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-47-00\n",
      "  done: false\n",
      "  episode_len_mean: 381.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.81\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1514\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0018688440322876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008136342279613018\n",
      "          model: {}\n",
      "          policy_loss: -0.04580383747816086\n",
      "          total_loss: 0.04809773713350296\n",
      "          vf_explained_var: 0.5651395916938782\n",
      "          vf_loss: 0.09390156716108322\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.16319444444444\n",
      "    ram_util_percent: 95.69930555555555\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11041032980625808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0253189998949148\n",
      "    mean_inference_ms: 2.316226281915826\n",
      "    mean_raw_obs_processing_ms: 0.2322170595935603\n",
      "  time_since_restore: 20878.922607183456\n",
      "  time_this_iter_s: 216.7293848991394\n",
      "  time_total_s: 20878.922607183456\n",
      "  timers:\n",
      "    learn_throughput: 20.127\n",
      "    learn_time_ms: 198742.443\n",
      "    load_throughput: 3254236.447\n",
      "    load_time_ms: 1.229\n",
      "    sample_throughput: 18.495\n",
      "    sample_time_ms: 216272.189\n",
      "    update_time_ms: 6.304\n",
      "  timestamp: 1650239220\n",
      "  timesteps_since_restore: 388000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 00:48:48 (running for 05:50:08.65)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=3.81 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-50-20\n",
      "  done: false\n",
      "  episode_len_mean: 1714.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 201\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2621774671615285e-30\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014127939939498901\n",
      "          total_loss: -0.014111279509961605\n",
      "          vf_explained_var: 3.595506115061653e-08\n",
      "          vf_loss: 1.66664340213174e-05\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.94375000000001\n",
      "    ram_util_percent: 95.68576388888889\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10668083469388473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9817617388108569\n",
      "    mean_inference_ms: 2.2133991953840426\n",
      "    mean_raw_obs_processing_ms: 0.13297204161390677\n",
      "  time_since_restore: 21075.131859779358\n",
      "  time_this_iter_s: 215.99221682548523\n",
      "  time_total_s: 21075.131859779358\n",
      "  timers:\n",
      "    learn_throughput: 20.115\n",
      "    learn_time_ms: 198852.418\n",
      "    load_throughput: 6596892.104\n",
      "    load_time_ms: 0.606\n",
      "    sample_throughput: 18.488\n",
      "    sample_time_ms: 216354.787\n",
      "    update_time_ms: 18.078\n",
      "  timestamp: 1650239420\n",
      "  timesteps_since_restore: 392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-50-36\n",
      "  done: false\n",
      "  episode_len_mean: 390.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.97\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1523\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0008201599121094\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008065169677138329\n",
      "          model: {}\n",
      "          policy_loss: -0.04803056642413139\n",
      "          total_loss: 0.04298964515328407\n",
      "          vf_explained_var: 0.6921918392181396\n",
      "          vf_loss: 0.09102020412683487\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.05121951219512\n",
      "    ram_util_percent: 95.69756097560978\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11056365198295728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0262834501769915\n",
      "    mean_inference_ms: 2.3207527175148805\n",
      "    mean_raw_obs_processing_ms: 0.2321283515924446\n",
      "  time_since_restore: 21094.713533878326\n",
      "  time_this_iter_s: 215.79092669487\n",
      "  time_total_s: 21094.713533878326\n",
      "  timers:\n",
      "    learn_throughput: 20.135\n",
      "    learn_time_ms: 198661.067\n",
      "    load_throughput: 6454264.83\n",
      "    load_time_ms: 0.62\n",
      "    sample_throughput: 18.485\n",
      "    sample_time_ms: 216390.403\n",
      "    update_time_ms: 6.292\n",
      "  timestamp: 1650239436\n",
      "  timesteps_since_restore: 392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-53-56\n",
      "  done: false\n",
      "  episode_len_mean: 1714.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 201\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.3108873358076425e-31\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128303155303001\n",
      "          total_loss: -0.014126665890216827\n",
      "          vf_explained_var: -1.0152017182463169e-07\n",
      "          vf_loss: 1.631776513022487e-06\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.27770034843206\n",
      "    ram_util_percent: 95.78362369337978\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10668083469388473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9817617388108569\n",
      "    mean_inference_ms: 2.2133991953840426\n",
      "    mean_raw_obs_processing_ms: 0.13297204161390677\n",
      "  time_since_restore: 21290.47025179863\n",
      "  time_this_iter_s: 215.33839201927185\n",
      "  time_total_s: 21290.47025179863\n",
      "  timers:\n",
      "    learn_throughput: 20.124\n",
      "    learn_time_ms: 198762.733\n",
      "    load_throughput: 6522769.721\n",
      "    load_time_ms: 0.613\n",
      "    sample_throughput: 18.489\n",
      "    sample_time_ms: 216342.374\n",
      "    update_time_ms: 18.601\n",
      "  timestamp: 1650239636\n",
      "  timesteps_since_restore: 396000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-54-11\n",
      "  done: false\n",
      "  episode_len_mean: 384.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.88\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1536\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0137486457824707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006819169037044048\n",
      "          model: {}\n",
      "          policy_loss: -0.04593313857913017\n",
      "          total_loss: 0.030606092885136604\n",
      "          vf_explained_var: 0.6876955628395081\n",
      "          vf_loss: 0.07653923332691193\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.26132404181185\n",
      "    ram_util_percent: 95.77770034843205\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11076890145290985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0276498947308332\n",
      "    mean_inference_ms: 2.3271080179987798\n",
      "    mean_raw_obs_processing_ms: 0.23199442505732829\n",
      "  time_since_restore: 21309.604425668716\n",
      "  time_this_iter_s: 214.89089179039001\n",
      "  time_total_s: 21309.604425668716\n",
      "  timers:\n",
      "    learn_throughput: 20.142\n",
      "    learn_time_ms: 198594.466\n",
      "    load_throughput: 5975217.608\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 18.492\n",
      "    sample_time_ms: 216306.422\n",
      "    update_time_ms: 8.074\n",
      "  timestamp: 1650239651\n",
      "  timesteps_since_restore: 396000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-57-33\n",
      "  done: false\n",
      "  episode_len_mean: 1816.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 206\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1554436679038213e-31\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.118236906529911e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.1395068812898294e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0017848603893071413\n",
      "          total_loss: 0.001767510548233986\n",
      "          vf_explained_var: 0.26373669505119324\n",
      "          vf_loss: 0.0035523800179362297\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.24982698961938\n",
      "    ram_util_percent: 95.76401384083044\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1071081139307451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9845965868738159\n",
      "    mean_inference_ms: 2.2256835336948355\n",
      "    mean_raw_obs_processing_ms: 0.13347353272232032\n",
      "  time_since_restore: 21507.83786058426\n",
      "  time_this_iter_s: 217.36760878562927\n",
      "  time_total_s: 21507.83786058426\n",
      "  timers:\n",
      "    learn_throughput: 20.106\n",
      "    learn_time_ms: 198950.485\n",
      "    load_throughput: 3312513.031\n",
      "    load_time_ms: 1.208\n",
      "    sample_throughput: 18.497\n",
      "    sample_time_ms: 216251.369\n",
      "    update_time_ms: 18.127\n",
      "  timestamp: 1650239853\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_00-57-47\n",
      "  done: false\n",
      "  episode_len_mean: 389.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 4.0\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1547\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.992944061756134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00820606853812933\n",
      "          model: {}\n",
      "          policy_loss: -0.04617639631032944\n",
      "          total_loss: 0.016805805265903473\n",
      "          vf_explained_var: 0.7414910793304443\n",
      "          vf_loss: 0.06298220157623291\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.99094076655051\n",
      "    ram_util_percent: 95.74982578397213\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1109381004830507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0288178864337243\n",
      "    mean_inference_ms: 2.3321776651962787\n",
      "    mean_raw_obs_processing_ms: 0.23185906046566282\n",
      "  time_since_restore: 21525.769577741623\n",
      "  time_this_iter_s: 216.1651520729065\n",
      "  time_total_s: 21525.769577741623\n",
      "  timers:\n",
      "    learn_throughput: 20.133\n",
      "    learn_time_ms: 198676.011\n",
      "    load_throughput: 5964384.088\n",
      "    load_time_ms: 0.671\n",
      "    sample_throughput: 18.505\n",
      "    sample_time_ms: 216158.0\n",
      "    update_time_ms: 8.508\n",
      "  timestamp: 1650239867\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-01-10\n",
      "  done: false\n",
      "  episode_len_mean: 1816.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 206\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5777218339519106e-31\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6875004068904024e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.668840318176706e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.01412815973162651\n",
      "          total_loss: 0.014276498928666115\n",
      "          vf_explained_var: 1.8522303690815534e-08\n",
      "          vf_loss: 0.00014834805915597826\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.96159169550174\n",
      "    ram_util_percent: 95.8076124567474\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1071081139307451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9845965868738159\n",
      "    mean_inference_ms: 2.2256835336948355\n",
      "    mean_raw_obs_processing_ms: 0.13347353272232032\n",
      "  time_since_restore: 21725.104058742523\n",
      "  time_this_iter_s: 217.26619815826416\n",
      "  time_total_s: 21725.104058742523\n",
      "  timers:\n",
      "    learn_throughput: 20.081\n",
      "    learn_time_ms: 199197.076\n",
      "    load_throughput: 3342474.399\n",
      "    load_time_ms: 1.197\n",
      "    sample_throughput: 18.489\n",
      "    sample_time_ms: 216340.658\n",
      "    update_time_ms: 15.968\n",
      "  timestamp: 1650240070\n",
      "  timesteps_since_restore: 404000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-01-24\n",
      "  done: false\n",
      "  episode_len_mean: 391.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 4.03\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1558\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9997580647468567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007609779015183449\n",
      "          model: {}\n",
      "          policy_loss: -0.04706140607595444\n",
      "          total_loss: 0.017041100189089775\n",
      "          vf_explained_var: 0.7070603370666504\n",
      "          vf_loss: 0.06410250812768936\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.89583333333333\n",
      "    ram_util_percent: 95.80833333333334\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11110038861891088\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0299385360654807\n",
      "    mean_inference_ms: 2.3367529061465846\n",
      "    mean_raw_obs_processing_ms: 0.2317219388317274\n",
      "  time_since_restore: 21742.741285562515\n",
      "  time_this_iter_s: 216.97170782089233\n",
      "  time_total_s: 21742.741285562515\n",
      "  timers:\n",
      "    learn_throughput: 20.108\n",
      "    learn_time_ms: 198924.605\n",
      "    load_throughput: 5791637.669\n",
      "    load_time_ms: 0.691\n",
      "    sample_throughput: 18.511\n",
      "    sample_time_ms: 216083.607\n",
      "    update_time_ms: 9.322\n",
      "  timestamp: 1650240084\n",
      "  timesteps_since_restore: 404000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-04-46\n",
      "  done: false\n",
      "  episode_len_mean: 1816.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 206\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.888609169759553e-32\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6875004068904024e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.668840318176706e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128184877336025\n",
      "          total_loss: 0.014153005555272102\n",
      "          vf_explained_var: 1.839412178128441e-08\n",
      "          vf_loss: 2.4814555217744783e-05\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.94355400696864\n",
      "    ram_util_percent: 95.7595818815331\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1071081139307451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9845965868738159\n",
      "    mean_inference_ms: 2.2256835336948355\n",
      "    mean_raw_obs_processing_ms: 0.13347353272232032\n",
      "  time_since_restore: 21940.5162627697\n",
      "  time_this_iter_s: 215.4122040271759\n",
      "  time_total_s: 21940.5162627697\n",
      "  timers:\n",
      "    learn_throughput: 20.073\n",
      "    learn_time_ms: 199275.489\n",
      "    load_throughput: 3370678.668\n",
      "    load_time_ms: 1.187\n",
      "    sample_throughput: 18.483\n",
      "    sample_time_ms: 216411.949\n",
      "    update_time_ms: 15.291\n",
      "  timestamp: 1650240286\n",
      "  timesteps_since_restore: 408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-05-00\n",
      "  done: false\n",
      "  episode_len_mean: 390.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 4.02\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1570\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0242962837219238\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009297266602516174\n",
      "          model: {}\n",
      "          policy_loss: -0.03866557404398918\n",
      "          total_loss: 0.037589333951473236\n",
      "          vf_explained_var: 0.5789746642112732\n",
      "          vf_loss: 0.07625491172075272\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.83472222222223\n",
      "    ram_util_percent: 95.76041666666667\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11126760072137426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0311051878319046\n",
      "    mean_inference_ms: 2.341391968926027\n",
      "    mean_raw_obs_processing_ms: 0.23156568246932388\n",
      "  time_since_restore: 21958.70557451248\n",
      "  time_this_iter_s: 215.96428894996643\n",
      "  time_total_s: 21958.70557451248\n",
      "  timers:\n",
      "    learn_throughput: 20.089\n",
      "    learn_time_ms: 199111.555\n",
      "    load_throughput: 5831294.011\n",
      "    load_time_ms: 0.686\n",
      "    sample_throughput: 18.499\n",
      "    sample_time_ms: 216227.127\n",
      "    update_time_ms: 9.266\n",
      "  timestamp: 1650240300\n",
      "  timesteps_since_restore: 408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 01:05:28 (running for 06:06:48.67)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=4.02 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-08-26\n",
      "  done: false\n",
      "  episode_len_mean: 1912.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 210\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.9443045848797766e-32\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0629266993897284e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.423433029766447e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0019982196390628815\n",
      "          total_loss: 0.0027529029175639153\n",
      "          vf_explained_var: 0.08959048986434937\n",
      "          vf_loss: 0.0007546819397248328\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.92662116040955\n",
      "    ram_util_percent: 95.7740614334471\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10746493540264757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9868996029279536\n",
      "    mean_inference_ms: 2.2356903137769106\n",
      "    mean_raw_obs_processing_ms: 0.13386576805960013\n",
      "  time_since_restore: 22160.45366168022\n",
      "  time_this_iter_s: 219.93739891052246\n",
      "  time_total_s: 22160.45366168022\n",
      "  timers:\n",
      "    learn_throughput: 20.047\n",
      "    learn_time_ms: 199535.243\n",
      "    load_throughput: 3350016.174\n",
      "    load_time_ms: 1.194\n",
      "    sample_throughput: 18.479\n",
      "    sample_time_ms: 216463.809\n",
      "    update_time_ms: 15.247\n",
      "  timestamp: 1650240506\n",
      "  timesteps_since_restore: 412000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-08-40\n",
      "  done: false\n",
      "  episode_len_mean: 388.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.98\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1581\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9974913001060486\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008486106991767883\n",
      "          model: {}\n",
      "          policy_loss: -0.041063424199819565\n",
      "          total_loss: 0.024247804656624794\n",
      "          vf_explained_var: 0.6588370203971863\n",
      "          vf_loss: 0.06531122326850891\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.91604095563139\n",
      "    ram_util_percent: 95.75529010238908\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1114105842095864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0321318327640931\n",
      "    mean_inference_ms: 2.345571729709712\n",
      "    mean_raw_obs_processing_ms: 0.2314209528813002\n",
      "  time_since_restore: 22178.478117465973\n",
      "  time_this_iter_s: 219.7725429534912\n",
      "  time_total_s: 22178.478117465973\n",
      "  timers:\n",
      "    learn_throughput: 20.066\n",
      "    learn_time_ms: 199346.79\n",
      "    load_throughput: 5892117.721\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 18.49\n",
      "    sample_time_ms: 216328.683\n",
      "    update_time_ms: 9.302\n",
      "  timestamp: 1650240520\n",
      "  timesteps_since_restore: 412000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-12-05\n",
      "  done: false\n",
      "  episode_len_mean: 1912.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 210\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9721522924398883e-32\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0223031486489103e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.735102420806745e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014127880334854126\n",
      "          total_loss: 0.01414780505001545\n",
      "          vf_explained_var: -2.1662764027041703e-08\n",
      "          vf_loss: 1.9919963961001486e-05\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.04109589041096\n",
      "    ram_util_percent: 95.88527397260275\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10746493540264757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9868996029279536\n",
      "    mean_inference_ms: 2.2356903137769106\n",
      "    mean_raw_obs_processing_ms: 0.13386576805960013\n",
      "  time_since_restore: 22378.918080806732\n",
      "  time_this_iter_s: 218.46441912651062\n",
      "  time_total_s: 22378.918080806732\n",
      "  timers:\n",
      "    learn_throughput: 20.015\n",
      "    learn_time_ms: 199848.097\n",
      "    load_throughput: 3319131.897\n",
      "    load_time_ms: 1.205\n",
      "    sample_throughput: 18.466\n",
      "    sample_time_ms: 216614.57\n",
      "    update_time_ms: 15.312\n",
      "  timestamp: 1650240725\n",
      "  timesteps_since_restore: 416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-12-19\n",
      "  done: false\n",
      "  episode_len_mean: 385.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.91\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1592\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9861879348754883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007735191844403744\n",
      "          model: {}\n",
      "          policy_loss: -0.03960048779845238\n",
      "          total_loss: 0.017334073781967163\n",
      "          vf_explained_var: 0.7741255164146423\n",
      "          vf_loss: 0.05693456158041954\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.7\n",
      "    ram_util_percent: 95.89760273972603\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1115422325485782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.033086106066956\n",
      "    mean_inference_ms: 2.349460923366992\n",
      "    mean_raw_obs_processing_ms: 0.23126493430453057\n",
      "  time_since_restore: 22397.700863599777\n",
      "  time_this_iter_s: 219.22274613380432\n",
      "  time_total_s: 22397.700863599777\n",
      "  timers:\n",
      "    learn_throughput: 20.022\n",
      "    learn_time_ms: 199782.497\n",
      "    load_throughput: 5888808.705\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 18.475\n",
      "    sample_time_ms: 216505.26\n",
      "    update_time_ms: 9.603\n",
      "  timestamp: 1650240739\n",
      "  timesteps_since_restore: 416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-15-45\n",
      "  done: false\n",
      "  episode_len_mean: 1914.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 212\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.860761462199441e-33\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1757381083175932e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -5.975098659662258e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0012128729140385985\n",
      "          total_loss: -0.00019805674673989415\n",
      "          vf_explained_var: 0.12502515316009521\n",
      "          vf_loss: 0.0010148227447643876\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.12517006802722\n",
      "    ram_util_percent: 95.91360544217686\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10763786343826774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9879990657196995\n",
      "    mean_inference_ms: 2.240513474019926\n",
      "    mean_raw_obs_processing_ms: 0.13405133909982805\n",
      "  time_since_restore: 22599.02867794037\n",
      "  time_this_iter_s: 220.11059713363647\n",
      "  time_total_s: 22599.02867794037\n",
      "  timers:\n",
      "    learn_throughput: 19.965\n",
      "    learn_time_ms: 200345.697\n",
      "    load_throughput: 3309311.399\n",
      "    load_time_ms: 1.209\n",
      "    sample_throughput: 18.436\n",
      "    sample_time_ms: 216967.072\n",
      "    update_time_ms: 14.894\n",
      "  timestamp: 1650240945\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-15-59\n",
      "  done: false\n",
      "  episode_len_mean: 391.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 4.02\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1603\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9994419813156128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00803735014051199\n",
      "          model: {}\n",
      "          policy_loss: -0.04895643889904022\n",
      "          total_loss: 0.030990809202194214\n",
      "          vf_explained_var: 0.6487386226654053\n",
      "          vf_loss: 0.07994726300239563\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.07849829351535\n",
      "    ram_util_percent: 95.91023890784983\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11166578703885444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0339905244142316\n",
      "    mean_inference_ms: 2.3532545257363036\n",
      "    mean_raw_obs_processing_ms: 0.23111437904587803\n",
      "  time_since_restore: 22617.567219495773\n",
      "  time_this_iter_s: 219.8663558959961\n",
      "  time_total_s: 22617.567219495773\n",
      "  timers:\n",
      "    learn_throughput: 19.986\n",
      "    learn_time_ms: 200141.386\n",
      "    load_throughput: 5953589.78\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 18.435\n",
      "    sample_time_ms: 216983.557\n",
      "    update_time_ms: 9.692\n",
      "  timestamp: 1650240959\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-19-29\n",
      "  done: false\n",
      "  episode_len_mean: 1914.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 212\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.930380731099721e-33\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128051698207855\n",
      "          total_loss: 0.014130126684904099\n",
      "          vf_explained_var: 8.671514706293237e-08\n",
      "          vf_loss: 2.078483248624252e-06\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.66275167785236\n",
      "    ram_util_percent: 95.87751677852349\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10763786343826774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9879990657196995\n",
      "    mean_inference_ms: 2.240513474019926\n",
      "    mean_raw_obs_processing_ms: 0.13405133909982805\n",
      "  time_since_restore: 22823.005494832993\n",
      "  time_this_iter_s: 223.9768168926239\n",
      "  time_total_s: 22823.005494832993\n",
      "  timers:\n",
      "    learn_throughput: 19.89\n",
      "    learn_time_ms: 201103.505\n",
      "    load_throughput: 3307680.297\n",
      "    load_time_ms: 1.209\n",
      "    sample_throughput: 18.401\n",
      "    sample_time_ms: 217373.987\n",
      "    update_time_ms: 15.011\n",
      "  timestamp: 1650241169\n",
      "  timesteps_since_restore: 424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-19-44\n",
      "  done: false\n",
      "  episode_len_mean: 389.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.96\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1613\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.986754834651947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00819174014031887\n",
      "          model: {}\n",
      "          policy_loss: -0.038567498326301575\n",
      "          total_loss: 0.0447692908346653\n",
      "          vf_explained_var: 0.6001971364021301\n",
      "          vf_loss: 0.08333679288625717\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.37993311036789\n",
      "    ram_util_percent: 95.876254180602\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11177737430008321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0348251101942791\n",
      "    mean_inference_ms: 2.356681031910632\n",
      "    mean_raw_obs_processing_ms: 0.23101082258438965\n",
      "  time_since_restore: 22842.012756347656\n",
      "  time_this_iter_s: 224.44553685188293\n",
      "  time_total_s: 22842.012756347656\n",
      "  timers:\n",
      "    learn_throughput: 19.924\n",
      "    learn_time_ms: 200760.694\n",
      "    load_throughput: 4997681.263\n",
      "    load_time_ms: 0.8\n",
      "    sample_throughput: 18.399\n",
      "    sample_time_ms: 217407.13\n",
      "    update_time_ms: 9.831\n",
      "  timestamp: 1650241184\n",
      "  timesteps_since_restore: 424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 01:22:09 (running for 06:23:29.10)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=3.96 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-23-06\n",
      "  done: false\n",
      "  episode_len_mean: 1914.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 212\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4651903655498604e-33\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128140173852444\n",
      "          total_loss: 0.014128724113106728\n",
      "          vf_explained_var: -4.4299710566519934e-07\n",
      "          vf_loss: 5.769662720922497e-07\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.73539518900344\n",
      "    ram_util_percent: 95.71821305841924\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10763786343826774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9879990657196995\n",
      "    mean_inference_ms: 2.240513474019926\n",
      "    mean_raw_obs_processing_ms: 0.13405133909982805\n",
      "  time_since_restore: 23039.660540103912\n",
      "  time_this_iter_s: 216.6550452709198\n",
      "  time_total_s: 23039.660540103912\n",
      "  timers:\n",
      "    learn_throughput: 19.885\n",
      "    learn_time_ms: 201154.8\n",
      "    load_throughput: 3275968.211\n",
      "    load_time_ms: 1.221\n",
      "    sample_throughput: 18.344\n",
      "    sample_time_ms: 218060.194\n",
      "    update_time_ms: 15.188\n",
      "  timestamp: 1650241386\n",
      "  timesteps_since_restore: 428000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-23-20\n",
      "  done: false\n",
      "  episode_len_mean: 386.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.95\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1624\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0009254217147827\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008005327545106411\n",
      "          model: {}\n",
      "          policy_loss: -0.05276983231306076\n",
      "          total_loss: 0.01553609874099493\n",
      "          vf_explained_var: 0.6566380858421326\n",
      "          vf_loss: 0.06830593943595886\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.05586206896552\n",
      "    ram_util_percent: 95.74413793103449\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11188630772787896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0356864446995182\n",
      "    mean_inference_ms: 2.3603058908948737\n",
      "    mean_raw_obs_processing_ms: 0.23090300338316147\n",
      "  time_since_restore: 23058.624813079834\n",
      "  time_this_iter_s: 216.61205673217773\n",
      "  time_total_s: 23058.624813079834\n",
      "  timers:\n",
      "    learn_throughput: 19.921\n",
      "    learn_time_ms: 200791.916\n",
      "    load_throughput: 4941888.126\n",
      "    load_time_ms: 0.809\n",
      "    sample_throughput: 18.352\n",
      "    sample_time_ms: 217964.589\n",
      "    update_time_ms: 9.695\n",
      "  timestamp: 1650241400\n",
      "  timesteps_since_restore: 428000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 1818.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 216\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2325951827749302e-33\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1458499731000435e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.901560189835932e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0009354975773021579\n",
      "          total_loss: 0.0017384792445227504\n",
      "          vf_explained_var: 0.1019829735159874\n",
      "          vf_loss: 0.0008029791642911732\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.57620689655172\n",
      "    ram_util_percent: 95.81344827586207\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10797682448807316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9901351403303758\n",
      "    mean_inference_ms: 2.2498734647885703\n",
      "    mean_raw_obs_processing_ms: 0.13443525691816152\n",
      "  time_since_restore: 23256.90041923523\n",
      "  time_this_iter_s: 217.23987913131714\n",
      "  time_total_s: 23256.90041923523\n",
      "  timers:\n",
      "    learn_throughput: 19.868\n",
      "    learn_time_ms: 201333.562\n",
      "    load_throughput: 3207206.133\n",
      "    load_time_ms: 1.247\n",
      "    sample_throughput: 18.344\n",
      "    sample_time_ms: 218057.551\n",
      "    update_time_ms: 14.917\n",
      "  timestamp: 1650241603\n",
      "  timesteps_since_restore: 432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 390.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 4.02\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1636\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9836265444755554\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007831714116036892\n",
      "          model: {}\n",
      "          policy_loss: -0.04682309553027153\n",
      "          total_loss: 0.03443608060479164\n",
      "          vf_explained_var: 0.6476240158081055\n",
      "          vf_loss: 0.08125918358564377\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.5051724137931\n",
      "    ram_util_percent: 95.84172413793102\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11200086067503796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0365820406884227\n",
      "    mean_inference_ms: 2.364080386939155\n",
      "    mean_raw_obs_processing_ms: 0.23078228392911207\n",
      "  time_since_restore: 23276.190300941467\n",
      "  time_this_iter_s: 217.5654878616333\n",
      "  time_total_s: 23276.190300941467\n",
      "  timers:\n",
      "    learn_throughput: 19.9\n",
      "    learn_time_ms: 201005.315\n",
      "    load_throughput: 4036380.609\n",
      "    load_time_ms: 0.991\n",
      "    sample_throughput: 18.352\n",
      "    sample_time_ms: 217959.402\n",
      "    update_time_ms: 9.73\n",
      "  timestamp: 1650241618\n",
      "  timesteps_since_restore: 432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-30-22\n",
      "  done: false\n",
      "  episode_len_mean: 1818.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 216\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.162975913874651e-34\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0038494053296744823\n",
      "          total_loss: -0.0038494013715535402\n",
      "          vf_explained_var: 5.651545961882221e-06\n",
      "          vf_loss: 5.152101856253921e-09\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.95924657534246\n",
      "    ram_util_percent: 95.87636986301371\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10797682448807316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9901351403303758\n",
      "    mean_inference_ms: 2.2498734647885703\n",
      "    mean_raw_obs_processing_ms: 0.13443525691816152\n",
      "  time_since_restore: 23475.499106168747\n",
      "  time_this_iter_s: 218.59868693351746\n",
      "  time_total_s: 23475.499106168747\n",
      "  timers:\n",
      "    learn_throughput: 19.832\n",
      "    learn_time_ms: 201698.064\n",
      "    load_throughput: 3186979.465\n",
      "    load_time_ms: 1.255\n",
      "    sample_throughput: 18.332\n",
      "    sample_time_ms: 218198.502\n",
      "    update_time_ms: 15.046\n",
      "  timestamp: 1650241822\n",
      "  timesteps_since_restore: 436000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-30-36\n",
      "  done: false\n",
      "  episode_len_mean: 390.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.05\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1646\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9679213762283325\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009508085437119007\n",
      "          model: {}\n",
      "          policy_loss: -0.05199861526489258\n",
      "          total_loss: 0.014200818724930286\n",
      "          vf_explained_var: 0.7294759154319763\n",
      "          vf_loss: 0.06619942933320999\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.31202749140893\n",
      "    ram_util_percent: 95.84810996563574\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11209520931657853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0373004186956354\n",
      "    mean_inference_ms: 2.3672411223067824\n",
      "    mean_raw_obs_processing_ms: 0.23069574100298687\n",
      "  time_since_restore: 23494.440984249115\n",
      "  time_this_iter_s: 218.2506833076477\n",
      "  time_total_s: 23494.440984249115\n",
      "  timers:\n",
      "    learn_throughput: 19.862\n",
      "    learn_time_ms: 201392.494\n",
      "    load_throughput: 4274014.368\n",
      "    load_time_ms: 0.936\n",
      "    sample_throughput: 18.339\n",
      "    sample_time_ms: 218120.061\n",
      "    update_time_ms: 7.976\n",
      "  timestamp: 1650241836\n",
      "  timesteps_since_restore: 436000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-33-58\n",
      "  done: false\n",
      "  episode_len_mean: 1818.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 216\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.0814879569373254e-34\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0011770017445087433\n",
      "          total_loss: -0.0011770017445087433\n",
      "          vf_explained_var: -0.00010884090443141758\n",
      "          vf_loss: 1.1808773403565453e-10\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.03737024221454\n",
      "    ram_util_percent: 95.86678200692043\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10797682448807316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9901351403303758\n",
      "    mean_inference_ms: 2.2498734647885703\n",
      "    mean_raw_obs_processing_ms: 0.13443525691816152\n",
      "  time_since_restore: 23691.418923139572\n",
      "  time_this_iter_s: 215.9198169708252\n",
      "  time_total_s: 23691.418923139572\n",
      "  timers:\n",
      "    learn_throughput: 19.845\n",
      "    learn_time_ms: 201561.146\n",
      "    load_throughput: 6010538.459\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 18.301\n",
      "    sample_time_ms: 218561.441\n",
      "    update_time_ms: 15.04\n",
      "  timestamp: 1650242038\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-34-14\n",
      "  done: false\n",
      "  episode_len_mean: 389.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.01\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1658\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0123358964920044\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006723917089402676\n",
      "          model: {}\n",
      "          policy_loss: -0.04305478185415268\n",
      "          total_loss: 0.008966069668531418\n",
      "          vf_explained_var: 0.7419840693473816\n",
      "          vf_loss: 0.052020855247974396\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.71099656357387\n",
      "    ram_util_percent: 95.86357388316151\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11221983361609367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0381810265277724\n",
      "    mean_inference_ms: 2.3714691677788\n",
      "    mean_raw_obs_processing_ms: 0.2306211611878652\n",
      "  time_since_restore: 23711.836493253708\n",
      "  time_this_iter_s: 217.3955090045929\n",
      "  time_total_s: 23711.836493253708\n",
      "  timers:\n",
      "    learn_throughput: 19.863\n",
      "    learn_time_ms: 201383.925\n",
      "    load_throughput: 3834091.138\n",
      "    load_time_ms: 1.043\n",
      "    sample_throughput: 18.295\n",
      "    sample_time_ms: 218636.899\n",
      "    update_time_ms: 7.61\n",
      "  timestamp: 1650242054\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-37-43\n",
      "  done: false\n",
      "  episode_len_mean: 1724.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 232\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5407439784686627e-34\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.120002807890741e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.3887455198458648e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.000898645375855267\n",
      "          total_loss: 0.002937793731689453\n",
      "          vf_explained_var: 0.586823582649231\n",
      "          vf_loss: 0.0038364343345165253\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.49337748344371\n",
      "    ram_util_percent: 95.85728476821193\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10920807141987193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9978862557330902\n",
      "    mean_inference_ms: 2.284309084071669\n",
      "    mean_raw_obs_processing_ms: 0.13591421533029022\n",
      "  time_since_restore: 23917.077398061752\n",
      "  time_this_iter_s: 225.65847492218018\n",
      "  time_total_s: 23917.077398061752\n",
      "  timers:\n",
      "    learn_throughput: 19.778\n",
      "    learn_time_ms: 202247.553\n",
      "    load_throughput: 6014848.17\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 18.3\n",
      "    sample_time_ms: 218582.809\n",
      "    update_time_ms: 16.766\n",
      "  timestamp: 1650242263\n",
      "  timesteps_since_restore: 444000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-37-58\n",
      "  done: false\n",
      "  episode_len_mean: 393.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.11\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1668\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9908093810081482\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007551965303719044\n",
      "          model: {}\n",
      "          policy_loss: -0.05097499117255211\n",
      "          total_loss: -0.005768996197730303\n",
      "          vf_explained_var: 0.7896331548690796\n",
      "          vf_loss: 0.045205991715192795\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.59733333333334\n",
      "    ram_util_percent: 95.85733333333333\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11232065457000283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0389224511992214\n",
      "    mean_inference_ms: 2.374990501078094\n",
      "    mean_raw_obs_processing_ms: 0.23055323012804416\n",
      "  time_since_restore: 23936.362131118774\n",
      "  time_this_iter_s: 224.52563786506653\n",
      "  time_total_s: 23936.362131118774\n",
      "  timers:\n",
      "    learn_throughput: 19.796\n",
      "    learn_time_ms: 202058.327\n",
      "    load_throughput: 3919452.4\n",
      "    load_time_ms: 1.021\n",
      "    sample_throughput: 18.289\n",
      "    sample_time_ms: 218708.252\n",
      "    update_time_ms: 7.034\n",
      "  timestamp: 1650242278\n",
      "  timesteps_since_restore: 444000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 01:38:50 (running for 06:40:09.90)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=4.11 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-41-24\n",
      "  done: false\n",
      "  episode_len_mean: 1623.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 237\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.703719892343314e-35\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.5128991231927094e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.2556009143071254e-26\n",
      "          model: {}\n",
      "          policy_loss: -4.569022712530568e-05\n",
      "          total_loss: 0.00011561596329556778\n",
      "          vf_explained_var: -0.0649128407239914\n",
      "          vf_loss: 0.00016130637959577143\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.89251700680272\n",
      "    ram_util_percent: 95.69115646258503\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10957648882227047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.000199851186373\n",
      "    mean_inference_ms: 2.2945518217699066\n",
      "    mean_raw_obs_processing_ms: 0.1363513499669519\n",
      "  time_since_restore: 24137.47229743004\n",
      "  time_this_iter_s: 220.39489936828613\n",
      "  time_total_s: 24137.47229743004\n",
      "  timers:\n",
      "    learn_throughput: 19.741\n",
      "    learn_time_ms: 202624.685\n",
      "    load_throughput: 6094821.811\n",
      "    load_time_ms: 0.656\n",
      "    sample_throughput: 18.233\n",
      "    sample_time_ms: 219385.486\n",
      "    update_time_ms: 17.186\n",
      "  timestamp: 1650242484\n",
      "  timesteps_since_restore: 448000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-41-38\n",
      "  done: false\n",
      "  episode_len_mean: 396.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.15\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1678\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9829548001289368\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008265594951808453\n",
      "          model: {}\n",
      "          policy_loss: -0.044723253697156906\n",
      "          total_loss: 0.003891495754942298\n",
      "          vf_explained_var: 0.8195652365684509\n",
      "          vf_loss: 0.0486147478222847\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.7561224489796\n",
      "    ram_util_percent: 95.65374149659864\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11242624061288872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0396535033079588\n",
      "    mean_inference_ms: 2.378691294643828\n",
      "    mean_raw_obs_processing_ms: 0.23051877602407508\n",
      "  time_since_restore: 24156.468200206757\n",
      "  time_this_iter_s: 220.10606908798218\n",
      "  time_total_s: 24156.468200206757\n",
      "  timers:\n",
      "    learn_throughput: 19.768\n",
      "    learn_time_ms: 202343.811\n",
      "    load_throughput: 3908404.231\n",
      "    load_time_ms: 1.023\n",
      "    sample_throughput: 18.223\n",
      "    sample_time_ms: 219503.138\n",
      "    update_time_ms: 7.355\n",
      "  timestamp: 1650242498\n",
      "  timesteps_since_restore: 448000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 1623.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 237\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.851859946171657e-35\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128066599369049\n",
      "          total_loss: 0.014128215610980988\n",
      "          vf_explained_var: 6.338601821198608e-08\n",
      "          vf_loss: 1.5207399428618373e-07\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.04349315068492\n",
      "    ram_util_percent: 95.4722602739726\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10957648882227047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.000199851186373\n",
      "    mean_inference_ms: 2.2945518217699066\n",
      "    mean_raw_obs_processing_ms: 0.1363513499669519\n",
      "  time_since_restore: 24356.178047418594\n",
      "  time_this_iter_s: 218.7057499885559\n",
      "  time_total_s: 24356.178047418594\n",
      "  timers:\n",
      "    learn_throughput: 19.752\n",
      "    learn_time_ms: 202510.208\n",
      "    load_throughput: 6196799.882\n",
      "    load_time_ms: 0.645\n",
      "    sample_throughput: 18.202\n",
      "    sample_time_ms: 219756.945\n",
      "    update_time_ms: 19.29\n",
      "  timestamp: 1650242703\n",
      "  timesteps_since_restore: 452000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-45-18\n",
      "  done: false\n",
      "  episode_len_mean: 391.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.09\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1692\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9880812168121338\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007380732335150242\n",
      "          model: {}\n",
      "          policy_loss: -0.042620111256837845\n",
      "          total_loss: 0.02088581770658493\n",
      "          vf_explained_var: 0.7121186852455139\n",
      "          vf_loss: 0.06350593268871307\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.22047781569967\n",
      "    ram_util_percent: 95.46962457337884\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11257391035278808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0406512163018475\n",
      "    mean_inference_ms: 2.383813593280511\n",
      "    mean_raw_obs_processing_ms: 0.23051237527997287\n",
      "  time_since_restore: 24375.88464832306\n",
      "  time_this_iter_s: 219.4164481163025\n",
      "  time_total_s: 24375.88464832306\n",
      "  timers:\n",
      "    learn_throughput: 19.77\n",
      "    learn_time_ms: 202329.449\n",
      "    load_throughput: 3521074.547\n",
      "    load_time_ms: 1.136\n",
      "    sample_throughput: 18.201\n",
      "    sample_time_ms: 219767.386\n",
      "    update_time_ms: 7.98\n",
      "  timestamp: 1650242718\n",
      "  timesteps_since_restore: 452000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-48-37\n",
      "  done: false\n",
      "  episode_len_mean: 1722.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 240\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9259299730858284e-35\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.384504794747082e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.2009938670359475e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0007526038680225611\n",
      "          total_loss: 0.0010926341637969017\n",
      "          vf_explained_var: -0.00042999058496207\n",
      "          vf_loss: 0.00034003634937107563\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.99094076655052\n",
      "    ram_util_percent: 95.65958188153311\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10979055634541814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0015445619344372\n",
      "    mean_inference_ms: 2.3005460666391286\n",
      "    mean_raw_obs_processing_ms: 0.13660014709992704\n",
      "  time_since_restore: 24569.993346452713\n",
      "  time_this_iter_s: 213.81529903411865\n",
      "  time_total_s: 24569.993346452713\n",
      "  timers:\n",
      "    learn_throughput: 19.809\n",
      "    learn_time_ms: 201924.676\n",
      "    load_throughput: 6069026.19\n",
      "    load_time_ms: 0.659\n",
      "    sample_throughput: 18.201\n",
      "    sample_time_ms: 219766.278\n",
      "    update_time_ms: 19.294\n",
      "  timestamp: 1650242917\n",
      "  timesteps_since_restore: 456000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-48-52\n",
      "  done: false\n",
      "  episode_len_mean: 385.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.97\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1703\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9976238012313843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007827828638255596\n",
      "          model: {}\n",
      "          policy_loss: -0.0410536453127861\n",
      "          total_loss: 0.00903527345508337\n",
      "          vf_explained_var: 0.7295919060707092\n",
      "          vf_loss: 0.05008891224861145\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.70557491289199\n",
      "    ram_util_percent: 95.68571428571428\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11269468357269062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.041404016050557\n",
      "    mean_inference_ms: 2.3876357296301745\n",
      "    mean_raw_obs_processing_ms: 0.23051090323746784\n",
      "  time_since_restore: 24589.819303274155\n",
      "  time_this_iter_s: 213.93465495109558\n",
      "  time_total_s: 24589.819303274155\n",
      "  timers:\n",
      "    learn_throughput: 19.832\n",
      "    learn_time_ms: 201696.256\n",
      "    load_throughput: 3477864.013\n",
      "    load_time_ms: 1.15\n",
      "    sample_throughput: 18.193\n",
      "    sample_time_ms: 219861.349\n",
      "    update_time_ms: 7.874\n",
      "  timestamp: 1650242932\n",
      "  timesteps_since_restore: 456000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-52-10\n",
      "  done: false\n",
      "  episode_len_mean: 1722.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 240\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.629649865429142e-36\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412828080356121\n",
      "          total_loss: 0.014194871298968792\n",
      "          vf_explained_var: -2.775141005884052e-08\n",
      "          vf_loss: 6.659862992819399e-05\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.35874125874126\n",
      "    ram_util_percent: 95.58671328671329\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10979055634541814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0015445619344372\n",
      "    mean_inference_ms: 2.3005460666391286\n",
      "    mean_raw_obs_processing_ms: 0.13660014709992704\n",
      "  time_since_restore: 24783.498945236206\n",
      "  time_this_iter_s: 213.50559878349304\n",
      "  time_total_s: 24783.498945236206\n",
      "  timers:\n",
      "    learn_throughput: 19.872\n",
      "    learn_time_ms: 201290.706\n",
      "    load_throughput: 6118159.142\n",
      "    load_time_ms: 0.654\n",
      "    sample_throughput: 18.251\n",
      "    sample_time_ms: 219162.348\n",
      "    update_time_ms: 18.623\n",
      "  timestamp: 1650243130\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-52-26\n",
      "  done: false\n",
      "  episode_len_mean: 384.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.98\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1715\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.967969536781311\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010069980286061764\n",
      "          model: {}\n",
      "          policy_loss: -0.048532310873270035\n",
      "          total_loss: 0.007945959456264973\n",
      "          vf_explained_var: 0.7719171047210693\n",
      "          vf_loss: 0.05647826939821243\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.67630662020905\n",
      "    ram_util_percent: 95.53484320557492\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11281742943538849\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0421386168700097\n",
      "    mean_inference_ms: 2.3914070902050315\n",
      "    mean_raw_obs_processing_ms: 0.2304805738360527\n",
      "  time_since_restore: 24803.610360383987\n",
      "  time_this_iter_s: 213.79105710983276\n",
      "  time_total_s: 24803.610360383987\n",
      "  timers:\n",
      "    learn_throughput: 19.884\n",
      "    learn_time_ms: 201162.328\n",
      "    load_throughput: 3436123.377\n",
      "    load_time_ms: 1.164\n",
      "    sample_throughput: 18.252\n",
      "    sample_time_ms: 219154.329\n",
      "    update_time_ms: 7.961\n",
      "  timestamp: 1650243146\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 01:55:30 (running for 06:56:50.57)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=3.98 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-55-45\n",
      "  done: false\n",
      "  episode_len_mean: 1722.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 240\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.814824932714571e-36\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128022827208042\n",
      "          total_loss: 0.014141296036541462\n",
      "          vf_explained_var: -8.460014022659834e-09\n",
      "          vf_loss: 1.3267004760564305e-05\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.8621527777778\n",
      "    ram_util_percent: 95.59618055555556\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10979055634541814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0015445619344372\n",
      "    mean_inference_ms: 2.3005460666391286\n",
      "    mean_raw_obs_processing_ms: 0.13660014709992704\n",
      "  time_since_restore: 24998.2828912735\n",
      "  time_this_iter_s: 214.78394603729248\n",
      "  time_total_s: 24998.2828912735\n",
      "  timers:\n",
      "    learn_throughput: 19.967\n",
      "    learn_time_ms: 200325.905\n",
      "    load_throughput: 6145050.179\n",
      "    load_time_ms: 0.651\n",
      "    sample_throughput: 18.301\n",
      "    sample_time_ms: 218563.098\n",
      "    update_time_ms: 17.521\n",
      "  timestamp: 1650243345\n",
      "  timesteps_since_restore: 464000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-56-01\n",
      "  done: false\n",
      "  episode_len_mean: 380.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.91\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1726\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.976672351360321\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008235812187194824\n",
      "          model: {}\n",
      "          policy_loss: -0.03778420388698578\n",
      "          total_loss: 0.04155001416802406\n",
      "          vf_explained_var: 0.7041432857513428\n",
      "          vf_loss: 0.07933421432971954\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.53391003460207\n",
      "    ram_util_percent: 95.59307958477508\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1129368302648643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0428178957259029\n",
      "    mean_inference_ms: 2.394776909843991\n",
      "    mean_raw_obs_processing_ms: 0.23046680668929725\n",
      "  time_since_restore: 25018.883328437805\n",
      "  time_this_iter_s: 215.27296805381775\n",
      "  time_total_s: 25018.883328437805\n",
      "  timers:\n",
      "    learn_throughput: 19.965\n",
      "    learn_time_ms: 200353.237\n",
      "    load_throughput: 3785045.911\n",
      "    load_time_ms: 1.057\n",
      "    sample_throughput: 18.306\n",
      "    sample_time_ms: 218508.389\n",
      "    update_time_ms: 7.682\n",
      "  timestamp: 1650243361\n",
      "  timesteps_since_restore: 464000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-59-20\n",
      "  done: false\n",
      "  episode_len_mean: 1819.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 241\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4074124663572855e-36\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.779671820652063e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8477158108227314e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0055471137166023254\n",
      "          total_loss: 0.005961195565760136\n",
      "          vf_explained_var: -0.02715153433382511\n",
      "          vf_loss: 0.00041408700053580105\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.08719723183391\n",
      "    ram_util_percent: 95.39584775086506\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10986579680387377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0020206964480607\n",
      "    mean_inference_ms: 2.3026426543077236\n",
      "    mean_raw_obs_processing_ms: 0.13668266936959877\n",
      "  time_since_restore: 25213.34428215027\n",
      "  time_this_iter_s: 215.06139087677002\n",
      "  time_total_s: 25213.34428215027\n",
      "  timers:\n",
      "    learn_throughput: 19.984\n",
      "    learn_time_ms: 200156.099\n",
      "    load_throughput: 6531657.712\n",
      "    load_time_ms: 0.612\n",
      "    sample_throughput: 18.38\n",
      "    sample_time_ms: 217623.528\n",
      "    update_time_ms: 17.468\n",
      "  timestamp: 1650243560\n",
      "  timesteps_since_restore: 468000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_01-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 378.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.89\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1738\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9672433137893677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0084694130346179\n",
      "          model: {}\n",
      "          policy_loss: -0.05281577259302139\n",
      "          total_loss: 0.02567536011338234\n",
      "          vf_explained_var: 0.6296008229255676\n",
      "          vf_loss: 0.07849112898111343\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.36875\n",
      "    ram_util_percent: 95.36840277777779\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11306691323764514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0435599073273436\n",
      "    mean_inference_ms: 2.3984540063689055\n",
      "    mean_raw_obs_processing_ms: 0.2304707755689593\n",
      "  time_since_restore: 25234.01270341873\n",
      "  time_this_iter_s: 215.1293749809265\n",
      "  time_total_s: 25234.01270341873\n",
      "  timers:\n",
      "    learn_throughput: 19.982\n",
      "    learn_time_ms: 200176.948\n",
      "    load_throughput: 3836370.621\n",
      "    load_time_ms: 1.043\n",
      "    sample_throughput: 18.37\n",
      "    sample_time_ms: 217746.99\n",
      "    update_time_ms: 7.585\n",
      "  timestamp: 1650243576\n",
      "  timesteps_since_restore: 468000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-02-55\n",
      "  done: false\n",
      "  episode_len_mean: 1819.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 241\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2037062331786428e-36\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128105714917183\n",
      "          total_loss: 0.01416452694684267\n",
      "          vf_explained_var: -3.678824356256882e-08\n",
      "          vf_loss: 3.641954390332103e-05\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.96076388888889\n",
      "    ram_util_percent: 95.43159722222224\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10986579680387377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0020206964480607\n",
      "    mean_inference_ms: 2.3026426543077236\n",
      "    mean_raw_obs_processing_ms: 0.13668266936959877\n",
      "  time_since_restore: 25427.596897363663\n",
      "  time_this_iter_s: 214.25261521339417\n",
      "  time_total_s: 25427.596897363663\n",
      "  timers:\n",
      "    learn_throughput: 20.018\n",
      "    learn_time_ms: 199819.534\n",
      "    load_throughput: 6863250.562\n",
      "    load_time_ms: 0.583\n",
      "    sample_throughput: 18.392\n",
      "    sample_time_ms: 217491.745\n",
      "    update_time_ms: 17.117\n",
      "  timestamp: 1650243775\n",
      "  timesteps_since_restore: 472000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-03-10\n",
      "  done: false\n",
      "  episode_len_mean: 371.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.71\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1750\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0114203691482544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007650323677808046\n",
      "          model: {}\n",
      "          policy_loss: -0.0428735688328743\n",
      "          total_loss: 0.001601592404767871\n",
      "          vf_explained_var: 0.640442430973053\n",
      "          vf_loss: 0.04447515308856964\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.70381944444443\n",
      "    ram_util_percent: 95.4388888888889\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11318735064828701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.044286907081929\n",
      "    mean_inference_ms: 2.401970871915474\n",
      "    mean_raw_obs_processing_ms: 0.23047344413082754\n",
      "  time_since_restore: 25448.361916303635\n",
      "  time_this_iter_s: 214.34921288490295\n",
      "  time_total_s: 25448.361916303635\n",
      "  timers:\n",
      "    learn_throughput: 20.016\n",
      "    learn_time_ms: 199842.988\n",
      "    load_throughput: 4654519.628\n",
      "    load_time_ms: 0.859\n",
      "    sample_throughput: 18.384\n",
      "    sample_time_ms: 217584.411\n",
      "    update_time_ms: 7.573\n",
      "  timestamp: 1650243790\n",
      "  timesteps_since_restore: 472000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-06-29\n",
      "  done: false\n",
      "  episode_len_mean: 1917.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 245\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.018531165893214e-37\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.073092170970228e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.0602033036461874e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.004173471592366695\n",
      "          total_loss: 0.004555810708552599\n",
      "          vf_explained_var: 0.1448291689157486\n",
      "          vf_loss: 0.00038233958184719086\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.72387543252594\n",
      "    ram_util_percent: 95.50622837370241\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11017911266209018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0039990076413556\n",
      "    mean_inference_ms: 2.31132913729178\n",
      "    mean_raw_obs_processing_ms: 0.13702020415760663\n",
      "  time_since_restore: 25642.12198138237\n",
      "  time_this_iter_s: 214.52508401870728\n",
      "  time_total_s: 25642.12198138237\n",
      "  timers:\n",
      "    learn_throughput: 20.063\n",
      "    learn_time_ms: 199368.77\n",
      "    load_throughput: 6969887.416\n",
      "    load_time_ms: 0.574\n",
      "    sample_throughput: 18.416\n",
      "    sample_time_ms: 217200.111\n",
      "    update_time_ms: 16.154\n",
      "  timestamp: 1650243989\n",
      "  timesteps_since_restore: 476000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-06-45\n",
      "  done: false\n",
      "  episode_len_mean: 375.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.79\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1761\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9469975829124451\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00994822010397911\n",
      "          model: {}\n",
      "          policy_loss: -0.041610054671764374\n",
      "          total_loss: 0.02455589920282364\n",
      "          vf_explained_var: 0.7360448837280273\n",
      "          vf_loss: 0.06616595387458801\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.4446366782007\n",
      "    ram_util_percent: 95.50865051903114\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11329016204039394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0449752334183622\n",
      "    mean_inference_ms: 2.405162268585958\n",
      "    mean_raw_obs_processing_ms: 0.23045257168186647\n",
      "  time_since_restore: 25662.950251102448\n",
      "  time_this_iter_s: 214.58833479881287\n",
      "  time_total_s: 25662.950251102448\n",
      "  timers:\n",
      "    learn_throughput: 20.058\n",
      "    learn_time_ms: 199423.03\n",
      "    load_throughput: 4269880.892\n",
      "    load_time_ms: 0.937\n",
      "    sample_throughput: 18.407\n",
      "    sample_time_ms: 217304.359\n",
      "    update_time_ms: 7.748\n",
      "  timestamp: 1650244005\n",
      "  timesteps_since_restore: 476000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-10-06\n",
      "  done: false\n",
      "  episode_len_mean: 1820.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 254\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.009265582946607e-37\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.2146913358324224e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8025810647970332e-26\n",
      "          model: {}\n",
      "          policy_loss: -4.004393849754706e-05\n",
      "          total_loss: 0.0006681755185127258\n",
      "          vf_explained_var: 0.2234903872013092\n",
      "          vf_loss: 0.0007082137162797153\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.25034482758622\n",
      "    ram_util_percent: 95.3603448275862\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11085921733314993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.008325407196906\n",
      "    mean_inference_ms: 2.3303558102236983\n",
      "    mean_raw_obs_processing_ms: 0.13771628484871623\n",
      "  time_since_restore: 25858.645476341248\n",
      "  time_this_iter_s: 216.52349495887756\n",
      "  time_total_s: 25858.645476341248\n",
      "  timers:\n",
      "    learn_throughput: 20.061\n",
      "    learn_time_ms: 199395.076\n",
      "    load_throughput: 7006563.374\n",
      "    load_time_ms: 0.571\n",
      "    sample_throughput: 18.452\n",
      "    sample_time_ms: 216777.841\n",
      "    update_time_ms: 16.403\n",
      "  timestamp: 1650244206\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-10-20\n",
      "  done: false\n",
      "  episode_len_mean: 370.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.67\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1772\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9882349371910095\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007977050729095936\n",
      "          model: {}\n",
      "          policy_loss: -0.03735390678048134\n",
      "          total_loss: 0.006499758921563625\n",
      "          vf_explained_var: 0.7064707279205322\n",
      "          vf_loss: 0.04385366663336754\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.2454861111111\n",
      "    ram_util_percent: 95.41562499999999\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11338900810382552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.045659249174051\n",
      "    mean_inference_ms: 2.4082759535511054\n",
      "    mean_raw_obs_processing_ms: 0.2304424649152652\n",
      "  time_since_restore: 25878.138718128204\n",
      "  time_this_iter_s: 215.18846702575684\n",
      "  time_total_s: 25878.138718128204\n",
      "  timers:\n",
      "    learn_throughput: 20.073\n",
      "    learn_time_ms: 199275.719\n",
      "    load_throughput: 4783649.635\n",
      "    load_time_ms: 0.836\n",
      "    sample_throughput: 18.449\n",
      "    sample_time_ms: 216812.733\n",
      "    update_time_ms: 7.73\n",
      "  timestamp: 1650244220\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 02:12:11 (running for 07:13:31.56)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=3.67 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-13-42\n",
      "  done: false\n",
      "  episode_len_mean: 1820.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 254\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5046327914733034e-37\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128132723271847\n",
      "          total_loss: -0.0141242491081357\n",
      "          vf_explained_var: -1.1215927742114218e-08\n",
      "          vf_loss: 3.889965682901675e-06\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.48172413793102\n",
      "    ram_util_percent: 95.46620689655172\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11085921733314993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.008325407196906\n",
      "    mean_inference_ms: 2.3303558102236983\n",
      "    mean_raw_obs_processing_ms: 0.13771628484871623\n",
      "  time_since_restore: 26075.08439540863\n",
      "  time_this_iter_s: 216.4389190673828\n",
      "  time_total_s: 26075.08439540863\n",
      "  timers:\n",
      "    learn_throughput: 20.141\n",
      "    learn_time_ms: 198602.665\n",
      "    load_throughput: 6935886.56\n",
      "    load_time_ms: 0.577\n",
      "    sample_throughput: 18.462\n",
      "    sample_time_ms: 216666.84\n",
      "    update_time_ms: 14.163\n",
      "  timestamp: 1650244422\n",
      "  timesteps_since_restore: 484000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-13-56\n",
      "  done: false\n",
      "  episode_len_mean: 374.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.75\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1783\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9218725562095642\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008641048334538937\n",
      "          model: {}\n",
      "          policy_loss: -0.0403372198343277\n",
      "          total_loss: 0.023326553404331207\n",
      "          vf_explained_var: 0.7473276853561401\n",
      "          vf_loss: 0.0636637806892395\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.92975778546712\n",
      "    ram_util_percent: 95.4076124567474\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11348453666288179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.046347661312728\n",
      "    mean_inference_ms: 2.411102788896776\n",
      "    mean_raw_obs_processing_ms: 0.2304017492488668\n",
      "  time_since_restore: 26094.005331993103\n",
      "  time_this_iter_s: 215.86661386489868\n",
      "  time_total_s: 26094.005331993103\n",
      "  timers:\n",
      "    learn_throughput: 20.155\n",
      "    learn_time_ms: 198464.349\n",
      "    load_throughput: 4901319.311\n",
      "    load_time_ms: 0.816\n",
      "    sample_throughput: 18.466\n",
      "    sample_time_ms: 216611.507\n",
      "    update_time_ms: 7.355\n",
      "  timestamp: 1650244436\n",
      "  timesteps_since_restore: 484000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-17-18\n",
      "  done: false\n",
      "  episode_len_mean: 1820.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 254\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.523163957366517e-38\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.018860626965761185\n",
      "          total_loss: 0.01886206679046154\n",
      "          vf_explained_var: -0.032258231192827225\n",
      "          vf_loss: 1.437171363249945e-06\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.80827586206897\n",
      "    ram_util_percent: 95.36068965517241\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11085921733314993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.008325407196906\n",
      "    mean_inference_ms: 2.3303558102236983\n",
      "    mean_raw_obs_processing_ms: 0.13771628484871623\n",
      "  time_since_restore: 26290.732048273087\n",
      "  time_this_iter_s: 215.64765286445618\n",
      "  time_total_s: 26290.732048273087\n",
      "  timers:\n",
      "    learn_throughput: 20.178\n",
      "    learn_time_ms: 198235.986\n",
      "    load_throughput: 6820561.021\n",
      "    load_time_ms: 0.586\n",
      "    sample_throughput: 18.538\n",
      "    sample_time_ms: 215769.921\n",
      "    update_time_ms: 13.282\n",
      "  timestamp: 1650244638\n",
      "  timesteps_since_restore: 488000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-17-31\n",
      "  done: false\n",
      "  episode_len_mean: 377.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 3.78\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1795\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9297804832458496\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008790405467152596\n",
      "          model: {}\n",
      "          policy_loss: -0.05611054599285126\n",
      "          total_loss: 0.021862834692001343\n",
      "          vf_explained_var: 0.6251131892204285\n",
      "          vf_loss: 0.0779733881354332\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.46562499999999\n",
      "    ram_util_percent: 95.34131944444445\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11358323140533218\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0471095700974073\n",
      "    mean_inference_ms: 2.414055226899927\n",
      "    mean_raw_obs_processing_ms: 0.23035051853570387\n",
      "  time_since_restore: 26309.0407269001\n",
      "  time_this_iter_s: 215.03539490699768\n",
      "  time_total_s: 26309.0407269001\n",
      "  timers:\n",
      "    learn_throughput: 20.19\n",
      "    learn_time_ms: 198114.504\n",
      "    load_throughput: 4960884.71\n",
      "    load_time_ms: 0.806\n",
      "    sample_throughput: 18.549\n",
      "    sample_time_ms: 215648.438\n",
      "    update_time_ms: 7.085\n",
      "  timestamp: 1650244651\n",
      "  timesteps_since_restore: 488000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-20-55\n",
      "  done: false\n",
      "  episode_len_mean: 1917.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 258\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7615819786832586e-38\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.61837803181828e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -6.386616347925356e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.001121416687965393\n",
      "          total_loss: -0.0007155460771173239\n",
      "          vf_explained_var: 0.08305571973323822\n",
      "          vf_loss: 0.00040587480179965496\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.19209621993127\n",
      "    ram_util_percent: 95.41030927835052\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1111691348128264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.010316353655473\n",
      "    mean_inference_ms: 2.3390358256175476\n",
      "    mean_raw_obs_processing_ms: 0.1380307864676781\n",
      "  time_since_restore: 26507.865374326706\n",
      "  time_this_iter_s: 217.13332605361938\n",
      "  time_total_s: 26507.865374326706\n",
      "  timers:\n",
      "    learn_throughput: 20.188\n",
      "    learn_time_ms: 198135.931\n",
      "    load_throughput: 6745965.42\n",
      "    load_time_ms: 0.593\n",
      "    sample_throughput: 18.575\n",
      "    sample_time_ms: 215342.139\n",
      "    update_time_ms: 10.707\n",
      "  timestamp: 1650244855\n",
      "  timesteps_since_restore: 492000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 378.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.8\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1805\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9229768514633179\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008678202517330647\n",
      "          model: {}\n",
      "          policy_loss: -0.042376089841127396\n",
      "          total_loss: 0.015203595161437988\n",
      "          vf_explained_var: 0.7840504050254822\n",
      "          vf_loss: 0.05757969617843628\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.74226804123711\n",
      "    ram_util_percent: 95.41512027491409\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1136602155249691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0477039629765033\n",
      "    mean_inference_ms: 2.416346664401469\n",
      "    mean_raw_obs_processing_ms: 0.23029165139381122\n",
      "  time_since_restore: 26526.404684066772\n",
      "  time_this_iter_s: 217.36395716667175\n",
      "  time_total_s: 26526.404684066772\n",
      "  timers:\n",
      "    learn_throughput: 20.207\n",
      "    learn_time_ms: 197955.956\n",
      "    load_throughput: 5702850.539\n",
      "    load_time_ms: 0.701\n",
      "    sample_throughput: 18.583\n",
      "    sample_time_ms: 215251.345\n",
      "    update_time_ms: 6.406\n",
      "  timestamp: 1650244869\n",
      "  timesteps_since_restore: 492000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-24-33\n",
      "  done: false\n",
      "  episode_len_mean: 1917.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 258\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8807909893416293e-38\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.686394281889638e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.041380143722529e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014127936214208603\n",
      "          total_loss: -0.01412560697644949\n",
      "          vf_explained_var: -7.588376149669784e-08\n",
      "          vf_loss: 2.3381678602163447e-06\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.87628865979381\n",
      "    ram_util_percent: 95.5938144329897\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1111691348128264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.010316353655473\n",
      "    mean_inference_ms: 2.3390358256175476\n",
      "    mean_raw_obs_processing_ms: 0.1380307864676781\n",
      "  time_since_restore: 26725.58456134796\n",
      "  time_this_iter_s: 217.7191870212555\n",
      "  time_total_s: 26725.58456134796\n",
      "  timers:\n",
      "    learn_throughput: 20.133\n",
      "    learn_time_ms: 198678.47\n",
      "    load_throughput: 6976553.56\n",
      "    load_time_ms: 0.573\n",
      "    sample_throughput: 18.597\n",
      "    sample_time_ms: 215083.458\n",
      "    update_time_ms: 11.519\n",
      "  timestamp: 1650245073\n",
      "  timesteps_since_restore: 496000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-24-46\n",
      "  done: false\n",
      "  episode_len_mean: 380.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.84\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1817\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9614331126213074\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01022249273955822\n",
      "          model: {}\n",
      "          policy_loss: -0.040670979768037796\n",
      "          total_loss: 0.019566455855965614\n",
      "          vf_explained_var: 0.7447727918624878\n",
      "          vf_loss: 0.06023744121193886\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.83264604810996\n",
      "    ram_util_percent: 95.57250859106529\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11374201893255304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0483525705932784\n",
      "    mean_inference_ms: 2.4189398660118218\n",
      "    mean_raw_obs_processing_ms: 0.2302213648405483\n",
      "  time_since_restore: 26744.070625305176\n",
      "  time_this_iter_s: 217.66594123840332\n",
      "  time_total_s: 26744.070625305176\n",
      "  timers:\n",
      "    learn_throughput: 20.156\n",
      "    learn_time_ms: 198449.154\n",
      "    load_throughput: 5948734.532\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 18.607\n",
      "    sample_time_ms: 214968.838\n",
      "    update_time_ms: 6.309\n",
      "  timestamp: 1650245086\n",
      "  timesteps_since_restore: 496000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 2014.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 259\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.856118818888448e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -6.896624167614473e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.000747799058444798\n",
      "          total_loss: -0.0007417229935526848\n",
      "          vf_explained_var: -0.06451670825481415\n",
      "          vf_loss: 6.076801582821645e-06\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.61241379310344\n",
      "    ram_util_percent: 95.54310344827586\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11124822423080617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0108235285201734\n",
      "    mean_inference_ms: 2.341230303958966\n",
      "    mean_raw_obs_processing_ms: 0.13810924312752962\n",
      "  time_since_restore: 26941.93041729927\n",
      "  time_this_iter_s: 216.3458559513092\n",
      "  time_total_s: 26941.93041729927\n",
      "  timers:\n",
      "    learn_throughput: 20.096\n",
      "    learn_time_ms: 199043.988\n",
      "    load_throughput: 7014472.782\n",
      "    load_time_ms: 0.57\n",
      "    sample_throughput: 18.558\n",
      "    sample_time_ms: 215536.6\n",
      "    update_time_ms: 11.562\n",
      "  timestamp: 1650245289\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-28-23\n",
      "  done: false\n",
      "  episode_len_mean: 386.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.96\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1827\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9489123225212097\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008870633319020271\n",
      "          model: {}\n",
      "          policy_loss: -0.056429896503686905\n",
      "          total_loss: 0.001938577857799828\n",
      "          vf_explained_var: 0.7404375672340393\n",
      "          vf_loss: 0.05836847051978111\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.17965517241379\n",
      "    ram_util_percent: 95.54310344827586\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11380257547160558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0488636027177256\n",
      "    mean_inference_ms: 2.420894322755927\n",
      "    mean_raw_obs_processing_ms: 0.23015362274583892\n",
      "  time_since_restore: 26960.21456551552\n",
      "  time_this_iter_s: 216.1439402103424\n",
      "  time_total_s: 26960.21456551552\n",
      "  timers:\n",
      "    learn_throughput: 20.124\n",
      "    learn_time_ms: 198767.483\n",
      "    load_throughput: 6117936.039\n",
      "    load_time_ms: 0.654\n",
      "    sample_throughput: 18.572\n",
      "    sample_time_ms: 215379.286\n",
      "    update_time_ms: 6.17\n",
      "  timestamp: 1650245303\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 02:28:52 (running for 07:30:12.08)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=3.96 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-31-49\n",
      "  done: false\n",
      "  episode_len_mean: 2014.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 259\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.686394281889638e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.041380143722529e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128309674561024\n",
      "          total_loss: -0.014127467758953571\n",
      "          vf_explained_var: -1.5907390604752436e-07\n",
      "          vf_loss: 8.386384706682293e-07\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.73412969283277\n",
      "    ram_util_percent: 95.58191126279864\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11124822423080617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0108235285201734\n",
      "    mean_inference_ms: 2.341230303958966\n",
      "    mean_raw_obs_processing_ms: 0.13810924312752962\n",
      "  time_since_restore: 27161.121183395386\n",
      "  time_this_iter_s: 219.1907660961151\n",
      "  time_total_s: 27161.121183395386\n",
      "  timers:\n",
      "    learn_throughput: 20.042\n",
      "    learn_time_ms: 199577.558\n",
      "    load_throughput: 6962367.1\n",
      "    load_time_ms: 0.575\n",
      "    sample_throughput: 18.535\n",
      "    sample_time_ms: 215808.295\n",
      "    update_time_ms: 13.381\n",
      "  timestamp: 1650245509\n",
      "  timesteps_since_restore: 504000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-32-01\n",
      "  done: false\n",
      "  episode_len_mean: 391.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.03\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1838\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.899940013885498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008869048207998276\n",
      "          model: {}\n",
      "          policy_loss: -0.04837696626782417\n",
      "          total_loss: 0.030572405084967613\n",
      "          vf_explained_var: 0.7066226601600647\n",
      "          vf_loss: 0.07894936949014664\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.06518771331058\n",
      "    ram_util_percent: 95.5825938566553\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11385936452670346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0493935905659804\n",
      "    mean_inference_ms: 2.4228003746725237\n",
      "    mean_raw_obs_processing_ms: 0.23005756759182255\n",
      "  time_since_restore: 27178.785255670547\n",
      "  time_this_iter_s: 218.5706901550293\n",
      "  time_total_s: 27178.785255670547\n",
      "  timers:\n",
      "    learn_throughput: 20.079\n",
      "    learn_time_ms: 199211.602\n",
      "    load_throughput: 5636179.662\n",
      "    load_time_ms: 0.71\n",
      "    sample_throughput: 18.554\n",
      "    sample_time_ms: 215585.1\n",
      "    update_time_ms: 5.901\n",
      "  timestamp: 1650245521\n",
      "  timesteps_since_restore: 504000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-35-25\n",
      "  done: false\n",
      "  episode_len_mean: 2014.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 259\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.686394281889638e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.041380143722529e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128059148788452\n",
      "          total_loss: -0.014127515256404877\n",
      "          vf_explained_var: 2.2963810408782592e-07\n",
      "          vf_loss: 5.379308163355745e-07\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.1955172413793\n",
      "    ram_util_percent: 95.38620689655173\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11124822423080617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0108235285201734\n",
      "    mean_inference_ms: 2.341230303958966\n",
      "    mean_raw_obs_processing_ms: 0.13810924312752962\n",
      "  time_since_restore: 27377.880357265472\n",
      "  time_this_iter_s: 216.75917387008667\n",
      "  time_total_s: 27377.880357265472\n",
      "  timers:\n",
      "    learn_throughput: 20.009\n",
      "    learn_time_ms: 199907.079\n",
      "    load_throughput: 6841978.712\n",
      "    load_time_ms: 0.585\n",
      "    sample_throughput: 18.502\n",
      "    sample_time_ms: 216192.648\n",
      "    update_time_ms: 14.007\n",
      "  timestamp: 1650245725\n",
      "  timesteps_since_restore: 508000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-35-38\n",
      "  done: false\n",
      "  episode_len_mean: 391.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.04\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1849\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9386860728263855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008493351750075817\n",
      "          model: {}\n",
      "          policy_loss: -0.038643479347229004\n",
      "          total_loss: 0.012172909453511238\n",
      "          vf_explained_var: 0.7828922867774963\n",
      "          vf_loss: 0.05081638693809509\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.07172413793104\n",
      "    ram_util_percent: 95.4006896551724\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1139063865127127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.049859726441893\n",
      "    mean_inference_ms: 2.4243950982554425\n",
      "    mean_raw_obs_processing_ms: 0.22994946975225347\n",
      "  time_since_restore: 27395.151235580444\n",
      "  time_this_iter_s: 216.36597990989685\n",
      "  time_total_s: 27395.151235580444\n",
      "  timers:\n",
      "    learn_throughput: 20.049\n",
      "    learn_time_ms: 199513.833\n",
      "    load_throughput: 5521727.225\n",
      "    load_time_ms: 0.724\n",
      "    sample_throughput: 18.531\n",
      "    sample_time_ms: 215859.256\n",
      "    update_time_ms: 5.969\n",
      "  timestamp: 1650245738\n",
      "  timesteps_since_restore: 508000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-39-05\n",
      "  done: false\n",
      "  episode_len_mean: 2113.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 262\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.78342905951606e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.758323249165417e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.003994566388428211\n",
      "          total_loss: -0.0037929427344352007\n",
      "          vf_explained_var: -0.09444820135831833\n",
      "          vf_loss: 0.0002016273938352242\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.93197278911565\n",
      "    ram_util_percent: 95.55646258503401\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11148687314915622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0123345949461573\n",
      "    mean_inference_ms: 2.347822695649031\n",
      "    mean_raw_obs_processing_ms: 0.13831855459092948\n",
      "  time_since_restore: 27597.505384206772\n",
      "  time_this_iter_s: 219.62502694129944\n",
      "  time_total_s: 27597.505384206772\n",
      "  timers:\n",
      "    learn_throughput: 19.939\n",
      "    learn_time_ms: 200615.221\n",
      "    load_throughput: 3616246.928\n",
      "    load_time_ms: 1.106\n",
      "    sample_throughput: 18.488\n",
      "    sample_time_ms: 216358.36\n",
      "    update_time_ms: 14.51\n",
      "  timestamp: 1650245945\n",
      "  timesteps_since_restore: 512000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-39-17\n",
      "  done: false\n",
      "  episode_len_mean: 398.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.17\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1859\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8658730983734131\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00944297295063734\n",
      "          model: {}\n",
      "          policy_loss: -0.04823281243443489\n",
      "          total_loss: 0.02072465606033802\n",
      "          vf_explained_var: 0.7637186646461487\n",
      "          vf_loss: 0.06895747035741806\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.11972789115646\n",
      "    ram_util_percent: 95.53571428571429\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11394057189402464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.050220709767009\n",
      "    mean_inference_ms: 2.4255450657753204\n",
      "    mean_raw_obs_processing_ms: 0.22983913629338926\n",
      "  time_since_restore: 27614.263556718826\n",
      "  time_this_iter_s: 219.11232113838196\n",
      "  time_total_s: 27614.263556718826\n",
      "  timers:\n",
      "    learn_throughput: 19.985\n",
      "    learn_time_ms: 200150.945\n",
      "    load_throughput: 5540692.206\n",
      "    load_time_ms: 0.722\n",
      "    sample_throughput: 18.519\n",
      "    sample_time_ms: 216000.088\n",
      "    update_time_ms: 6.155\n",
      "  timestamp: 1650245957\n",
      "  timesteps_since_restore: 512000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-42-44\n",
      "  done: false\n",
      "  episode_len_mean: 2113.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 262\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128073118627071\n",
      "          total_loss: -0.014109787531197071\n",
      "          vf_explained_var: -2.2431856372406855e-09\n",
      "          vf_loss: 1.828837048378773e-05\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.69692832764505\n",
      "    ram_util_percent: 95.29692832764505\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11148687314915622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0123345949461573\n",
      "    mean_inference_ms: 2.347822695649031\n",
      "    mean_raw_obs_processing_ms: 0.13831855459092948\n",
      "  time_since_restore: 27815.79873752594\n",
      "  time_this_iter_s: 218.2933533191681\n",
      "  time_total_s: 27815.79873752594\n",
      "  timers:\n",
      "    learn_throughput: 19.883\n",
      "    learn_time_ms: 201181.364\n",
      "    load_throughput: 3676633.941\n",
      "    load_time_ms: 1.088\n",
      "    sample_throughput: 18.444\n",
      "    sample_time_ms: 216874.96\n",
      "    update_time_ms: 14.537\n",
      "  timestamp: 1650246164\n",
      "  timesteps_since_restore: 516000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-42-55\n",
      "  done: false\n",
      "  episode_len_mean: 398.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1869\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8561286926269531\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010399912483990192\n",
      "          model: {}\n",
      "          policy_loss: -0.04833933338522911\n",
      "          total_loss: 0.012675395235419273\n",
      "          vf_explained_var: 0.7402440309524536\n",
      "          vf_loss: 0.06101473048329353\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.51678082191782\n",
      "    ram_util_percent: 95.30650684931506\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1139666968621153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0505787304673848\n",
      "    mean_inference_ms: 2.4263623175248883\n",
      "    mean_raw_obs_processing_ms: 0.22970101095983064\n",
      "  time_since_restore: 27832.938057661057\n",
      "  time_this_iter_s: 218.67450094223022\n",
      "  time_total_s: 27832.938057661057\n",
      "  timers:\n",
      "    learn_throughput: 19.923\n",
      "    learn_time_ms: 200774.903\n",
      "    load_throughput: 6047151.096\n",
      "    load_time_ms: 0.661\n",
      "    sample_throughput: 18.482\n",
      "    sample_time_ms: 216422.104\n",
      "    update_time_ms: 5.869\n",
      "  timestamp: 1650246175\n",
      "  timesteps_since_restore: 516000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 02:45:33 (running for 07:46:53.00)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=4.23 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-46-22\n",
      "  done: false\n",
      "  episode_len_mean: 2210.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 264\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.750905610300182e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.601583132172317e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.0031648173462599516\n",
      "          total_loss: -0.0031086024828255177\n",
      "          vf_explained_var: -0.05115101486444473\n",
      "          vf_loss: 5.6203669373644516e-05\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.88259385665529\n",
      "    ram_util_percent: 95.2061433447099\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11164580494978102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0133434772655254\n",
      "    mean_inference_ms: 2.352245353593618\n",
      "    mean_raw_obs_processing_ms: 0.13845415192688956\n",
      "  time_since_restore: 28034.480657339096\n",
      "  time_this_iter_s: 218.68191981315613\n",
      "  time_total_s: 28034.480657339096\n",
      "  timers:\n",
      "    learn_throughput: 19.841\n",
      "    learn_time_ms: 201601.05\n",
      "    load_throughput: 3699659.522\n",
      "    load_time_ms: 1.081\n",
      "    sample_throughput: 18.413\n",
      "    sample_time_ms: 217235.456\n",
      "    update_time_ms: 17.388\n",
      "  timestamp: 1650246382\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-46-34\n",
      "  done: false\n",
      "  episode_len_mean: 403.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.33\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1880\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8774133324623108\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009309838525950909\n",
      "          model: {}\n",
      "          policy_loss: -0.049176957458257675\n",
      "          total_loss: 0.01569759100675583\n",
      "          vf_explained_var: 0.715217649936676\n",
      "          vf_loss: 0.0648745447397232\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.07713310580206\n",
      "    ram_util_percent: 95.24880546075086\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11398840300871477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0508901328218236\n",
      "    mean_inference_ms: 2.4271068905079902\n",
      "    mean_raw_obs_processing_ms: 0.22951954827264542\n",
      "  time_since_restore: 28051.731164693832\n",
      "  time_this_iter_s: 218.79310703277588\n",
      "  time_total_s: 28051.731164693832\n",
      "  timers:\n",
      "    learn_throughput: 19.867\n",
      "    learn_time_ms: 201337.453\n",
      "    load_throughput: 6150456.778\n",
      "    load_time_ms: 0.65\n",
      "    sample_throughput: 18.446\n",
      "    sample_time_ms: 216843.829\n",
      "    update_time_ms: 5.928\n",
      "  timestamp: 1650246394\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 2210.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 264\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.012681319378316402\n",
      "          total_loss: -0.012681285850703716\n",
      "          vf_explained_var: -7.666567398700863e-07\n",
      "          vf_loss: 3.897764599969378e-08\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.54089347079038\n",
      "    ram_util_percent: 95.34192439862542\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11164580494978102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0133434772655254\n",
      "    mean_inference_ms: 2.352245353593618\n",
      "    mean_raw_obs_processing_ms: 0.13845415192688956\n",
      "  time_since_restore: 28251.24753022194\n",
      "  time_this_iter_s: 216.76687288284302\n",
      "  time_total_s: 28251.24753022194\n",
      "  timers:\n",
      "    learn_throughput: 19.824\n",
      "    learn_time_ms: 201771.576\n",
      "    load_throughput: 3693632.161\n",
      "    load_time_ms: 1.083\n",
      "    sample_throughput: 18.389\n",
      "    sample_time_ms: 217522.877\n",
      "    update_time_ms: 17.424\n",
      "  timestamp: 1650246599\n",
      "  timesteps_since_restore: 524000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-50-13\n",
      "  done: false\n",
      "  episode_len_mean: 401.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.29\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1891\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.9190625548362732\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009006948210299015\n",
      "          model: {}\n",
      "          policy_loss: -0.04242798686027527\n",
      "          total_loss: 0.01594780571758747\n",
      "          vf_explained_var: 0.7316357493400574\n",
      "          vf_loss: 0.05837578698992729\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.42054794520548\n",
      "    ram_util_percent: 95.33424657534246\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11400680735916847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0511425956819784\n",
      "    mean_inference_ms: 2.427837375252985\n",
      "    mean_raw_obs_processing_ms: 0.22932284506279108\n",
      "  time_since_restore: 28269.918043613434\n",
      "  time_this_iter_s: 218.18687891960144\n",
      "  time_total_s: 28269.918043613434\n",
      "  timers:\n",
      "    learn_throughput: 19.839\n",
      "    learn_time_ms: 201619.436\n",
      "    load_throughput: 6061132.948\n",
      "    load_time_ms: 0.66\n",
      "    sample_throughput: 18.403\n",
      "    sample_time_ms: 217357.513\n",
      "    update_time_ms: 5.833\n",
      "  timestamp: 1650246613\n",
      "  timesteps_since_restore: 524000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 2210.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 264\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.005815621931105852\n",
      "          total_loss: -0.005815621931105852\n",
      "          vf_explained_var: -8.968896509031765e-06\n",
      "          vf_loss: 2.6569944022725167e-09\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.49658703071673\n",
      "    ram_util_percent: 95.38430034129692\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11164580494978102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0133434772655254\n",
      "    mean_inference_ms: 2.352245353593618\n",
      "    mean_raw_obs_processing_ms: 0.13845415192688956\n",
      "  time_since_restore: 28469.259261608124\n",
      "  time_this_iter_s: 218.0117313861847\n",
      "  time_total_s: 28469.259261608124\n",
      "  timers:\n",
      "    learn_throughput: 19.799\n",
      "    learn_time_ms: 202034.387\n",
      "    load_throughput: 3705788.439\n",
      "    load_time_ms: 1.079\n",
      "    sample_throughput: 18.377\n",
      "    sample_time_ms: 217665.573\n",
      "    update_time_ms: 18.365\n",
      "  timestamp: 1650246817\n",
      "  timesteps_since_restore: 528000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-53-51\n",
      "  done: false\n",
      "  episode_len_mean: 405.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.38\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1900\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8775073885917664\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009797288104891777\n",
      "          model: {}\n",
      "          policy_loss: -0.045760542154312134\n",
      "          total_loss: 0.0073656318709254265\n",
      "          vf_explained_var: 0.7947784066200256\n",
      "          vf_loss: 0.053126174956560135\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.2704081632653\n",
      "    ram_util_percent: 95.4017006802721\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11401952237582598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0513498020825123\n",
      "    mean_inference_ms: 2.428468160508024\n",
      "    mean_raw_obs_processing_ms: 0.22915614427113895\n",
      "  time_since_restore: 28488.358130693436\n",
      "  time_this_iter_s: 218.44008708000183\n",
      "  time_total_s: 28488.358130693436\n",
      "  timers:\n",
      "    learn_throughput: 19.807\n",
      "    learn_time_ms: 201946.951\n",
      "    load_throughput: 5815729.34\n",
      "    load_time_ms: 0.688\n",
      "    sample_throughput: 18.378\n",
      "    sample_time_ms: 217652.532\n",
      "    update_time_ms: 6.088\n",
      "  timestamp: 1650246831\n",
      "  timesteps_since_restore: 528000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-57-16\n",
      "  done: false\n",
      "  episode_len_mean: 2307.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 266\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0778887164601935e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.1812017896056576e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.001402854803018272\n",
      "          total_loss: -0.0012160507030785084\n",
      "          vf_explained_var: 0.000555283622816205\n",
      "          vf_loss: 0.00018679973436519504\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.9486394557823\n",
      "    ram_util_percent: 95.44081632653061\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11180600779405825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.014374213700664\n",
      "    mean_inference_ms: 2.3567411392835154\n",
      "    mean_raw_obs_processing_ms: 0.1385887726281182\n",
      "  time_since_restore: 28688.266801595688\n",
      "  time_this_iter_s: 219.0075399875641\n",
      "  time_total_s: 28688.266801595688\n",
      "  timers:\n",
      "    learn_throughput: 19.786\n",
      "    learn_time_ms: 202163.504\n",
      "    load_throughput: 3666269.531\n",
      "    load_time_ms: 1.091\n",
      "    sample_throughput: 18.349\n",
      "    sample_time_ms: 217996.077\n",
      "    update_time_ms: 18.182\n",
      "  timestamp: 1650247036\n",
      "  timesteps_since_restore: 532000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_02-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 414.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 4.59\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1909\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8920957446098328\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00980965793132782\n",
      "          model: {}\n",
      "          policy_loss: -0.04129201918840408\n",
      "          total_loss: 0.010145754553377628\n",
      "          vf_explained_var: 0.7437366843223572\n",
      "          vf_loss: 0.05143777281045914\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.61262798634812\n",
      "    ram_util_percent: 95.42389078498293\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11402971579250132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0515590635568994\n",
      "    mean_inference_ms: 2.4291263566490886\n",
      "    mean_raw_obs_processing_ms: 0.22898353499318896\n",
      "  time_since_restore: 28706.587458610535\n",
      "  time_this_iter_s: 218.229327917099\n",
      "  time_total_s: 28706.587458610535\n",
      "  timers:\n",
      "    learn_throughput: 19.8\n",
      "    learn_time_ms: 202022.228\n",
      "    load_throughput: 5826838.468\n",
      "    load_time_ms: 0.686\n",
      "    sample_throughput: 18.349\n",
      "    sample_time_ms: 217993.621\n",
      "    update_time_ms: 6.056\n",
      "  timestamp: 1650247049\n",
      "  timesteps_since_restore: 532000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-00-55\n",
      "  done: false\n",
      "  episode_len_mean: 2307.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 266\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128251932561398\n",
      "          total_loss: 0.014128588140010834\n",
      "          vf_explained_var: 8.602295906712243e-07\n",
      "          vf_loss: 3.245736195367499e-07\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.94402730375427\n",
      "    ram_util_percent: 95.51877133105802\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11180600779405825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.014374213700664\n",
      "    mean_inference_ms: 2.3567411392835154\n",
      "    mean_raw_obs_processing_ms: 0.1385887726281182\n",
      "  time_since_restore: 28907.104558467865\n",
      "  time_this_iter_s: 218.83775687217712\n",
      "  time_total_s: 28907.104558467865\n",
      "  timers:\n",
      "    learn_throughput: 19.774\n",
      "    learn_time_ms: 202287.871\n",
      "    load_throughput: 3715390.203\n",
      "    load_time_ms: 1.077\n",
      "    sample_throughput: 18.339\n",
      "    sample_time_ms: 218114.803\n",
      "    update_time_ms: 17.453\n",
      "  timestamp: 1650247255\n",
      "  timesteps_since_restore: 536000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-01-08\n",
      "  done: false\n",
      "  episode_len_mean: 419.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 4.66\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1920\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8508575558662415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0104170897975564\n",
      "          model: {}\n",
      "          policy_loss: -0.044904209673404694\n",
      "          total_loss: 0.01170093473047018\n",
      "          vf_explained_var: 0.7809923887252808\n",
      "          vf_loss: 0.05660514533519745\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.46518771331058\n",
      "    ram_util_percent: 95.52798634812287\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11403672966816071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0518207771689665\n",
      "    mean_inference_ms: 2.4300487274201044\n",
      "    mean_raw_obs_processing_ms: 0.22875913958069996\n",
      "  time_since_restore: 28925.506731510162\n",
      "  time_this_iter_s: 218.91927289962769\n",
      "  time_total_s: 28925.506731510162\n",
      "  timers:\n",
      "    learn_throughput: 19.792\n",
      "    learn_time_ms: 202102.433\n",
      "    load_throughput: 5548938.647\n",
      "    load_time_ms: 0.721\n",
      "    sample_throughput: 18.339\n",
      "    sample_time_ms: 218113.877\n",
      "    update_time_ms: 5.986\n",
      "  timestamp: 1650247268\n",
      "  timesteps_since_restore: 536000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 03:02:13 (running for 08:03:33.18)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=4.66 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-04-35\n",
      "  done: false\n",
      "  episode_len_mean: 2404.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 267\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1482691203853264e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.276584317510611e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0006125111831352115\n",
      "          total_loss: 0.0006231029983609915\n",
      "          vf_explained_var: -0.032258540391922\n",
      "          vf_loss: 1.0593347724352498e-05\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.94709897610922\n",
      "    ram_util_percent: 95.73993174061434\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1118862026613806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.01489637479688\n",
      "    mean_inference_ms: 2.3590080572562275\n",
      "    mean_raw_obs_processing_ms: 0.13865518488238662\n",
      "  time_since_restore: 29126.85984158516\n",
      "  time_this_iter_s: 219.7552831172943\n",
      "  time_total_s: 29126.85984158516\n",
      "  timers:\n",
      "    learn_throughput: 19.739\n",
      "    learn_time_ms: 202646.036\n",
      "    load_throughput: 3698517.702\n",
      "    load_time_ms: 1.082\n",
      "    sample_throughput: 18.33\n",
      "    sample_time_ms: 218226.206\n",
      "    update_time_ms: 17.679\n",
      "  timestamp: 1650247475\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-04-48\n",
      "  done: false\n",
      "  episode_len_mean: 426.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 4.75\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1929\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8257502913475037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01033820305019617\n",
      "          model: {}\n",
      "          policy_loss: -0.05206618085503578\n",
      "          total_loss: 0.031499236822128296\n",
      "          vf_explained_var: 0.7164541482925415\n",
      "          vf_loss: 0.08356541395187378\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.63163265306125\n",
      "    ram_util_percent: 95.73843537414967\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11404032399543264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0520649877145205\n",
      "    mean_inference_ms: 2.430868042221074\n",
      "    mean_raw_obs_processing_ms: 0.22857019498183587\n",
      "  time_since_restore: 29145.140607357025\n",
      "  time_this_iter_s: 219.6338758468628\n",
      "  time_total_s: 29145.140607357025\n",
      "  timers:\n",
      "    learn_throughput: 19.764\n",
      "    learn_time_ms: 202391.708\n",
      "    load_throughput: 5075392.062\n",
      "    load_time_ms: 0.788\n",
      "    sample_throughput: 18.327\n",
      "    sample_time_ms: 218255.091\n",
      "    update_time_ms: 6.005\n",
      "  timestamp: 1650247488\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-08-13\n",
      "  done: false\n",
      "  episode_len_mean: 2404.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 267\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128102920949459\n",
      "          total_loss: -0.014127721078693867\n",
      "          vf_explained_var: 1.9188850330920104e-07\n",
      "          vf_loss: 3.831526669273444e-07\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.88150684931507\n",
      "    ram_util_percent: 95.58527397260275\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1118862026613806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.01489637479688\n",
      "    mean_inference_ms: 2.3590080572562275\n",
      "    mean_raw_obs_processing_ms: 0.13865518488238662\n",
      "  time_since_restore: 29344.720624685287\n",
      "  time_this_iter_s: 217.86078310012817\n",
      "  time_total_s: 29344.720624685287\n",
      "  timers:\n",
      "    learn_throughput: 19.747\n",
      "    learn_time_ms: 202563.064\n",
      "    load_throughput: 3715966.245\n",
      "    load_time_ms: 1.076\n",
      "    sample_throughput: 18.303\n",
      "    sample_time_ms: 218540.257\n",
      "    update_time_ms: 15.292\n",
      "  timestamp: 1650247693\n",
      "  timesteps_since_restore: 544000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-08-27\n",
      "  done: false\n",
      "  episode_len_mean: 428.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 4.79\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1938\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8809138536453247\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009304733946919441\n",
      "          model: {}\n",
      "          policy_loss: -0.04180477559566498\n",
      "          total_loss: 0.008699013851583004\n",
      "          vf_explained_var: 0.769045352935791\n",
      "          vf_loss: 0.050503794103860855\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.14353741496599\n",
      "    ram_util_percent: 95.57244897959184\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11404741174311983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.052306896541574\n",
      "    mean_inference_ms: 2.4317094231927725\n",
      "    mean_raw_obs_processing_ms: 0.22836927180351096\n",
      "  time_since_restore: 29363.806816339493\n",
      "  time_this_iter_s: 218.66620898246765\n",
      "  time_total_s: 29363.806816339493\n",
      "  timers:\n",
      "    learn_throughput: 19.763\n",
      "    learn_time_ms: 202398.702\n",
      "    load_throughput: 5613925.381\n",
      "    load_time_ms: 0.713\n",
      "    sample_throughput: 18.303\n",
      "    sample_time_ms: 218548.123\n",
      "    update_time_ms: 5.991\n",
      "  timestamp: 1650247707\n",
      "  timesteps_since_restore: 544000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-11-53\n",
      "  done: false\n",
      "  episode_len_mean: 2404.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 267\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128067530691624\n",
      "          total_loss: -0.014127940870821476\n",
      "          vf_explained_var: 7.899858474047505e-07\n",
      "          vf_loss: 1.260156352600461e-07\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.14421768707483\n",
      "    ram_util_percent: 95.49149659863944\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1118862026613806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.01489637479688\n",
      "    mean_inference_ms: 2.3590080572562275\n",
      "    mean_raw_obs_processing_ms: 0.13865518488238662\n",
      "  time_since_restore: 29564.521208524704\n",
      "  time_this_iter_s: 219.8005838394165\n",
      "  time_total_s: 29564.521208524704\n",
      "  timers:\n",
      "    learn_throughput: 19.725\n",
      "    learn_time_ms: 202789.621\n",
      "    load_throughput: 3727027.88\n",
      "    load_time_ms: 1.073\n",
      "    sample_throughput: 18.305\n",
      "    sample_time_ms: 218520.375\n",
      "    update_time_ms: 14.81\n",
      "  timestamp: 1650247913\n",
      "  timesteps_since_restore: 548000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-12-06\n",
      "  done: false\n",
      "  episode_len_mean: 431.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 4.86\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1949\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8984153270721436\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009205637499690056\n",
      "          model: {}\n",
      "          policy_loss: -0.04969795048236847\n",
      "          total_loss: 0.012313641607761383\n",
      "          vf_explained_var: 0.7461780905723572\n",
      "          vf_loss: 0.062011588364839554\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.75870307167237\n",
      "    ram_util_percent: 95.48703071672354\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11405548640979178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0526136351637307\n",
      "    mean_inference_ms: 2.4328075277330483\n",
      "    mean_raw_obs_processing_ms: 0.22812116080114975\n",
      "  time_since_restore: 29582.818767547607\n",
      "  time_this_iter_s: 219.01195120811462\n",
      "  time_total_s: 29582.818767547607\n",
      "  timers:\n",
      "    learn_throughput: 19.741\n",
      "    learn_time_ms: 202624.409\n",
      "    load_throughput: 5762791.88\n",
      "    load_time_ms: 0.694\n",
      "    sample_throughput: 18.299\n",
      "    sample_time_ms: 218586.557\n",
      "    update_time_ms: 5.937\n",
      "  timestamp: 1650247926\n",
      "  timesteps_since_restore: 548000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-15-32\n",
      "  done: false\n",
      "  episode_len_mean: 2309.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 273\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.818929263801469e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8127120340834934e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.00020632320956792682\n",
      "          total_loss: 0.0003520151658449322\n",
      "          vf_explained_var: 0.2556943893432617\n",
      "          vf_loss: 0.0005583446472883224\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.47074829931972\n",
      "    ram_util_percent: 95.38469387755102\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1123422985749987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0178134401212222\n",
      "    mean_inference_ms: 2.371950820852274\n",
      "    mean_raw_obs_processing_ms: 0.13903587847354462\n",
      "  time_since_restore: 29783.31826353073\n",
      "  time_this_iter_s: 218.79705500602722\n",
      "  time_total_s: 29783.31826353073\n",
      "  timers:\n",
      "    learn_throughput: 19.737\n",
      "    learn_time_ms: 202663.47\n",
      "    load_throughput: 7194963.547\n",
      "    load_time_ms: 0.556\n",
      "    sample_throughput: 18.283\n",
      "    sample_time_ms: 218781.929\n",
      "    update_time_ms: 17.629\n",
      "  timestamp: 1650248132\n",
      "  timesteps_since_restore: 552000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-15-45\n",
      "  done: false\n",
      "  episode_len_mean: 435.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 4.97\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1957\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8331270217895508\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013896448537707329\n",
      "          model: {}\n",
      "          policy_loss: -0.04085984081029892\n",
      "          total_loss: 0.050806861370801926\n",
      "          vf_explained_var: 0.6955642700195312\n",
      "          vf_loss: 0.09166670590639114\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.37823129251701\n",
      "    ram_util_percent: 95.40340136054421\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11406091356294386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0528212416330442\n",
      "    mean_inference_ms: 2.4335765919607577\n",
      "    mean_raw_obs_processing_ms: 0.22793310333152925\n",
      "  time_since_restore: 29802.51276254654\n",
      "  time_this_iter_s: 219.69399499893188\n",
      "  time_total_s: 29802.51276254654\n",
      "  timers:\n",
      "    learn_throughput: 19.732\n",
      "    learn_time_ms: 202712.805\n",
      "    load_throughput: 5766357.106\n",
      "    load_time_ms: 0.694\n",
      "    sample_throughput: 18.283\n",
      "    sample_time_ms: 218782.571\n",
      "    update_time_ms: 5.919\n",
      "  timestamp: 1650248145\n",
      "  timesteps_since_restore: 552000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 03:18:53 (running for 08:20:13.54)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=4.97 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-19-21\n",
      "  done: false\n",
      "  episode_len_mean: 2309.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 273\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128225855529308\n",
      "          total_loss: -0.014127031899988651\n",
      "          vf_explained_var: -2.6085043458579094e-08\n",
      "          vf_loss: 1.191392698274285e-06\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.34803921568627\n",
      "    ram_util_percent: 95.90751633986929\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1123422985749987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0178134401212222\n",
      "    mean_inference_ms: 2.371950820852274\n",
      "    mean_raw_obs_processing_ms: 0.13903587847354462\n",
      "  time_since_restore: 30012.552998542786\n",
      "  time_this_iter_s: 229.23473501205444\n",
      "  time_total_s: 30012.552998542786\n",
      "  timers:\n",
      "    learn_throughput: 19.638\n",
      "    learn_time_ms: 203685.132\n",
      "    load_throughput: 6883525.212\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 18.288\n",
      "    sample_time_ms: 218722.366\n",
      "    update_time_ms: 19.469\n",
      "  timestamp: 1650248361\n",
      "  timesteps_since_restore: 556000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-19-35\n",
      "  done: false\n",
      "  episode_len_mean: 437.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 4.95\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1968\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8612520098686218\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008966121822595596\n",
      "          model: {}\n",
      "          policy_loss: -0.03304389491677284\n",
      "          total_loss: 0.03716275095939636\n",
      "          vf_explained_var: 0.6304736137390137\n",
      "          vf_loss: 0.0702066496014595\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.03934426229509\n",
      "    ram_util_percent: 95.91508196721313\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1140782932058394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.053117486099766\n",
      "    mean_inference_ms: 2.434896966110079\n",
      "    mean_raw_obs_processing_ms: 0.22770845999856312\n",
      "  time_since_restore: 30031.856155633926\n",
      "  time_this_iter_s: 229.34339308738708\n",
      "  time_total_s: 30031.856155633926\n",
      "  timers:\n",
      "    learn_throughput: 19.645\n",
      "    learn_time_ms: 203613.402\n",
      "    load_throughput: 5909759.414\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 18.262\n",
      "    sample_time_ms: 219037.594\n",
      "    update_time_ms: 6.387\n",
      "  timestamp: 1650248375\n",
      "  timesteps_since_restore: 556000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-23-10\n",
      "  done: false\n",
      "  episode_len_mean: 2309.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 273\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.01412805262953043\n",
      "          total_loss: -0.014127825386822224\n",
      "          vf_explained_var: -8.331831935493028e-08\n",
      "          vf_loss: 2.305020245785272e-07\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.78065573770492\n",
      "    ram_util_percent: 95.83639344262296\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1123422985749987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0178134401212222\n",
      "    mean_inference_ms: 2.371950820852274\n",
      "    mean_raw_obs_processing_ms: 0.13903587847354462\n",
      "  time_since_restore: 30241.013374328613\n",
      "  time_this_iter_s: 228.46037578582764\n",
      "  time_total_s: 30241.013374328613\n",
      "  timers:\n",
      "    learn_throughput: 19.556\n",
      "    learn_time_ms: 204544.308\n",
      "    load_throughput: 6791570.255\n",
      "    load_time_ms: 0.589\n",
      "    sample_throughput: 18.192\n",
      "    sample_time_ms: 219879.452\n",
      "    update_time_ms: 16.639\n",
      "  timestamp: 1650248590\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-23-24\n",
      "  done: false\n",
      "  episode_len_mean: 436.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 4.91\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1978\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8345966935157776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009412402287125587\n",
      "          model: {}\n",
      "          policy_loss: -0.04296687990427017\n",
      "          total_loss: 0.02797451987862587\n",
      "          vf_explained_var: 0.6908507347106934\n",
      "          vf_loss: 0.07094141095876694\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.44527687296416\n",
      "    ram_util_percent: 95.83583061889252\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11410066692660212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.053430575698995\n",
      "    mean_inference_ms: 2.436288889549825\n",
      "    mean_raw_obs_processing_ms: 0.2275459052570263\n",
      "  time_since_restore: 30260.60244035721\n",
      "  time_this_iter_s: 228.74628472328186\n",
      "  time_total_s: 30260.60244035721\n",
      "  timers:\n",
      "    learn_throughput: 19.566\n",
      "    learn_time_ms: 204439.305\n",
      "    load_throughput: 5747590.271\n",
      "    load_time_ms: 0.696\n",
      "    sample_throughput: 18.173\n",
      "    sample_time_ms: 220107.474\n",
      "    update_time_ms: 6.639\n",
      "  timestamp: 1650248604\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-26-56\n",
      "  done: false\n",
      "  episode_len_mean: 2406.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 282\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1141683411388864e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1271612081231206e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0017797105247154832\n",
      "          total_loss: -0.0001780989987310022\n",
      "          vf_explained_var: 0.05761764198541641\n",
      "          vf_loss: 0.0016016052104532719\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.42046204620462\n",
      "    ram_util_percent: 95.67392739273929\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1130166150870247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0220803694126384\n",
      "    mean_inference_ms: 2.3909741742909616\n",
      "    mean_raw_obs_processing_ms: 0.13959026720735174\n",
      "  time_since_restore: 30467.194719076157\n",
      "  time_this_iter_s: 226.18134474754333\n",
      "  time_total_s: 30467.194719076157\n",
      "  timers:\n",
      "    learn_throughput: 19.481\n",
      "    learn_time_ms: 205324.029\n",
      "    load_throughput: 6531912.011\n",
      "    load_time_ms: 0.612\n",
      "    sample_throughput: 18.108\n",
      "    sample_time_ms: 220899.022\n",
      "    update_time_ms: 16.799\n",
      "  timestamp: 1650248816\n",
      "  timesteps_since_restore: 564000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 445.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 5.12\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1987\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.820019006729126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010589221492409706\n",
      "          model: {}\n",
      "          policy_loss: -0.03816605359315872\n",
      "          total_loss: 0.03658660128712654\n",
      "          vf_explained_var: 0.7587774991989136\n",
      "          vf_loss: 0.07475265860557556\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.15894039735099\n",
      "    ram_util_percent: 95.66258278145698\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11413640229028615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0538565942368323\n",
      "    mean_inference_ms: 2.4378174281133083\n",
      "    mean_raw_obs_processing_ms: 0.2274200093753348\n",
      "  time_since_restore: 30486.2936835289\n",
      "  time_this_iter_s: 225.6912431716919\n",
      "  time_total_s: 30486.2936835289\n",
      "  timers:\n",
      "    learn_throughput: 19.522\n",
      "    learn_time_ms: 204902.27\n",
      "    load_throughput: 5778472.136\n",
      "    load_time_ms: 0.692\n",
      "    sample_throughput: 18.081\n",
      "    sample_time_ms: 221220.916\n",
      "    update_time_ms: 6.711\n",
      "  timestamp: 1650248829\n",
      "  timesteps_since_restore: 564000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-30-33\n",
      "  done: false\n",
      "  episode_len_mean: 2406.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 282\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128157868981361\n",
      "          total_loss: -0.01412476971745491\n",
      "          vf_explained_var: -2.4143085397554387e-07\n",
      "          vf_loss: 3.39395819537458e-06\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.03793103448275\n",
      "    ram_util_percent: 95.75862068965516\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1130166150870247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0220803694126384\n",
      "    mean_inference_ms: 2.3909741742909616\n",
      "    mean_raw_obs_processing_ms: 0.13959026720735174\n",
      "  time_since_restore: 30684.396234035492\n",
      "  time_this_iter_s: 217.20151495933533\n",
      "  time_total_s: 30684.396234035492\n",
      "  timers:\n",
      "    learn_throughput: 19.491\n",
      "    learn_time_ms: 205227.304\n",
      "    load_throughput: 6591967.31\n",
      "    load_time_ms: 0.607\n",
      "    sample_throughput: 18.043\n",
      "    sample_time_ms: 221696.799\n",
      "    update_time_ms: 16.366\n",
      "  timestamp: 1650249033\n",
      "  timesteps_since_restore: 568000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 448.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 5.17\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1996\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8043063282966614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009247783571481705\n",
      "          model: {}\n",
      "          policy_loss: -0.042454205453395844\n",
      "          total_loss: 0.007652313448488712\n",
      "          vf_explained_var: 0.7812513113021851\n",
      "          vf_loss: 0.050106510519981384\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.6693103448276\n",
      "    ram_util_percent: 95.76206896551723\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11417572230189356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.054300224288094\n",
      "    mean_inference_ms: 2.43941140037303\n",
      "    mean_raw_obs_processing_ms: 0.22729252314936915\n",
      "  time_since_restore: 30704.004710674286\n",
      "  time_this_iter_s: 217.71102714538574\n",
      "  time_total_s: 30704.004710674286\n",
      "  timers:\n",
      "    learn_throughput: 19.532\n",
      "    learn_time_ms: 204786.935\n",
      "    load_throughput: 5927925.942\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 18.04\n",
      "    sample_time_ms: 221725.916\n",
      "    update_time_ms: 6.568\n",
      "  timestamp: 1650249047\n",
      "  timesteps_since_restore: 568000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 2406.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 282\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128263108432293\n",
      "          total_loss: -0.01412777230143547\n",
      "          vf_explained_var: 5.138817869010381e-07\n",
      "          vf_loss: 4.874631827078701e-07\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.22508591065294\n",
      "    ram_util_percent: 95.86254295532645\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1130166150870247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0220803694126384\n",
      "    mean_inference_ms: 2.3909741742909616\n",
      "    mean_raw_obs_processing_ms: 0.13959026720735174\n",
      "  time_since_restore: 30903.455060005188\n",
      "  time_this_iter_s: 219.05882596969604\n",
      "  time_total_s: 30903.455060005188\n",
      "  timers:\n",
      "    learn_throughput: 19.49\n",
      "    learn_time_ms: 205234.102\n",
      "    load_throughput: 6703914.329\n",
      "    load_time_ms: 0.597\n",
      "    sample_throughput: 18.052\n",
      "    sample_time_ms: 221586.531\n",
      "    update_time_ms: 20.004\n",
      "  timestamp: 1650249252\n",
      "  timesteps_since_restore: 572000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-34-25\n",
      "  done: false\n",
      "  episode_len_mean: 439.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 4.96\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2007\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8375749588012695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009457825683057308\n",
      "          model: {}\n",
      "          policy_loss: -0.04138529673218727\n",
      "          total_loss: 0.039357006549835205\n",
      "          vf_explained_var: 0.6992084383964539\n",
      "          vf_loss: 0.08074230700731277\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.14636678200694\n",
      "    ram_util_percent: 95.8674740484429\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11422944415787793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0548640287633433\n",
      "    mean_inference_ms: 2.4412822176464477\n",
      "    mean_raw_obs_processing_ms: 0.2271674628192268\n",
      "  time_since_restore: 30922.14915561676\n",
      "  time_this_iter_s: 218.14444494247437\n",
      "  time_total_s: 30922.14915561676\n",
      "  timers:\n",
      "    learn_throughput: 19.535\n",
      "    learn_time_ms: 204765.162\n",
      "    load_throughput: 5954434.98\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 18.049\n",
      "    sample_time_ms: 221622.894\n",
      "    update_time_ms: 6.647\n",
      "  timestamp: 1650249265\n",
      "  timesteps_since_restore: 572000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 03:35:34 (running for 08:36:54.39)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=4.96 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-37-49\n",
      "  done: false\n",
      "  episode_len_mean: 2404.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 286\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.461962055393624e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4156642041599663e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0011587160406634212\n",
      "          total_loss: 0.0012683814857155085\n",
      "          vf_explained_var: -0.032360002398490906\n",
      "          vf_loss: 0.00010966480476781726\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.00311418685122\n",
      "    ram_util_percent: 95.74256055363323\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11330901341101246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0238882980497745\n",
      "    mean_inference_ms: 2.3990594321869354\n",
      "    mean_raw_obs_processing_ms: 0.13980876490768002\n",
      "  time_since_restore: 31119.775759220123\n",
      "  time_this_iter_s: 216.3206992149353\n",
      "  time_total_s: 31119.775759220123\n",
      "  timers:\n",
      "    learn_throughput: 19.516\n",
      "    learn_time_ms: 204962.391\n",
      "    load_throughput: 6493735.872\n",
      "    load_time_ms: 0.616\n",
      "    sample_throughput: 18.05\n",
      "    sample_time_ms: 221612.544\n",
      "    update_time_ms: 20.695\n",
      "  timestamp: 1650249469\n",
      "  timesteps_since_restore: 576000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-38-02\n",
      "  done: false\n",
      "  episode_len_mean: 441.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 5.07\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2016\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8299276828765869\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010946226306259632\n",
      "          model: {}\n",
      "          policy_loss: -0.048696234822273254\n",
      "          total_loss: 0.02289484813809395\n",
      "          vf_explained_var: 0.7296730875968933\n",
      "          vf_loss: 0.07159107178449631\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.59034482758621\n",
      "    ram_util_percent: 95.70103448275862\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1142790324698123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0552978189854574\n",
      "    mean_inference_ms: 2.4427482758199712\n",
      "    mean_raw_obs_processing_ms: 0.22708199684591285\n",
      "  time_since_restore: 31139.124544620514\n",
      "  time_this_iter_s: 216.97538900375366\n",
      "  time_total_s: 31139.124544620514\n",
      "  timers:\n",
      "    learn_throughput: 19.552\n",
      "    learn_time_ms: 204583.13\n",
      "    load_throughput: 6073640.083\n",
      "    load_time_ms: 0.659\n",
      "    sample_throughput: 18.052\n",
      "    sample_time_ms: 221577.063\n",
      "    update_time_ms: 6.763\n",
      "  timestamp: 1650249482\n",
      "  timesteps_since_restore: 576000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 2404.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 286\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014127973467111588\n",
      "          total_loss: 0.014128204435110092\n",
      "          vf_explained_var: -8.107513593813565e-08\n",
      "          vf_loss: 2.319262080163753e-07\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_agent_steps_trained: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.59206896551724\n",
      "    ram_util_percent: 95.68448275862069\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11330901341101246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0238882980497745\n",
      "    mean_inference_ms: 2.3990594321869354\n",
      "    mean_raw_obs_processing_ms: 0.13980876490768002\n",
      "  time_since_restore: 31336.90087914467\n",
      "  time_this_iter_s: 217.1251199245453\n",
      "  time_total_s: 31336.90087914467\n",
      "  timers:\n",
      "    learn_throughput: 19.545\n",
      "    learn_time_ms: 204654.891\n",
      "    load_throughput: 6647074.485\n",
      "    load_time_ms: 0.602\n",
      "    sample_throughput: 18.069\n",
      "    sample_time_ms: 221379.096\n",
      "    update_time_ms: 22.06\n",
      "  timestamp: 1650249686\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-41-40\n",
      "  done: false\n",
      "  episode_len_mean: 439.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 5.04\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2026\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8502671122550964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011185893788933754\n",
      "          model: {}\n",
      "          policy_loss: -0.04660426452755928\n",
      "          total_loss: 0.013646050356328487\n",
      "          vf_explained_var: 0.717535674571991\n",
      "          vf_loss: 0.060250312089920044\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_agent_steps_trained: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.31890034364261\n",
      "    ram_util_percent: 95.67353951890034\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11433849615218557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.055744186295086\n",
      "    mean_inference_ms: 2.4443838506242166\n",
      "    mean_raw_obs_processing_ms: 0.2269779073205548\n",
      "  time_since_restore: 31356.513882875443\n",
      "  time_this_iter_s: 217.3893382549286\n",
      "  time_total_s: 31356.513882875443\n",
      "  timers:\n",
      "    learn_throughput: 19.572\n",
      "    learn_time_ms: 204377.951\n",
      "    load_throughput: 6770193.293\n",
      "    load_time_ms: 0.591\n",
      "    sample_throughput: 18.069\n",
      "    sample_time_ms: 221374.13\n",
      "    update_time_ms: 7.135\n",
      "  timestamp: 1650249700\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-45-08\n",
      "  done: false\n",
      "  episode_len_mean: 2502.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 287\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.308812181653323e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.599261565014446e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.00037078242166899145\n",
      "          total_loss: 0.00038027262780815363\n",
      "          vf_explained_var: -0.06451577693223953\n",
      "          vf_loss: 9.489460353506729e-06\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.15796610169491\n",
      "    ram_util_percent: 95.64949152542374\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11338258937711616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.024340699954625\n",
      "    mean_inference_ms: 2.4010876871392925\n",
      "    mean_raw_obs_processing_ms: 0.1398629142536742\n",
      "  time_since_restore: 31559.45351910591\n",
      "  time_this_iter_s: 222.55263996124268\n",
      "  time_total_s: 31559.45351910591\n",
      "  timers:\n",
      "    learn_throughput: 19.509\n",
      "    learn_time_ms: 205029.018\n",
      "    load_throughput: 6514917.676\n",
      "    load_time_ms: 0.614\n",
      "    sample_throughput: 18.086\n",
      "    sample_time_ms: 221163.221\n",
      "    update_time_ms: 25.076\n",
      "  timestamp: 1650249908\n",
      "  timesteps_since_restore: 584000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 437.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 5.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2036\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8493874073028564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010222426615655422\n",
      "          model: {}\n",
      "          policy_loss: -0.04121823608875275\n",
      "          total_loss: 0.027608994394540787\n",
      "          vf_explained_var: 0.7379977703094482\n",
      "          vf_loss: 0.06882722675800323\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.60918367346939\n",
      "    ram_util_percent: 95.64183673469388\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11439876062478342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0561809707786105\n",
      "    mean_inference_ms: 2.445988096879702\n",
      "    mean_raw_obs_processing_ms: 0.2268870051579468\n",
      "  time_since_restore: 31578.165933847427\n",
      "  time_this_iter_s: 221.65205097198486\n",
      "  time_total_s: 31578.165933847427\n",
      "  timers:\n",
      "    learn_throughput: 19.546\n",
      "    learn_time_ms: 204643.866\n",
      "    load_throughput: 6816127.407\n",
      "    load_time_ms: 0.587\n",
      "    sample_throughput: 18.083\n",
      "    sample_time_ms: 221201.044\n",
      "    update_time_ms: 7.647\n",
      "  timestamp: 1650249921\n",
      "  timesteps_since_restore: 584000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-48-46\n",
      "  done: false\n",
      "  episode_len_mean: 2502.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 287\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0135855907574296\n",
      "          total_loss: 0.013585605658590794\n",
      "          vf_explained_var: -5.293917979543039e-07\n",
      "          vf_loss: 2.387224284916556e-08\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.54117647058824\n",
      "    ram_util_percent: 95.60830449826989\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11338258937711616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.024340699954625\n",
      "    mean_inference_ms: 2.4010876871392925\n",
      "    mean_raw_obs_processing_ms: 0.1398629142536742\n",
      "  time_since_restore: 31777.173063993454\n",
      "  time_this_iter_s: 217.71954488754272\n",
      "  time_total_s: 31777.173063993454\n",
      "  timers:\n",
      "    learn_throughput: 19.525\n",
      "    learn_time_ms: 204869.313\n",
      "    load_throughput: 6469696.128\n",
      "    load_time_ms: 0.618\n",
      "    sample_throughput: 18.059\n",
      "    sample_time_ms: 221495.895\n",
      "    update_time_ms: 25.838\n",
      "  timestamp: 1650250126\n",
      "  timesteps_since_restore: 588000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-48-59\n",
      "  done: false\n",
      "  episode_len_mean: 446.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2044\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.803119957447052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013396047987043858\n",
      "          model: {}\n",
      "          policy_loss: -0.045275989919900894\n",
      "          total_loss: 0.06896384805440903\n",
      "          vf_explained_var: 0.6761602163314819\n",
      "          vf_loss: 0.11423982679843903\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.06379310344828\n",
      "    ram_util_percent: 95.62137931034482\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11444688760298134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0565199714851206\n",
      "    mean_inference_ms: 2.4472748974759493\n",
      "    mean_raw_obs_processing_ms: 0.22681038996962818\n",
      "  time_since_restore: 31795.838730812073\n",
      "  time_this_iter_s: 217.67279696464539\n",
      "  time_total_s: 31795.838730812073\n",
      "  timers:\n",
      "    learn_throughput: 19.561\n",
      "    learn_time_ms: 204486.379\n",
      "    load_throughput: 6651818.254\n",
      "    load_time_ms: 0.601\n",
      "    sample_throughput: 18.06\n",
      "    sample_time_ms: 221489.295\n",
      "    update_time_ms: 7.988\n",
      "  timestamp: 1650250139\n",
      "  timesteps_since_restore: 588000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 03:52:14 (running for 08:53:34.49)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=5.23 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-52-25\n",
      "  done: false\n",
      "  episode_len_mean: 2502.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 287\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.012589219957590103\n",
      "          total_loss: 0.012589247897267342\n",
      "          vf_explained_var: -1.6202208996674017e-07\n",
      "          vf_loss: 3.3369808249972266e-08\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.01477663230239\n",
      "    ram_util_percent: 95.79896907216494\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11338258937711616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.024340699954625\n",
      "    mean_inference_ms: 2.4010876871392925\n",
      "    mean_raw_obs_processing_ms: 0.1398629142536742\n",
      "  time_since_restore: 31996.022912740707\n",
      "  time_this_iter_s: 218.84984874725342\n",
      "  time_total_s: 31996.022912740707\n",
      "  timers:\n",
      "    learn_throughput: 19.524\n",
      "    learn_time_ms: 204879.832\n",
      "    load_throughput: 6515929.781\n",
      "    load_time_ms: 0.614\n",
      "    sample_throughput: 18.072\n",
      "    sample_time_ms: 221333.36\n",
      "    update_time_ms: 23.104\n",
      "  timestamp: 1650250345\n",
      "  timesteps_since_restore: 592000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-52-38\n",
      "  done: false\n",
      "  episode_len_mean: 446.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2055\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8215659260749817\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009612779133021832\n",
      "          model: {}\n",
      "          policy_loss: -0.04715734347701073\n",
      "          total_loss: 0.037983886897563934\n",
      "          vf_explained_var: 0.6812570691108704\n",
      "          vf_loss: 0.08514123409986496\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.06413793103448\n",
      "    ram_util_percent: 95.82931034482759\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11451743427803437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0569833824710744\n",
      "    mean_inference_ms: 2.449096149664806\n",
      "    mean_raw_obs_processing_ms: 0.22671715233043038\n",
      "  time_since_restore: 32014.4437789917\n",
      "  time_this_iter_s: 218.60504817962646\n",
      "  time_total_s: 32014.4437789917\n",
      "  timers:\n",
      "    learn_throughput: 19.578\n",
      "    learn_time_ms: 204311.383\n",
      "    load_throughput: 3911411.186\n",
      "    load_time_ms: 1.023\n",
      "    sample_throughput: 18.067\n",
      "    sample_time_ms: 221396.228\n",
      "    update_time_ms: 7.971\n",
      "  timestamp: 1650250358\n",
      "  timesteps_since_restore: 592000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-56-05\n",
      "  done: false\n",
      "  episode_len_mean: 2504.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 291\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.556833855539774e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.113589064249227e-27\n",
      "          model: {}\n",
      "          policy_loss: 6.180027412483469e-05\n",
      "          total_loss: 0.000497654837090522\n",
      "          vf_explained_var: -0.020773164927959442\n",
      "          vf_loss: 0.0004358544247224927\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_agent_steps_trained: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.45938566552901\n",
      "    ram_util_percent: 95.88464163822525\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11366185872755213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0260343619864036\n",
      "    mean_inference_ms: 2.4086546779868083\n",
      "    mean_raw_obs_processing_ms: 0.1400470020649651\n",
      "  time_since_restore: 32215.750356674194\n",
      "  time_this_iter_s: 219.72744393348694\n",
      "  time_total_s: 32215.750356674194\n",
      "  timers:\n",
      "    learn_throughput: 19.608\n",
      "    learn_time_ms: 203993.368\n",
      "    load_throughput: 6571311.739\n",
      "    load_time_ms: 0.609\n",
      "    sample_throughput: 18.078\n",
      "    sample_time_ms: 221267.444\n",
      "    update_time_ms: 24.193\n",
      "  timestamp: 1650250565\n",
      "  timesteps_since_restore: 596000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-56-17\n",
      "  done: false\n",
      "  episode_len_mean: 436.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2066\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8614559769630432\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008947895839810371\n",
      "          model: {}\n",
      "          policy_loss: -0.04875646531581879\n",
      "          total_loss: 0.008769426494836807\n",
      "          vf_explained_var: 0.7105429172515869\n",
      "          vf_loss: 0.057525891810655594\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_agent_steps_trained: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.76404109589042\n",
      "    ram_util_percent: 95.9085616438356\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11458579926435909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0574179130191386\n",
      "    mean_inference_ms: 2.4507657912180756\n",
      "    mean_raw_obs_processing_ms: 0.2266220472437255\n",
      "  time_since_restore: 32233.642467021942\n",
      "  time_this_iter_s: 219.19868803024292\n",
      "  time_total_s: 32233.642467021942\n",
      "  timers:\n",
      "    learn_throughput: 19.665\n",
      "    learn_time_ms: 203412.179\n",
      "    load_throughput: 3212795.098\n",
      "    load_time_ms: 1.245\n",
      "    sample_throughput: 18.092\n",
      "    sample_time_ms: 221094.716\n",
      "    update_time_ms: 7.769\n",
      "  timestamp: 1650250577\n",
      "  timesteps_since_restore: 596000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-59-41\n",
      "  done: false\n",
      "  episode_len_mean: 2504.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 291\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.174468971152284e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -6.297684606813331e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128198847174644\n",
      "          total_loss: -0.014125173911452293\n",
      "          vf_explained_var: -9.222696917277062e-08\n",
      "          vf_loss: 3.0211017474357504e-06\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.46829268292683\n",
      "    ram_util_percent: 95.92020905923343\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11366185872755213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0260343619864036\n",
      "    mean_inference_ms: 2.4086546779868083\n",
      "    mean_raw_obs_processing_ms: 0.1400470020649651\n",
      "  time_since_restore: 32432.00794672966\n",
      "  time_this_iter_s: 216.2575900554657\n",
      "  time_total_s: 32432.00794672966\n",
      "  timers:\n",
      "    learn_throughput: 19.716\n",
      "    learn_time_ms: 202885.334\n",
      "    load_throughput: 6587308.493\n",
      "    load_time_ms: 0.607\n",
      "    sample_throughput: 18.16\n",
      "    sample_time_ms: 220260.263\n",
      "    update_time_ms: 24.201\n",
      "  timestamp: 1650250781\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_03-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 435.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.07\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2076\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8396111130714417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009814898483455181\n",
      "          model: {}\n",
      "          policy_loss: -0.050583649426698685\n",
      "          total_loss: 0.005625465419143438\n",
      "          vf_explained_var: 0.7732424736022949\n",
      "          vf_loss: 0.05620911717414856\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.87569444444445\n",
      "    ram_util_percent: 95.9267361111111\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11464516162213442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.057772174247021\n",
      "    mean_inference_ms: 2.4521175171605263\n",
      "    mean_raw_obs_processing_ms: 0.2265202159684478\n",
      "  time_since_restore: 32450.74388194084\n",
      "  time_this_iter_s: 217.10141491889954\n",
      "  time_total_s: 32450.74388194084\n",
      "  timers:\n",
      "    learn_throughput: 19.766\n",
      "    learn_time_ms: 202364.093\n",
      "    load_throughput: 3227256.569\n",
      "    load_time_ms: 1.239\n",
      "    sample_throughput: 18.175\n",
      "    sample_time_ms: 220079.766\n",
      "    update_time_ms: 7.396\n",
      "  timestamp: 1650250794\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-03-19\n",
      "  done: false\n",
      "  episode_len_mean: 2600.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 294\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.367600780781784e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -5.6770848822996434e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0030004791915416718\n",
      "          total_loss: 0.003125822404399514\n",
      "          vf_explained_var: 0.01279237400740385\n",
      "          vf_loss: 0.00012534290726762265\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_agent_steps_trained: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.88961937716263\n",
      "    ram_util_percent: 95.9598615916955\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11386755324042827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0272785264480362\n",
      "    mean_inference_ms: 2.4142064622395907\n",
      "    mean_raw_obs_processing_ms: 0.14017858137658593\n",
      "  time_since_restore: 32649.451762914658\n",
      "  time_this_iter_s: 217.44381618499756\n",
      "  time_total_s: 32649.451762914658\n",
      "  timers:\n",
      "    learn_throughput: 19.791\n",
      "    learn_time_ms: 202114.187\n",
      "    load_throughput: 6850080.026\n",
      "    load_time_ms: 0.584\n",
      "    sample_throughput: 18.261\n",
      "    sample_time_ms: 219047.86\n",
      "    update_time_ms: 23.976\n",
      "  timestamp: 1650250999\n",
      "  timesteps_since_restore: 604000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 440.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.1\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2085\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8324052095413208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009305798448622227\n",
      "          model: {}\n",
      "          policy_loss: -0.04613415524363518\n",
      "          total_loss: 0.004293695092201233\n",
      "          vf_explained_var: 0.7760632634162903\n",
      "          vf_loss: 0.05042785033583641\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_agent_steps_trained: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.56262975778546\n",
      "    ram_util_percent: 95.93979238754325\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11468645576292434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0579833005128203\n",
      "    mean_inference_ms: 2.4530533071562637\n",
      "    mean_raw_obs_processing_ms: 0.22641244248249848\n",
      "  time_since_restore: 32667.851555109024\n",
      "  time_this_iter_s: 217.10767316818237\n",
      "  time_total_s: 32667.851555109024\n",
      "  timers:\n",
      "    learn_throughput: 19.82\n",
      "    learn_time_ms: 201812.908\n",
      "    load_throughput: 3220442.26\n",
      "    load_time_ms: 1.242\n",
      "    sample_throughput: 18.288\n",
      "    sample_time_ms: 218722.512\n",
      "    update_time_ms: 7.709\n",
      "  timestamp: 1650251011\n",
      "  timesteps_since_restore: 604000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-06-57\n",
      "  done: false\n",
      "  episode_len_mean: 2407.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 299\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.355386942049556e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.5222910701788324e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.002265381393954158\n",
      "          total_loss: -0.0019255653023719788\n",
      "          vf_explained_var: 0.031933315098285675\n",
      "          vf_loss: 0.0003398108237888664\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.63979238754325\n",
      "    ram_util_percent: 95.97058823529412\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11416041099095725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.029044638323575\n",
      "    mean_inference_ms: 2.4219531316954197\n",
      "    mean_raw_obs_processing_ms: 0.14036820337940686\n",
      "  time_since_restore: 32867.02427101135\n",
      "  time_this_iter_s: 217.57250809669495\n",
      "  time_total_s: 32867.02427101135\n",
      "  timers:\n",
      "    learn_throughput: 19.783\n",
      "    learn_time_ms: 202192.92\n",
      "    load_throughput: 6804516.548\n",
      "    load_time_ms: 0.588\n",
      "    sample_throughput: 18.328\n",
      "    sample_time_ms: 218240.969\n",
      "    update_time_ms: 23.367\n",
      "  timestamp: 1650251217\n",
      "  timesteps_since_restore: 608000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-07-08\n",
      "  done: false\n",
      "  episode_len_mean: 434.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.06\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2095\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.857093334197998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009991108439862728\n",
      "          model: {}\n",
      "          policy_loss: -0.042200010269880295\n",
      "          total_loss: 0.006931687239557505\n",
      "          vf_explained_var: 0.8289198279380798\n",
      "          vf_loss: 0.049131691455841064\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.95069444444444\n",
      "    ram_util_percent: 95.95763888888888\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1147310156782085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.058162273293617\n",
      "    mean_inference_ms: 2.4538692532832997\n",
      "    mean_raw_obs_processing_ms: 0.22628568472911567\n",
      "  time_since_restore: 32884.5868062973\n",
      "  time_this_iter_s: 216.7352511882782\n",
      "  time_total_s: 32884.5868062973\n",
      "  timers:\n",
      "    learn_throughput: 19.818\n",
      "    learn_time_ms: 201834.177\n",
      "    load_throughput: 3237907.17\n",
      "    load_time_ms: 1.235\n",
      "    sample_throughput: 18.344\n",
      "    sample_time_ms: 218052.346\n",
      "    update_time_ms: 7.627\n",
      "  timestamp: 1650251228\n",
      "  timesteps_since_restore: 608000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 04:08:54 (running for 09:10:14.67)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=5.06 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-10-34\n",
      "  done: false\n",
      "  episode_len_mean: 2407.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 299\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128006063401699\n",
      "          total_loss: -0.01411527395248413\n",
      "          vf_explained_var: -6.152737341835746e-08\n",
      "          vf_loss: 1.2737597899104003e-05\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_agent_steps_trained: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.58472222222221\n",
      "    ram_util_percent: 95.88263888888889\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11416041099095725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.029044638323575\n",
      "    mean_inference_ms: 2.4219531316954197\n",
      "    mean_raw_obs_processing_ms: 0.14036820337940686\n",
      "  time_since_restore: 33084.67904210091\n",
      "  time_this_iter_s: 217.65477108955383\n",
      "  time_total_s: 33084.67904210091\n",
      "  timers:\n",
      "    learn_throughput: 19.784\n",
      "    learn_time_ms: 202180.371\n",
      "    load_throughput: 6846446.031\n",
      "    load_time_ms: 0.584\n",
      "    sample_throughput: 18.332\n",
      "    sample_time_ms: 218195.381\n",
      "    update_time_ms: 19.993\n",
      "  timestamp: 1650251434\n",
      "  timesteps_since_restore: 612000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-10-46\n",
      "  done: false\n",
      "  episode_len_mean: 436.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.07\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2105\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8455565571784973\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009455295279622078\n",
      "          model: {}\n",
      "          policy_loss: -0.042598895728588104\n",
      "          total_loss: 0.008276240900158882\n",
      "          vf_explained_var: 0.8043907284736633\n",
      "          vf_loss: 0.050875138491392136\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_agent_steps_trained: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.64913494809689\n",
      "    ram_util_percent: 95.8719723183391\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11476678386709324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0582755784588367\n",
      "    mean_inference_ms: 2.4544749151203353\n",
      "    mean_raw_obs_processing_ms: 0.22613732809982434\n",
      "  time_since_restore: 33102.519062280655\n",
      "  time_this_iter_s: 217.93225598335266\n",
      "  time_total_s: 33102.519062280655\n",
      "  timers:\n",
      "    learn_throughput: 19.805\n",
      "    learn_time_ms: 201974.227\n",
      "    load_throughput: 3128443.35\n",
      "    load_time_ms: 1.279\n",
      "    sample_throughput: 18.356\n",
      "    sample_time_ms: 217910.752\n",
      "    update_time_ms: 7.804\n",
      "  timestamp: 1650251446\n",
      "  timesteps_since_restore: 612000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-14-14\n",
      "  done: false\n",
      "  episode_len_mean: 2210.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 303\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.958925705066948e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.488155735181027e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.0004619132378138602\n",
      "          total_loss: -0.0003672278835438192\n",
      "          vf_explained_var: -0.08418108522891998\n",
      "          vf_loss: 9.468317875871435e-05\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.80896551724138\n",
      "    ram_util_percent: 95.77758620689656\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.114338375272883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0301445208046358\n",
      "    mean_inference_ms: 2.4266465745991055\n",
      "    mean_raw_obs_processing_ms: 0.14049574236542295\n",
      "  time_since_restore: 33304.250709056854\n",
      "  time_this_iter_s: 219.57166695594788\n",
      "  time_total_s: 33304.250709056854\n",
      "  timers:\n",
      "    learn_throughput: 19.749\n",
      "    learn_time_ms: 202541.712\n",
      "    load_throughput: 6923578.739\n",
      "    load_time_ms: 0.578\n",
      "    sample_throughput: 18.337\n",
      "    sample_time_ms: 218136.609\n",
      "    update_time_ms: 19.142\n",
      "  timestamp: 1650251654\n",
      "  timesteps_since_restore: 616000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 439.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.12\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2113\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8042526245117188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011507079005241394\n",
      "          model: {}\n",
      "          policy_loss: -0.04216330498456955\n",
      "          total_loss: 0.011943540535867214\n",
      "          vf_explained_var: 0.7521336078643799\n",
      "          vf_loss: 0.05410684645175934\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.16172413793105\n",
      "    ram_util_percent: 95.77517241379311\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11478956742938702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0583485941566837\n",
      "    mean_inference_ms: 2.4548547341597216\n",
      "    mean_raw_obs_processing_ms: 0.2260049332603492\n",
      "  time_since_restore: 33321.83739423752\n",
      "  time_this_iter_s: 219.3183319568634\n",
      "  time_total_s: 33321.83739423752\n",
      "  timers:\n",
      "    learn_throughput: 19.769\n",
      "    learn_time_ms: 202332.827\n",
      "    load_throughput: 3138274.598\n",
      "    load_time_ms: 1.275\n",
      "    sample_throughput: 18.355\n",
      "    sample_time_ms: 217927.333\n",
      "    update_time_ms: 7.785\n",
      "  timestamp: 1650251665\n",
      "  timesteps_since_restore: 616000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 2210.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 303\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0141282444819808\n",
      "          total_loss: -0.014127789996564388\n",
      "          vf_explained_var: 4.396643760173902e-08\n",
      "          vf_loss: 4.539554652183142e-07\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_agent_steps_trained: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.27291666666666\n",
      "    ram_util_percent: 95.734375\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.114338375272883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0301445208046358\n",
      "    mean_inference_ms: 2.4266465745991055\n",
      "    mean_raw_obs_processing_ms: 0.14049574236542295\n",
      "  time_since_restore: 33520.63763189316\n",
      "  time_this_iter_s: 216.3869228363037\n",
      "  time_total_s: 33520.63763189316\n",
      "  timers:\n",
      "    learn_throughput: 19.745\n",
      "    learn_time_ms: 202587.302\n",
      "    load_throughput: 6796522.585\n",
      "    load_time_ms: 0.589\n",
      "    sample_throughput: 18.317\n",
      "    sample_time_ms: 218379.9\n",
      "    update_time_ms: 18.546\n",
      "  timestamp: 1650251870\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-18-02\n",
      "  done: false\n",
      "  episode_len_mean: 442.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2124\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8190377354621887\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011253781616687775\n",
      "          model: {}\n",
      "          policy_loss: -0.04388914629817009\n",
      "          total_loss: 0.03492409363389015\n",
      "          vf_explained_var: 0.6936196684837341\n",
      "          vf_loss: 0.07881323993206024\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_agent_steps_trained: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.12673611111111\n",
      "    ram_util_percent: 95.75381944444445\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11481974918606838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0584404953443394\n",
      "    mean_inference_ms: 2.4551970183508454\n",
      "    mean_raw_obs_processing_ms: 0.22582253328629837\n",
      "  time_since_restore: 33538.38876223564\n",
      "  time_this_iter_s: 216.55136799812317\n",
      "  time_total_s: 33538.38876223564\n",
      "  timers:\n",
      "    learn_throughput: 19.767\n",
      "    learn_time_ms: 202356.82\n",
      "    load_throughput: 3102984.39\n",
      "    load_time_ms: 1.289\n",
      "    sample_throughput: 18.334\n",
      "    sample_time_ms: 218177.857\n",
      "    update_time_ms: 7.465\n",
      "  timestamp: 1650251882\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 2210.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 303\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014127951115369797\n",
      "          total_loss: -0.014127866365015507\n",
      "          vf_explained_var: -2.8001363716612104e-07\n",
      "          vf_loss: 8.955062469340191e-08\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.9548275862069\n",
      "    ram_util_percent: 95.83241379310344\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.114338375272883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0301445208046358\n",
      "    mean_inference_ms: 2.4266465745991055\n",
      "    mean_raw_obs_processing_ms: 0.14049574236542295\n",
      "  time_since_restore: 33739.165071725845\n",
      "  time_this_iter_s: 218.52743983268738\n",
      "  time_total_s: 33739.165071725845\n",
      "  timers:\n",
      "    learn_throughput: 19.772\n",
      "    learn_time_ms: 202305.937\n",
      "    load_throughput: 6831670.331\n",
      "    load_time_ms: 0.586\n",
      "    sample_throughput: 18.323\n",
      "    sample_time_ms: 218303.816\n",
      "    update_time_ms: 16.236\n",
      "  timestamp: 1650252089\n",
      "  timesteps_since_restore: 624000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-21-40\n",
      "  done: false\n",
      "  episode_len_mean: 438.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 5.12\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2134\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8117814660072327\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009178755804896355\n",
      "          model: {}\n",
      "          policy_loss: -0.04101500287652016\n",
      "          total_loss: 0.02561981789767742\n",
      "          vf_explained_var: 0.6940938830375671\n",
      "          vf_loss: 0.06663482636213303\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.2\n",
      "    ram_util_percent: 95.84131944444442\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11483932562742469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0584879627386965\n",
      "    mean_inference_ms: 2.455333300364513\n",
      "    mean_raw_obs_processing_ms: 0.22565546661508798\n",
      "  time_since_restore: 33755.96766614914\n",
      "  time_this_iter_s: 217.57890391349792\n",
      "  time_total_s: 33755.96766614914\n",
      "  timers:\n",
      "    learn_throughput: 19.794\n",
      "    learn_time_ms: 202080.491\n",
      "    load_throughput: 3118267.755\n",
      "    load_time_ms: 1.283\n",
      "    sample_throughput: 18.343\n",
      "    sample_time_ms: 218070.621\n",
      "    update_time_ms: 6.942\n",
      "  timestamp: 1650252100\n",
      "  timesteps_since_restore: 624000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-25-08\n",
      "  done: false\n",
      "  episode_len_mean: 2305.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 305\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.789896060942576e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1907027214435087e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0026955960784107447\n",
      "          total_loss: -0.002093562623485923\n",
      "          vf_explained_var: 0.06697174161672592\n",
      "          vf_loss: 0.0006020420114509761\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_agent_steps_trained: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.84\n",
      "    ram_util_percent: 95.92379310344828\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11442234545681035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0306612324020619\n",
      "    mean_inference_ms: 2.4288083932514497\n",
      "    mean_raw_obs_processing_ms: 0.14054771602871108\n",
      "  time_since_restore: 33957.63404464722\n",
      "  time_this_iter_s: 218.46897292137146\n",
      "  time_total_s: 33957.63404464722\n",
      "  timers:\n",
      "    learn_throughput: 19.751\n",
      "    learn_time_ms: 202523.179\n",
      "    load_throughput: 6871402.359\n",
      "    load_time_ms: 0.582\n",
      "    sample_throughput: 18.359\n",
      "    sample_time_ms: 217878.32\n",
      "    update_time_ms: 16.124\n",
      "  timestamp: 1650252308\n",
      "  timesteps_since_restore: 628000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-25-17\n",
      "  done: false\n",
      "  episode_len_mean: 435.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.07\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2143\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7895597815513611\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010631052777171135\n",
      "          model: {}\n",
      "          policy_loss: -0.042700059711933136\n",
      "          total_loss: 0.02024536393582821\n",
      "          vf_explained_var: 0.7571181058883667\n",
      "          vf_loss: 0.0629454255104065\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_agent_steps_trained: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.28892733564014\n",
      "    ram_util_percent: 95.90207612456746\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11485744747927859\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0585139936562316\n",
      "    mean_inference_ms: 2.4552711008438\n",
      "    mean_raw_obs_processing_ms: 0.22550401652814922\n",
      "  time_since_restore: 33973.11034202576\n",
      "  time_this_iter_s: 217.14267587661743\n",
      "  time_total_s: 33973.11034202576\n",
      "  timers:\n",
      "    learn_throughput: 19.785\n",
      "    learn_time_ms: 202176.055\n",
      "    load_throughput: 3119717.357\n",
      "    load_time_ms: 1.282\n",
      "    sample_throughput: 18.379\n",
      "    sample_time_ms: 217642.405\n",
      "    update_time_ms: 7.299\n",
      "  timestamp: 1650252317\n",
      "  timesteps_since_restore: 628000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 04:25:35 (running for 09:26:55.22)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=5.07 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-28-46\n",
      "  done: false\n",
      "  episode_len_mean: 2305.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 305\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0083342981903997e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.143498353251182e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128139242529869\n",
      "          total_loss: -0.014052215963602066\n",
      "          vf_explained_var: -6.0886464758880265e-09\n",
      "          vf_loss: 7.591593748657033e-05\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.43586206896552\n",
      "    ram_util_percent: 95.90241379310343\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11442234545681035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0306612324020619\n",
      "    mean_inference_ms: 2.4288083932514497\n",
      "    mean_raw_obs_processing_ms: 0.14054771602871108\n",
      "  time_since_restore: 34176.35085558891\n",
      "  time_this_iter_s: 218.71681094169617\n",
      "  time_total_s: 34176.35085558891\n",
      "  timers:\n",
      "    learn_throughput: 19.733\n",
      "    learn_time_ms: 202709.868\n",
      "    load_throughput: 6956593.274\n",
      "    load_time_ms: 0.575\n",
      "    sample_throughput: 18.357\n",
      "    sample_time_ms: 217897.416\n",
      "    update_time_ms: 16.397\n",
      "  timestamp: 1650252526\n",
      "  timesteps_since_restore: 632000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-28-56\n",
      "  done: false\n",
      "  episode_len_mean: 435.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2153\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8262676000595093\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00841495580971241\n",
      "          model: {}\n",
      "          policy_loss: -0.041861724108457565\n",
      "          total_loss: 0.012179119512438774\n",
      "          vf_explained_var: 0.7771469950675964\n",
      "          vf_loss: 0.05404084548354149\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.07827586206895\n",
      "    ram_util_percent: 95.9055172413793\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11486977682426475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0584887043674076\n",
      "    mean_inference_ms: 2.4549752336839834\n",
      "    mean_raw_obs_processing_ms: 0.22532638238983171\n",
      "  time_since_restore: 34192.06952500343\n",
      "  time_this_iter_s: 218.9591829776764\n",
      "  time_total_s: 34192.06952500343\n",
      "  timers:\n",
      "    learn_throughput: 19.764\n",
      "    learn_time_ms: 202389.907\n",
      "    load_throughput: 4625007.857\n",
      "    load_time_ms: 0.865\n",
      "    sample_throughput: 18.386\n",
      "    sample_time_ms: 217559.645\n",
      "    update_time_ms: 7.48\n",
      "  timestamp: 1650252536\n",
      "  timesteps_since_restore: 632000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-32-26\n",
      "  done: false\n",
      "  episode_len_mean: 2402.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 306\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.9619595803703506e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.785197730099799e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0016682629939168692\n",
      "          total_loss: 0.001790024689398706\n",
      "          vf_explained_var: 0.06431692838668823\n",
      "          vf_loss: 0.0001217617973452434\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_agent_steps_trained: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.3085910652921\n",
      "    ram_util_percent: 95.95635738831615\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11446368354511131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0309129904390197\n",
      "    mean_inference_ms: 2.4298575358203744\n",
      "    mean_raw_obs_processing_ms: 0.14057144682228068\n",
      "  time_since_restore: 34396.07269048691\n",
      "  time_this_iter_s: 219.721834897995\n",
      "  time_total_s: 34396.07269048691\n",
      "  timers:\n",
      "    learn_throughput: 19.717\n",
      "    learn_time_ms: 202874.095\n",
      "    load_throughput: 6919295.583\n",
      "    load_time_ms: 0.578\n",
      "    sample_throughput: 18.354\n",
      "    sample_time_ms: 217937.469\n",
      "    update_time_ms: 14.199\n",
      "  timestamp: 1650252746\n",
      "  timesteps_since_restore: 636000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-32-35\n",
      "  done: false\n",
      "  episode_len_mean: 448.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.0\n",
      "  episode_reward_mean: 5.25\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2161\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7871971726417542\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011860769242048264\n",
      "          model: {}\n",
      "          policy_loss: -0.045116644352674484\n",
      "          total_loss: 0.020926130935549736\n",
      "          vf_explained_var: 0.779872715473175\n",
      "          vf_loss: 0.06604277342557907\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_agent_steps_trained: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.43608247422681\n",
      "    ram_util_percent: 95.95154639175257\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11487231626821222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.058410038695729\n",
      "    mean_inference_ms: 2.4546306809024783\n",
      "    mean_raw_obs_processing_ms: 0.22516598524967935\n",
      "  time_since_restore: 34411.5029361248\n",
      "  time_this_iter_s: 219.4334111213684\n",
      "  time_total_s: 34411.5029361248\n",
      "  timers:\n",
      "    learn_throughput: 19.746\n",
      "    learn_time_ms: 202572.908\n",
      "    load_throughput: 6104357.444\n",
      "    load_time_ms: 0.655\n",
      "    sample_throughput: 18.38\n",
      "    sample_time_ms: 217623.755\n",
      "    update_time_ms: 7.481\n",
      "  timestamp: 1650252755\n",
      "  timesteps_since_restore: 636000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-36-04\n",
      "  done: false\n",
      "  episode_len_mean: 2305.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 309\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1113349890469509e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.693751771057566e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.011183254420757294\n",
      "          total_loss: 0.015871066600084305\n",
      "          vf_explained_var: 0.10347533226013184\n",
      "          vf_loss: 0.004687803331762552\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.419723183391\n",
      "    ram_util_percent: 95.90103806228373\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11456990829827843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.031588760485682\n",
      "    mean_inference_ms: 2.4326685119193563\n",
      "    mean_raw_obs_processing_ms: 0.1406375520382721\n",
      "  time_since_restore: 34613.55155348778\n",
      "  time_this_iter_s: 217.47886300086975\n",
      "  time_total_s: 34613.55155348778\n",
      "  timers:\n",
      "    learn_throughput: 19.689\n",
      "    learn_time_ms: 203162.268\n",
      "    load_throughput: 6986140.329\n",
      "    load_time_ms: 0.573\n",
      "    sample_throughput: 18.354\n",
      "    sample_time_ms: 217940.533\n",
      "    update_time_ms: 14.697\n",
      "  timestamp: 1650252964\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-36-13\n",
      "  done: false\n",
      "  episode_len_mean: 453.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.0\n",
      "  episode_reward_mean: 5.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2169\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7739740610122681\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009806731715798378\n",
      "          model: {}\n",
      "          policy_loss: -0.041067734360694885\n",
      "          total_loss: 0.00707433745265007\n",
      "          vf_explained_var: 0.7794162034988403\n",
      "          vf_loss: 0.048142071813344955\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.95798611111111\n",
      "    ram_util_percent: 95.90381944444444\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11486818467559821\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0582847028062523\n",
      "    mean_inference_ms: 2.4541604659906073\n",
      "    mean_raw_obs_processing_ms: 0.22498435391021282\n",
      "  time_since_restore: 34628.95903801918\n",
      "  time_this_iter_s: 217.45610189437866\n",
      "  time_total_s: 34628.95903801918\n",
      "  timers:\n",
      "    learn_throughput: 19.726\n",
      "    learn_time_ms: 202779.59\n",
      "    load_throughput: 6119274.902\n",
      "    load_time_ms: 0.654\n",
      "    sample_throughput: 18.379\n",
      "    sample_time_ms: 217642.338\n",
      "    update_time_ms: 7.529\n",
      "  timestamp: 1650252973\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-39-42\n",
      "  done: false\n",
      "  episode_len_mean: 2305.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 309\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412810105830431\n",
      "          total_loss: 0.014831618405878544\n",
      "          vf_explained_var: -1.9419577057533388e-08\n",
      "          vf_loss: 0.0007035130402073264\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_agent_steps_trained: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.07750865051904\n",
      "    ram_util_percent: 95.93148788927334\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11456990829827843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.031588760485682\n",
      "    mean_inference_ms: 2.4326685119193563\n",
      "    mean_raw_obs_processing_ms: 0.1406375520382721\n",
      "  time_since_restore: 34831.48065948486\n",
      "  time_this_iter_s: 217.92910599708557\n",
      "  time_total_s: 34831.48065948486\n",
      "  timers:\n",
      "    learn_throughput: 19.663\n",
      "    learn_time_ms: 203428.3\n",
      "    load_throughput: 7025340.647\n",
      "    load_time_ms: 0.569\n",
      "    sample_throughput: 18.348\n",
      "    sample_time_ms: 218012.469\n",
      "    update_time_ms: 14.794\n",
      "  timestamp: 1650253182\n",
      "  timesteps_since_restore: 644000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-39-51\n",
      "  done: false\n",
      "  episode_len_mean: 467.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 5.65\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2176\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7346082925796509\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013596663251519203\n",
      "          model: {}\n",
      "          policy_loss: -0.04559295251965523\n",
      "          total_loss: 0.023398250341415405\n",
      "          vf_explained_var: 0.7008266448974609\n",
      "          vf_loss: 0.06899121403694153\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_agent_steps_trained: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.67965517241379\n",
      "    ram_util_percent: 95.91862068965517\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11485881099192814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.058142900903464\n",
      "    mean_inference_ms: 2.453653541615642\n",
      "    mean_raw_obs_processing_ms: 0.2248001607138059\n",
      "  time_since_restore: 34847.21801781654\n",
      "  time_this_iter_s: 218.25897979736328\n",
      "  time_total_s: 34847.21801781654\n",
      "  timers:\n",
      "    learn_throughput: 19.7\n",
      "    learn_time_ms: 203043.412\n",
      "    load_throughput: 6117489.881\n",
      "    load_time_ms: 0.654\n",
      "    sample_throughput: 18.374\n",
      "    sample_time_ms: 217700.802\n",
      "    update_time_ms: 7.488\n",
      "  timestamp: 1650253191\n",
      "  timesteps_since_restore: 644000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 04:42:16 (running for 09:43:36.20)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=5.65 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-43-23\n",
      "  done: false\n",
      "  episode_len_mean: 2204.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 317\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1146001552680332e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.791446493416553e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.006040862761437893\n",
      "          total_loss: -0.005798912141472101\n",
      "          vf_explained_var: 0.12736278772354126\n",
      "          vf_loss: 0.0002419512311462313\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.3815699658703\n",
      "    ram_util_percent: 95.89385665529008\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11480354370484726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.033063627058777\n",
      "    mean_inference_ms: 2.4387755264270106\n",
      "    mean_raw_obs_processing_ms: 0.14077927503477117\n",
      "  time_since_restore: 35052.70837545395\n",
      "  time_this_iter_s: 221.2277159690857\n",
      "  time_total_s: 35052.70837545395\n",
      "  timers:\n",
      "    learn_throughput: 19.611\n",
      "    learn_time_ms: 203964.924\n",
      "    load_throughput: 7025340.647\n",
      "    load_time_ms: 0.569\n",
      "    sample_throughput: 18.34\n",
      "    sample_time_ms: 218107.222\n",
      "    update_time_ms: 15.205\n",
      "  timestamp: 1650253403\n",
      "  timesteps_since_restore: 648000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-43-31\n",
      "  done: false\n",
      "  episode_len_mean: 469.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 5.7\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2184\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7757810950279236\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012223274447023869\n",
      "          model: {}\n",
      "          policy_loss: -0.038143008947372437\n",
      "          total_loss: 0.04633220657706261\n",
      "          vf_explained_var: 0.7316135764122009\n",
      "          vf_loss: 0.08447521924972534\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.27379310344828\n",
      "    ram_util_percent: 95.87793103448276\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11484255533124195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0579410812407197\n",
      "    mean_inference_ms: 2.4529102570354917\n",
      "    mean_raw_obs_processing_ms: 0.22457287124251743\n",
      "  time_since_restore: 35067.44409275055\n",
      "  time_this_iter_s: 220.22607493400574\n",
      "  time_total_s: 35067.44409275055\n",
      "  timers:\n",
      "    learn_throughput: 19.65\n",
      "    learn_time_ms: 203562.48\n",
      "    load_throughput: 2745277.764\n",
      "    load_time_ms: 1.457\n",
      "    sample_throughput: 18.366\n",
      "    sample_time_ms: 217794.276\n",
      "    update_time_ms: 7.567\n",
      "  timestamp: 1650253411\n",
      "  timesteps_since_restore: 648000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 652000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-47-02\n",
      "  done: false\n",
      "  episode_len_mean: 2105.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 318\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.9449115171926034e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.443502651550113e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.00018450457719154656\n",
      "          total_loss: 0.00031543918885290623\n",
      "          vf_explained_var: 0.003572591580450535\n",
      "          vf_loss: 0.0001309412473347038\n",
      "    num_agent_steps_sampled: 652000\n",
      "    num_agent_steps_trained: 652000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.4573883161512\n",
      "    ram_util_percent: 95.87560137457044\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11482701727576637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.033208533866608\n",
      "    mean_inference_ms: 2.439358369405623\n",
      "    mean_raw_obs_processing_ms: 0.1407861067629738\n",
      "  time_since_restore: 35271.93107461929\n",
      "  time_this_iter_s: 219.22269916534424\n",
      "  time_total_s: 35271.93107461929\n",
      "  timers:\n",
      "    learn_throughput: 19.579\n",
      "    learn_time_ms: 204305.018\n",
      "    load_throughput: 7064388.395\n",
      "    load_time_ms: 0.566\n",
      "    sample_throughput: 18.31\n",
      "    sample_time_ms: 218461.538\n",
      "    update_time_ms: 15.801\n",
      "  timestamp: 1650253622\n",
      "  timesteps_since_restore: 652000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 652000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-47-11\n",
      "  done: false\n",
      "  episode_len_mean: 478.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2193\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8056350350379944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009737852960824966\n",
      "          model: {}\n",
      "          policy_loss: -0.04278440400958061\n",
      "          total_loss: 0.06676413118839264\n",
      "          vf_explained_var: 0.6464642286300659\n",
      "          vf_loss: 0.10954853892326355\n",
      "    num_agent_steps_sampled: 652000\n",
      "    num_agent_steps_trained: 652000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.45223367697595\n",
      "    ram_util_percent: 95.89003436426117\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11481396806667313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.057666912603755\n",
      "    mean_inference_ms: 2.451875990048008\n",
      "    mean_raw_obs_processing_ms: 0.2243052516354497\n",
      "  time_since_restore: 35287.08120965958\n",
      "  time_this_iter_s: 219.6371169090271\n",
      "  time_total_s: 35287.08120965958\n",
      "  timers:\n",
      "    learn_throughput: 19.621\n",
      "    learn_time_ms: 203861.303\n",
      "    load_throughput: 2816244.943\n",
      "    load_time_ms: 1.42\n",
      "    sample_throughput: 18.333\n",
      "    sample_time_ms: 218188.117\n",
      "    update_time_ms: 7.788\n",
      "  timestamp: 1650253631\n",
      "  timesteps_since_restore: 652000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-50-40\n",
      "  done: false\n",
      "  episode_len_mean: 2105.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 318\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128063805401325\n",
      "          total_loss: -0.014127698726952076\n",
      "          vf_explained_var: 1.0485290147244086e-07\n",
      "          vf_loss: 3.6324408370091987e-07\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.51458333333333\n",
      "    ram_util_percent: 95.846875\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11482701727576637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.033208533866608\n",
      "    mean_inference_ms: 2.439358369405623\n",
      "    mean_raw_obs_processing_ms: 0.1407861067629738\n",
      "  time_since_restore: 35489.35270571709\n",
      "  time_this_iter_s: 217.42163109779358\n",
      "  time_total_s: 35489.35270571709\n",
      "  timers:\n",
      "    learn_throughput: 19.579\n",
      "    learn_time_ms: 204300.213\n",
      "    load_throughput: 7103271.095\n",
      "    load_time_ms: 0.563\n",
      "    sample_throughput: 18.298\n",
      "    sample_time_ms: 218598.59\n",
      "    update_time_ms: 15.779\n",
      "  timestamp: 1650253840\n",
      "  timesteps_since_restore: 656000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-50-48\n",
      "  done: false\n",
      "  episode_len_mean: 482.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.11\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2201\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7757911086082458\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010651842691004276\n",
      "          model: {}\n",
      "          policy_loss: -0.04518318176269531\n",
      "          total_loss: 0.028521550819277763\n",
      "          vf_explained_var: 0.7380823493003845\n",
      "          vf_loss: 0.07370472699403763\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.84895833333333\n",
      "    ram_util_percent: 95.85277777777777\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11478820464986539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.057406752020745\n",
      "    mean_inference_ms: 2.450868168920209\n",
      "    mean_raw_obs_processing_ms: 0.2240565874464037\n",
      "  time_since_restore: 35504.1004076004\n",
      "  time_this_iter_s: 217.01919794082642\n",
      "  time_total_s: 35504.1004076004\n",
      "  timers:\n",
      "    learn_throughput: 19.631\n",
      "    learn_time_ms: 203755.349\n",
      "    load_throughput: 2774056.449\n",
      "    load_time_ms: 1.442\n",
      "    sample_throughput: 18.318\n",
      "    sample_time_ms: 218366.607\n",
      "    update_time_ms: 7.705\n",
      "  timestamp: 1650253848\n",
      "  timesteps_since_restore: 656000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-54-19\n",
      "  done: false\n",
      "  episode_len_mean: 2203.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 320\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6337539588488573e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.962366887333696e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0004635915975086391\n",
      "          total_loss: -0.0003333701752126217\n",
      "          vf_explained_var: 0.07887107878923416\n",
      "          vf_loss: 0.00013022840721532702\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_agent_steps_trained: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.52034482758621\n",
      "    ram_util_percent: 95.9658620689655\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11487201832829781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.033484861956552\n",
      "    mean_inference_ms: 2.4404425643174026\n",
      "    mean_raw_obs_processing_ms: 0.1407948199640906\n",
      "  time_since_restore: 35708.39332270622\n",
      "  time_this_iter_s: 219.04061698913574\n",
      "  time_total_s: 35708.39332270622\n",
      "  timers:\n",
      "    learn_throughput: 19.538\n",
      "    learn_time_ms: 204724.755\n",
      "    load_throughput: 7103571.852\n",
      "    load_time_ms: 0.563\n",
      "    sample_throughput: 18.312\n",
      "    sample_time_ms: 218435.752\n",
      "    update_time_ms: 15.972\n",
      "  timestamp: 1650254059\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-54-27\n",
      "  done: false\n",
      "  episode_len_mean: 483.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2211\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7774298787117004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0115335239097476\n",
      "          model: {}\n",
      "          policy_loss: -0.042421936988830566\n",
      "          total_loss: 0.05141125246882439\n",
      "          vf_explained_var: 0.6584503054618835\n",
      "          vf_loss: 0.09383320063352585\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_agent_steps_trained: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.87137931034484\n",
      "    ram_util_percent: 95.97827586206895\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1147528960019175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0570310581913325\n",
      "    mean_inference_ms: 2.4494567271474836\n",
      "    mean_raw_obs_processing_ms: 0.2237367529343523\n",
      "  time_since_restore: 35722.744852781296\n",
      "  time_this_iter_s: 218.64444518089294\n",
      "  time_total_s: 35722.744852781296\n",
      "  timers:\n",
      "    learn_throughput: 19.594\n",
      "    learn_time_ms: 204140.679\n",
      "    load_throughput: 2779986.081\n",
      "    load_time_ms: 1.439\n",
      "    sample_throughput: 18.341\n",
      "    sample_time_ms: 218085.643\n",
      "    update_time_ms: 7.707\n",
      "  timestamp: 1650254067\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 2203.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 320\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0223031486489103e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.735102420806745e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128257520496845\n",
      "          total_loss: -0.014123544096946716\n",
      "          vf_explained_var: 1.5067797676238115e-07\n",
      "          vf_loss: 4.711703240900533e-06\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.98758620689655\n",
      "    ram_util_percent: 95.90310344827587\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11487201832829781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.033484861956552\n",
      "    mean_inference_ms: 2.4404425643174026\n",
      "    mean_raw_obs_processing_ms: 0.1407948199640906\n",
      "  time_since_restore: 35926.574536800385\n",
      "  time_this_iter_s: 218.181214094162\n",
      "  time_total_s: 35926.574536800385\n",
      "  timers:\n",
      "    learn_throughput: 19.522\n",
      "    learn_time_ms: 204900.289\n",
      "    load_throughput: 7137722.187\n",
      "    load_time_ms: 0.56\n",
      "    sample_throughput: 18.294\n",
      "    sample_time_ms: 218653.0\n",
      "    update_time_ms: 16.277\n",
      "  timestamp: 1650254277\n",
      "  timesteps_since_restore: 664000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_04-58-04\n",
      "  done: false\n",
      "  episode_len_mean: 484.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.2\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2219\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7662996649742126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012600962072610855\n",
      "          model: {}\n",
      "          policy_loss: -0.044799599796533585\n",
      "          total_loss: 0.02849975973367691\n",
      "          vf_explained_var: 0.7340896725654602\n",
      "          vf_loss: 0.0732993558049202\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.75259515570934\n",
      "    ram_util_percent: 95.87889273356402\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11471584789364173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0566829559138924\n",
      "    mean_inference_ms: 2.448130032833841\n",
      "    mean_raw_obs_processing_ms: 0.22347145390920198\n",
      "  time_since_restore: 35939.98323106766\n",
      "  time_this_iter_s: 217.2383782863617\n",
      "  time_total_s: 35939.98323106766\n",
      "  timers:\n",
      "    learn_throughput: 19.578\n",
      "    learn_time_ms: 204314.982\n",
      "    load_throughput: 2756554.228\n",
      "    load_time_ms: 1.451\n",
      "    sample_throughput: 18.326\n",
      "    sample_time_ms: 218264.14\n",
      "    update_time_ms: 8.58\n",
      "  timestamp: 1650254284\n",
      "  timesteps_since_restore: 664000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 04:58:57 (running for 10:00:16.90)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.2 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 668000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-01-35\n",
      "  done: false\n",
      "  episode_len_mean: 2203.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 320\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0223031486489103e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.735102420806745e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128191396594048\n",
      "          total_loss: -0.014125688001513481\n",
      "          vf_explained_var: -1.0959563923051974e-07\n",
      "          vf_loss: 2.5034055397554766e-06\n",
      "    num_agent_steps_sampled: 668000\n",
      "    num_agent_steps_trained: 668000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.1403448275862\n",
      "    ram_util_percent: 95.88241379310342\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11487201832829781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.033484861956552\n",
      "    mean_inference_ms: 2.4404425643174026\n",
      "    mean_raw_obs_processing_ms: 0.1407948199640906\n",
      "  time_since_restore: 36144.67873263359\n",
      "  time_this_iter_s: 218.10419583320618\n",
      "  time_total_s: 36144.67873263359\n",
      "  timers:\n",
      "    learn_throughput: 19.508\n",
      "    learn_time_ms: 205042.769\n",
      "    load_throughput: 6955439.658\n",
      "    load_time_ms: 0.575\n",
      "    sample_throughput: 18.294\n",
      "    sample_time_ms: 218655.16\n",
      "    update_time_ms: 16.418\n",
      "  timestamp: 1650254495\n",
      "  timesteps_since_restore: 668000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 668000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-01-42\n",
      "  done: false\n",
      "  episode_len_mean: 494.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.32\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2227\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7625946998596191\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010686208494007587\n",
      "          model: {}\n",
      "          policy_loss: -0.042689353227615356\n",
      "          total_loss: 0.02775442600250244\n",
      "          vf_explained_var: 0.7752995491027832\n",
      "          vf_loss: 0.0704437717795372\n",
      "    num_agent_steps_sampled: 668000\n",
      "    num_agent_steps_trained: 668000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.7045138888889\n",
      "    ram_util_percent: 95.90833333333333\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11467230730112947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.05627531855158\n",
      "    mean_inference_ms: 2.446568156317371\n",
      "    mean_raw_obs_processing_ms: 0.22319371590355705\n",
      "  time_since_restore: 36157.51099395752\n",
      "  time_this_iter_s: 217.52776288986206\n",
      "  time_total_s: 36157.51099395752\n",
      "  timers:\n",
      "    learn_throughput: 19.554\n",
      "    learn_time_ms: 204565.304\n",
      "    load_throughput: 2770346.103\n",
      "    load_time_ms: 1.444\n",
      "    sample_throughput: 18.329\n",
      "    sample_time_ms: 218229.855\n",
      "    update_time_ms: 7.964\n",
      "  timestamp: 1650254502\n",
      "  timesteps_since_restore: 668000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-05-16\n",
      "  done: false\n",
      "  episode_len_mean: 2302.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 336\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0227431699606973e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.792252495098279e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0009125221404246986\n",
      "          total_loss: 0.0010336816776543856\n",
      "          vf_explained_var: 0.4077196419239044\n",
      "          vf_loss: 0.0019462060881778598\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.21575342465754\n",
      "    ram_util_percent: 95.9527397260274\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11517004307565677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0352791718988434\n",
      "    mean_inference_ms: 2.447698981474711\n",
      "    mean_raw_obs_processing_ms: 0.140850390395359\n",
      "  time_since_restore: 36364.91340446472\n",
      "  time_this_iter_s: 220.23467183113098\n",
      "  time_total_s: 36364.91340446472\n",
      "  timers:\n",
      "    learn_throughput: 19.481\n",
      "    learn_time_ms: 205328.952\n",
      "    load_throughput: 6793220.229\n",
      "    load_time_ms: 0.589\n",
      "    sample_throughput: 18.293\n",
      "    sample_time_ms: 218663.322\n",
      "    update_time_ms: 16.201\n",
      "  timestamp: 1650254716\n",
      "  timesteps_since_restore: 672000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-05-20\n",
      "  done: false\n",
      "  episode_len_mean: 504.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.51\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2235\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7207726836204529\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012310881167650223\n",
      "          model: {}\n",
      "          policy_loss: -0.037775035947561264\n",
      "          total_loss: 0.016223669052124023\n",
      "          vf_explained_var: 0.7936108708381653\n",
      "          vf_loss: 0.05399870499968529\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.39655172413794\n",
      "    ram_util_percent: 95.97413793103448\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11461886615829923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0558019346445398\n",
      "    mean_inference_ms: 2.4446932397204266\n",
      "    mean_raw_obs_processing_ms: 0.22289403300150248\n",
      "  time_since_restore: 36375.63621997833\n",
      "  time_this_iter_s: 218.125226020813\n",
      "  time_total_s: 36375.63621997833\n",
      "  timers:\n",
      "    learn_throughput: 19.536\n",
      "    learn_time_ms: 204748.616\n",
      "    load_throughput: 2780354.645\n",
      "    load_time_ms: 1.439\n",
      "    sample_throughput: 18.331\n",
      "    sample_time_ms: 218213.292\n",
      "    update_time_ms: 8.404\n",
      "  timestamp: 1650254720\n",
      "  timesteps_since_restore: 672000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 676000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 2205.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 340\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.491631343755282e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.5723993376929162e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0005434658960439265\n",
      "          total_loss: 0.0006655041943304241\n",
      "          vf_explained_var: 0.1314268261194229\n",
      "          vf_loss: 0.00012203794904053211\n",
      "    num_agent_steps_sampled: 676000\n",
      "    num_agent_steps_trained: 676000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.09206896551723\n",
      "    ram_util_percent: 95.9510344827586\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11522823196756313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0356174022623599\n",
      "    mean_inference_ms: 2.4490197050042295\n",
      "    mean_raw_obs_processing_ms: 0.1408423147108646\n",
      "  time_since_restore: 36583.85770750046\n",
      "  time_this_iter_s: 218.94430303573608\n",
      "  time_total_s: 36583.85770750046\n",
      "  timers:\n",
      "    learn_throughput: 19.461\n",
      "    learn_time_ms: 205534.692\n",
      "    load_throughput: 6943062.407\n",
      "    load_time_ms: 0.576\n",
      "    sample_throughput: 18.293\n",
      "    sample_time_ms: 218664.815\n",
      "    update_time_ms: 16.075\n",
      "  timestamp: 1650254935\n",
      "  timesteps_since_restore: 676000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 676000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-08-59\n",
      "  done: false\n",
      "  episode_len_mean: 510.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.67\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2242\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7445166110992432\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012615601532161236\n",
      "          model: {}\n",
      "          policy_loss: -0.042969390749931335\n",
      "          total_loss: 0.010959144681692123\n",
      "          vf_explained_var: 0.8271706104278564\n",
      "          vf_loss: 0.05392852798104286\n",
      "    num_agent_steps_sampled: 676000\n",
      "    num_agent_steps_trained: 676000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.58304498269896\n",
      "    ram_util_percent: 95.96851211072665\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11456075336017556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0553118555282337\n",
      "    mean_inference_ms: 2.4427872654091094\n",
      "    mean_raw_obs_processing_ms: 0.22261569151114116\n",
      "  time_since_restore: 36594.53679704666\n",
      "  time_this_iter_s: 218.90057706832886\n",
      "  time_total_s: 36594.53679704666\n",
      "  timers:\n",
      "    learn_throughput: 19.511\n",
      "    learn_time_ms: 205015.331\n",
      "    load_throughput: 2783075.826\n",
      "    load_time_ms: 1.437\n",
      "    sample_throughput: 18.342\n",
      "    sample_time_ms: 218078.145\n",
      "    update_time_ms: 8.286\n",
      "  timestamp: 1650254939\n",
      "  timesteps_since_restore: 676000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-12-35\n",
      "  done: false\n",
      "  episode_len_mean: 2205.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 340\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128051698207855\n",
      "          total_loss: -0.01412780862301588\n",
      "          vf_explained_var: -4.886298938799882e-07\n",
      "          vf_loss: 2.464274757585372e-07\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.08453608247422\n",
      "    ram_util_percent: 95.84845360824742\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11522823196756313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0356174022623599\n",
      "    mean_inference_ms: 2.4490197050042295\n",
      "    mean_raw_obs_processing_ms: 0.1408423147108646\n",
      "  time_since_restore: 36803.86070275307\n",
      "  time_this_iter_s: 220.00299525260925\n",
      "  time_total_s: 36803.86070275307\n",
      "  timers:\n",
      "    learn_throughput: 19.412\n",
      "    learn_time_ms: 206063.157\n",
      "    load_throughput: 6876190.008\n",
      "    load_time_ms: 0.582\n",
      "    sample_throughput: 18.299\n",
      "    sample_time_ms: 218588.573\n",
      "    update_time_ms: 15.343\n",
      "  timestamp: 1650255155\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 517.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.79\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2251\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.775180995464325\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011794410645961761\n",
      "          model: {}\n",
      "          policy_loss: -0.045966487377882004\n",
      "          total_loss: 0.05681174620985985\n",
      "          vf_explained_var: 0.6551216244697571\n",
      "          vf_loss: 0.10277823358774185\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.47020547945205\n",
      "    ram_util_percent: 95.8541095890411\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11447784259204574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0546179191011873\n",
      "    mean_inference_ms: 2.4400450332483046\n",
      "    mean_raw_obs_processing_ms: 0.22224034230868825\n",
      "  time_since_restore: 36814.666357040405\n",
      "  time_this_iter_s: 220.1295599937439\n",
      "  time_total_s: 36814.666357040405\n",
      "  timers:\n",
      "    learn_throughput: 19.459\n",
      "    learn_time_ms: 205560.799\n",
      "    load_throughput: 2787885.475\n",
      "    load_time_ms: 1.435\n",
      "    sample_throughput: 18.344\n",
      "    sample_time_ms: 218056.741\n",
      "    update_time_ms: 9.651\n",
      "  timestamp: 1650255159\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 05:15:37 (running for 10:16:57.41)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.79 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 684000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-16-14\n",
      "  done: false\n",
      "  episode_len_mean: 2107.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 342\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.698339869688685e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1456458215061626e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.00011272654955973849\n",
      "          total_loss: 0.00013826973736286163\n",
      "          vf_explained_var: -0.06451661884784698\n",
      "          vf_loss: 2.5542010916979052e-05\n",
      "    num_agent_steps_sampled: 684000\n",
      "    num_agent_steps_trained: 684000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.48006872852234\n",
      "    ram_util_percent: 95.83539518900342\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11523957572791818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.035664115945217\n",
      "    mean_inference_ms: 2.449196503446765\n",
      "    mean_raw_obs_processing_ms: 0.1408277954672893\n",
      "  time_since_restore: 37023.02334666252\n",
      "  time_this_iter_s: 219.16264390945435\n",
      "  time_total_s: 37023.02334666252\n",
      "  timers:\n",
      "    learn_throughput: 19.375\n",
      "    learn_time_ms: 206451.663\n",
      "    load_throughput: 3768551.854\n",
      "    load_time_ms: 1.061\n",
      "    sample_throughput: 18.277\n",
      "    sample_time_ms: 218855.772\n",
      "    update_time_ms: 16.107\n",
      "  timestamp: 1650255374\n",
      "  timesteps_since_restore: 684000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 684000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-16-17\n",
      "  done: false\n",
      "  episode_len_mean: 517.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.86\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2260\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7892863154411316\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011824790388345718\n",
      "          model: {}\n",
      "          policy_loss: -0.04524504020810127\n",
      "          total_loss: 0.012637490406632423\n",
      "          vf_explained_var: 0.7953898906707764\n",
      "          vf_loss: 0.05788252875208855\n",
      "    num_agent_steps_sampled: 684000\n",
      "    num_agent_steps_trained: 684000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.44740484429067\n",
      "    ram_util_percent: 95.82941176470587\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11439120039480107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.05387873821862\n",
      "    mean_inference_ms: 2.437008997181579\n",
      "    mean_raw_obs_processing_ms: 0.221858984055428\n",
      "  time_since_restore: 37033.260342121124\n",
      "  time_this_iter_s: 218.593985080719\n",
      "  time_total_s: 37033.260342121124\n",
      "  timers:\n",
      "    learn_throughput: 19.431\n",
      "    learn_time_ms: 205854.875\n",
      "    load_throughput: 2787329.667\n",
      "    load_time_ms: 1.435\n",
      "    sample_throughput: 18.32\n",
      "    sample_time_ms: 218344.87\n",
      "    update_time_ms: 9.499\n",
      "  timestamp: 1650255377\n",
      "  timesteps_since_restore: 684000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-19-55\n",
      "  done: false\n",
      "  episode_len_mean: 2107.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 342\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.01412825845181942\n",
      "          total_loss: -0.014128143899142742\n",
      "          vf_explained_var: 3.9467246892854746e-07\n",
      "          vf_loss: 1.1738477923017854e-07\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.41780821917808\n",
      "    ram_util_percent: 95.93082191780822\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11523957572791818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.035664115945217\n",
      "    mean_inference_ms: 2.449196503446765\n",
      "    mean_raw_obs_processing_ms: 0.1408277954672893\n",
      "  time_since_restore: 37243.30700945854\n",
      "  time_this_iter_s: 220.2836627960205\n",
      "  time_total_s: 37243.30700945854\n",
      "  timers:\n",
      "    learn_throughput: 19.353\n",
      "    learn_time_ms: 206691.566\n",
      "    load_throughput: 2768700.244\n",
      "    load_time_ms: 1.445\n",
      "    sample_throughput: 18.273\n",
      "    sample_time_ms: 218906.991\n",
      "    update_time_ms: 16.444\n",
      "  timestamp: 1650255595\n",
      "  timesteps_since_restore: 688000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 514.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.87\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2268\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7639928460121155\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010199104435741901\n",
      "          model: {}\n",
      "          policy_loss: -0.043881066143512726\n",
      "          total_loss: 0.022087011486291885\n",
      "          vf_explained_var: 0.7776902914047241\n",
      "          vf_loss: 0.06596808135509491\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.6308219178082\n",
      "    ram_util_percent: 95.93390410958904\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1143083638496582\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0531809343326766\n",
      "    mean_inference_ms: 2.4340700477200277\n",
      "    mean_raw_obs_processing_ms: 0.22151570960374442\n",
      "  time_since_restore: 37254.02588915825\n",
      "  time_this_iter_s: 220.76554703712463\n",
      "  time_total_s: 37254.02588915825\n",
      "  timers:\n",
      "    learn_throughput: 19.401\n",
      "    learn_time_ms: 206177.288\n",
      "    load_throughput: 6284543.003\n",
      "    load_time_ms: 0.636\n",
      "    sample_throughput: 18.318\n",
      "    sample_time_ms: 218370.053\n",
      "    update_time_ms: 9.848\n",
      "  timestamp: 1650255598\n",
      "  timesteps_since_restore: 688000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 692000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-23-34\n",
      "  done: false\n",
      "  episode_len_mean: 2107.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 342\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128097333014011\n",
      "          total_loss: -0.01412803865969181\n",
      "          vf_explained_var: 1.8248634887640947e-06\n",
      "          vf_loss: 6.042775169134984e-08\n",
      "    num_agent_steps_sampled: 692000\n",
      "    num_agent_steps_trained: 692000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.87413793103448\n",
      "    ram_util_percent: 95.98172413793104\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11523957572791818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.035664115945217\n",
      "    mean_inference_ms: 2.449196503446765\n",
      "    mean_raw_obs_processing_ms: 0.1408277954672893\n",
      "  time_since_restore: 37462.41559553146\n",
      "  time_this_iter_s: 219.10858607292175\n",
      "  time_total_s: 37462.41559553146\n",
      "  timers:\n",
      "    learn_throughput: 19.33\n",
      "    learn_time_ms: 206934.073\n",
      "    load_throughput: 2750678.93\n",
      "    load_time_ms: 1.454\n",
      "    sample_throughput: 18.273\n",
      "    sample_time_ms: 218904.208\n",
      "    update_time_ms: 16.552\n",
      "  timestamp: 1650255814\n",
      "  timesteps_since_restore: 692000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 692000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-23-37\n",
      "  done: false\n",
      "  episode_len_mean: 513.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.88\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2275\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7307218909263611\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014910759404301643\n",
      "          model: {}\n",
      "          policy_loss: -0.04136382043361664\n",
      "          total_loss: 0.061231207102537155\n",
      "          vf_explained_var: 0.6562459468841553\n",
      "          vf_loss: 0.10259503126144409\n",
      "    num_agent_steps_sampled: 692000\n",
      "    num_agent_steps_trained: 692000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.13425605536332\n",
      "    ram_util_percent: 95.96920415224913\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11423110340677729\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0525308581456745\n",
      "    mean_inference_ms: 2.4312774149048284\n",
      "    mean_raw_obs_processing_ms: 0.2212112362972282\n",
      "  time_since_restore: 37472.62905430794\n",
      "  time_this_iter_s: 218.60316514968872\n",
      "  time_total_s: 37472.62905430794\n",
      "  timers:\n",
      "    learn_throughput: 19.389\n",
      "    learn_time_ms: 206303.592\n",
      "    load_throughput: 6172859.929\n",
      "    load_time_ms: 0.648\n",
      "    sample_throughput: 18.309\n",
      "    sample_time_ms: 218471.552\n",
      "    update_time_ms: 9.58\n",
      "  timestamp: 1650255817\n",
      "  timesteps_since_restore: 692000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-27-13\n",
      "  done: false\n",
      "  episode_len_mean: 2204.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 343\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.364301073885044e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.1500528772059073e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0004805380303878337\n",
      "          total_loss: 0.000522697577252984\n",
      "          vf_explained_var: -0.0645170733332634\n",
      "          vf_loss: 4.2161384044447914e-05\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.58546712802769\n",
      "    ram_util_percent: 95.95363321799307\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11523930254692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0356459969786425\n",
      "    mean_inference_ms: 2.44910480529835\n",
      "    mean_raw_obs_processing_ms: 0.1408122998734215\n",
      "  time_since_restore: 37681.120352745056\n",
      "  time_this_iter_s: 218.70475721359253\n",
      "  time_total_s: 37681.120352745056\n",
      "  timers:\n",
      "    learn_throughput: 19.29\n",
      "    learn_time_ms: 207363.469\n",
      "    load_throughput: 2762045.372\n",
      "    load_time_ms: 1.448\n",
      "    sample_throughput: 18.277\n",
      "    sample_time_ms: 218851.783\n",
      "    update_time_ms: 16.405\n",
      "  timestamp: 1650256033\n",
      "  timesteps_since_restore: 696000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-27-15\n",
      "  done: false\n",
      "  episode_len_mean: 516.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.0\n",
      "  episode_reward_mean: 6.98\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2283\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7815793752670288\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011058828793466091\n",
      "          model: {}\n",
      "          policy_loss: -0.04332083836197853\n",
      "          total_loss: 0.035252321511507034\n",
      "          vf_explained_var: 0.7467896938323975\n",
      "          vf_loss: 0.07857316732406616\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.18996539792388\n",
      "    ram_util_percent: 95.92768166089965\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11413424524754008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0517277981603137\n",
      "    mean_inference_ms: 2.427860189585863\n",
      "    mean_raw_obs_processing_ms: 0.2208583198863444\n",
      "  time_since_restore: 37690.77664756775\n",
      "  time_this_iter_s: 218.1475932598114\n",
      "  time_total_s: 37690.77664756775\n",
      "  timers:\n",
      "    learn_throughput: 19.349\n",
      "    learn_time_ms: 206725.327\n",
      "    load_throughput: 6529369.916\n",
      "    load_time_ms: 0.613\n",
      "    sample_throughput: 18.324\n",
      "    sample_time_ms: 218296.606\n",
      "    update_time_ms: 9.74\n",
      "  timestamp: 1650256035\n",
      "  timesteps_since_restore: 696000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-30-52\n",
      "  done: false\n",
      "  episode_len_mean: 2204.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 343\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128150418400764\n",
      "          total_loss: 0.014128168113529682\n",
      "          vf_explained_var: 5.5281707318499684e-06\n",
      "          vf_loss: 2.6799140684374834e-08\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_agent_steps_trained: 700000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.45360824742268\n",
      "    ram_util_percent: 95.9594501718213\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11523930254692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0356459969786425\n",
      "    mean_inference_ms: 2.44910480529835\n",
      "    mean_raw_obs_processing_ms: 0.1408122998734215\n",
      "  time_since_restore: 37900.83581352234\n",
      "  time_this_iter_s: 219.71546077728271\n",
      "  time_total_s: 37900.83581352234\n",
      "  timers:\n",
      "    learn_throughput: 19.255\n",
      "    learn_time_ms: 207739.134\n",
      "    load_throughput: 2734405.111\n",
      "    load_time_ms: 1.463\n",
      "    sample_throughput: 18.267\n",
      "    sample_time_ms: 218971.486\n",
      "    update_time_ms: 15.953\n",
      "  timestamp: 1650256252\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-30-55\n",
      "  done: false\n",
      "  episode_len_mean: 512.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 6.71\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2292\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7619670033454895\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009752069599926472\n",
      "          model: {}\n",
      "          policy_loss: -0.04183928668498993\n",
      "          total_loss: 0.039180874824523926\n",
      "          vf_explained_var: 0.6976755261421204\n",
      "          vf_loss: 0.08102016150951385\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_agent_steps_trained: 700000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.62989690721649\n",
      "    ram_util_percent: 95.98281786941581\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11401681605898747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.05076185581675\n",
      "    mean_inference_ms: 2.423770039869816\n",
      "    mean_raw_obs_processing_ms: 0.2204583952088954\n",
      "  time_since_restore: 37910.60091853142\n",
      "  time_this_iter_s: 219.82427096366882\n",
      "  time_total_s: 37910.60091853142\n",
      "  timers:\n",
      "    learn_throughput: 19.31\n",
      "    learn_time_ms: 207141.484\n",
      "    load_throughput: 6653401.015\n",
      "    load_time_ms: 0.601\n",
      "    sample_throughput: 18.313\n",
      "    sample_time_ms: 218421.765\n",
      "    update_time_ms: 10.022\n",
      "  timestamp: 1650256255\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 05:32:18 (running for 10:33:38.21)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.71 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-34-32\n",
      "  done: false\n",
      "  episode_len_mean: 2302.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 346\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.046210093531962e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1960466377787988e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.001546122133731842\n",
      "          total_loss: -0.0012304279953241348\n",
      "          vf_explained_var: 0.06706368178129196\n",
      "          vf_loss: 0.0003156957682222128\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.10655172413793\n",
      "    ram_util_percent: 95.92482758620689\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11522834618898604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0355156040263214\n",
      "    mean_inference_ms: 2.4484899038948273\n",
      "    mean_raw_obs_processing_ms: 0.1407485429658501\n",
      "  time_since_restore: 38120.231350660324\n",
      "  time_this_iter_s: 219.39553713798523\n",
      "  time_total_s: 38120.231350660324\n",
      "  timers:\n",
      "    learn_throughput: 19.217\n",
      "    learn_time_ms: 208146.378\n",
      "    load_throughput: 2723750.893\n",
      "    load_time_ms: 1.469\n",
      "    sample_throughput: 18.26\n",
      "    sample_time_ms: 219060.296\n",
      "    update_time_ms: 16.444\n",
      "  timestamp: 1650256472\n",
      "  timesteps_since_restore: 704000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-34-35\n",
      "  done: false\n",
      "  episode_len_mean: 516.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.86\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2300\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7288687229156494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01435289066284895\n",
      "          model: {}\n",
      "          policy_loss: -0.04146357625722885\n",
      "          total_loss: 0.038762908428907394\n",
      "          vf_explained_var: 0.7308180928230286\n",
      "          vf_loss: 0.08022649586200714\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.55620689655173\n",
      "    ram_util_percent: 95.91724137931034\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11390099781584728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.049839206425938\n",
      "    mean_inference_ms: 2.4198741462358146\n",
      "    mean_raw_obs_processing_ms: 0.22010144265763046\n",
      "  time_since_restore: 38130.21465277672\n",
      "  time_this_iter_s: 219.6137342453003\n",
      "  time_total_s: 38130.21465277672\n",
      "  timers:\n",
      "    learn_throughput: 19.266\n",
      "    learn_time_ms: 207616.933\n",
      "    load_throughput: 6548995.238\n",
      "    load_time_ms: 0.611\n",
      "    sample_throughput: 18.298\n",
      "    sample_time_ms: 218598.471\n",
      "    update_time_ms: 9.904\n",
      "  timestamp: 1650256475\n",
      "  timesteps_since_restore: 704000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 708000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-38-12\n",
      "  done: false\n",
      "  episode_len_mean: 2302.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 346\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.01412811316549778\n",
      "          total_loss: -0.014126548543572426\n",
      "          vf_explained_var: 5.2746905510048236e-08\n",
      "          vf_loss: 1.5702047448939993e-06\n",
      "    num_agent_steps_sampled: 708000\n",
      "    num_agent_steps_trained: 708000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.16883561643836\n",
      "    ram_util_percent: 95.79212328767123\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11522834618898604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0355156040263214\n",
      "    mean_inference_ms: 2.4484899038948273\n",
      "    mean_raw_obs_processing_ms: 0.1407485429658501\n",
      "  time_since_restore: 38340.13538789749\n",
      "  time_this_iter_s: 219.90403723716736\n",
      "  time_total_s: 38340.13538789749\n",
      "  timers:\n",
      "    learn_throughput: 19.182\n",
      "    learn_time_ms: 208526.175\n",
      "    load_throughput: 2760227.699\n",
      "    load_time_ms: 1.449\n",
      "    sample_throughput: 18.243\n",
      "    sample_time_ms: 219262.286\n",
      "    update_time_ms: 15.718\n",
      "  timestamp: 1650256692\n",
      "  timesteps_since_restore: 708000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 708000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-38-15\n",
      "  done: false\n",
      "  episode_len_mean: 520.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.92\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2309\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7868096232414246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010733050294220448\n",
      "          model: {}\n",
      "          policy_loss: -0.03943758085370064\n",
      "          total_loss: 0.0404035784304142\n",
      "          vf_explained_var: 0.7068551778793335\n",
      "          vf_loss: 0.07984115183353424\n",
      "    num_agent_steps_sampled: 708000\n",
      "    num_agent_steps_trained: 708000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.16484641638225\n",
      "    ram_util_percent: 95.80955631399317\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11376161837708942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0487563603916623\n",
      "    mean_inference_ms: 2.415238345403811\n",
      "    mean_raw_obs_processing_ms: 0.21969309564115838\n",
      "  time_since_restore: 38350.34250879288\n",
      "  time_this_iter_s: 220.12785601615906\n",
      "  time_total_s: 38350.34250879288\n",
      "  timers:\n",
      "    learn_throughput: 19.225\n",
      "    learn_time_ms: 208059.863\n",
      "    load_throughput: 5705177.679\n",
      "    load_time_ms: 0.701\n",
      "    sample_throughput: 18.274\n",
      "    sample_time_ms: 218891.104\n",
      "    update_time_ms: 10.266\n",
      "  timestamp: 1650256695\n",
      "  timesteps_since_restore: 708000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-41-51\n",
      "  done: false\n",
      "  episode_len_mean: 2302.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 346\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128098264336586\n",
      "          total_loss: -0.01412762887775898\n",
      "          vf_explained_var: 2.8840958066211897e-07\n",
      "          vf_loss: 4.729746763132425e-07\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.92405498281788\n",
      "    ram_util_percent: 95.66494845360825\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11522834618898604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0355156040263214\n",
      "    mean_inference_ms: 2.4484899038948273\n",
      "    mean_raw_obs_processing_ms: 0.1407485429658501\n",
      "  time_since_restore: 38558.87661910057\n",
      "  time_this_iter_s: 218.74123120307922\n",
      "  time_total_s: 38558.87661910057\n",
      "  timers:\n",
      "    learn_throughput: 19.177\n",
      "    learn_time_ms: 208582.017\n",
      "    load_throughput: 2773276.911\n",
      "    load_time_ms: 1.442\n",
      "    sample_throughput: 18.228\n",
      "    sample_time_ms: 219438.456\n",
      "    update_time_ms: 15.562\n",
      "  timestamp: 1650256911\n",
      "  timesteps_since_restore: 712000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-41-54\n",
      "  done: false\n",
      "  episode_len_mean: 524.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.99\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2317\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7968546748161316\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010984892025589943\n",
      "          model: {}\n",
      "          policy_loss: -0.04512430354952812\n",
      "          total_loss: 0.04938080161809921\n",
      "          vf_explained_var: 0.6733463406562805\n",
      "          vf_loss: 0.09450509399175644\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.32439862542955\n",
      "    ram_util_percent: 95.68625429553266\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11363344161805818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0477551380219179\n",
      "    mean_inference_ms: 2.41096749693723\n",
      "    mean_raw_obs_processing_ms: 0.219320343661155\n",
      "  time_since_restore: 38569.242491960526\n",
      "  time_this_iter_s: 218.89998316764832\n",
      "  time_total_s: 38569.242491960526\n",
      "  timers:\n",
      "    learn_throughput: 19.215\n",
      "    learn_time_ms: 208174.605\n",
      "    load_throughput: 5696267.273\n",
      "    load_time_ms: 0.702\n",
      "    sample_throughput: 18.24\n",
      "    sample_time_ms: 219299.184\n",
      "    update_time_ms: 9.8\n",
      "  timestamp: 1650256914\n",
      "  timesteps_since_restore: 712000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 716000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-45-35\n",
      "  done: false\n",
      "  episode_len_mean: 2400.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 351\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6187872888874467e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.6568954423048218e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0013237000675871968\n",
      "          total_loss: 0.0014824685640633106\n",
      "          vf_explained_var: 0.021694663912057877\n",
      "          vf_loss: 0.0001587688602739945\n",
      "    num_agent_steps_sampled: 716000\n",
      "    num_agent_steps_trained: 716000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.62885906040269\n",
      "    ram_util_percent: 95.62080536912751\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1151847823859631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0351148950233178\n",
      "    mean_inference_ms: 2.446652935890865\n",
      "    mean_raw_obs_processing_ms: 0.1405959864310457\n",
      "  time_since_restore: 38782.7913172245\n",
      "  time_this_iter_s: 223.91469812393188\n",
      "  time_total_s: 38782.7913172245\n",
      "  timers:\n",
      "    learn_throughput: 19.129\n",
      "    learn_time_ms: 209110.591\n",
      "    load_throughput: 2749101.396\n",
      "    load_time_ms: 1.455\n",
      "    sample_throughput: 18.226\n",
      "    sample_time_ms: 219462.045\n",
      "    update_time_ms: 19.294\n",
      "  timestamp: 1650257135\n",
      "  timesteps_since_restore: 716000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 716000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-45-38\n",
      "  done: false\n",
      "  episode_len_mean: 517.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.9\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2326\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7975320816040039\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010190739296376705\n",
      "          model: {}\n",
      "          policy_loss: -0.04263760894536972\n",
      "          total_loss: 0.04666780307888985\n",
      "          vf_explained_var: 0.7451654672622681\n",
      "          vf_loss: 0.08930541574954987\n",
      "    num_agent_steps_sampled: 716000\n",
      "    num_agent_steps_trained: 716000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.55521885521885\n",
      "    ram_util_percent: 95.61885521885522\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11348381968070077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0466043595272057\n",
      "    mean_inference_ms: 2.406047846282672\n",
      "    mean_raw_obs_processing_ms: 0.21889330904148402\n",
      "  time_since_restore: 38793.37592291832\n",
      "  time_this_iter_s: 224.1334309577942\n",
      "  time_total_s: 38793.37592291832\n",
      "  timers:\n",
      "    learn_throughput: 19.164\n",
      "    learn_time_ms: 208728.262\n",
      "    load_throughput: 5747787.18\n",
      "    load_time_ms: 0.696\n",
      "    sample_throughput: 18.233\n",
      "    sample_time_ms: 219384.112\n",
      "    update_time_ms: 10.232\n",
      "  timestamp: 1650257138\n",
      "  timesteps_since_restore: 716000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 05:48:58 (running for 10:50:18.58)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.9 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-49-14\n",
      "  done: false\n",
      "  episode_len_mean: 2400.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 351\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128044247627258\n",
      "          total_loss: 0.014158369973301888\n",
      "          vf_explained_var: -8.20365020359759e-09\n",
      "          vf_loss: 3.0321536542032845e-05\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.88493150684931\n",
      "    ram_util_percent: 95.61027397260276\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1151847823859631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0351148950233178\n",
      "    mean_inference_ms: 2.446652935890865\n",
      "    mean_raw_obs_processing_ms: 0.1405959864310457\n",
      "  time_since_restore: 39002.483493328094\n",
      "  time_this_iter_s: 219.69217610359192\n",
      "  time_total_s: 39002.483493328094\n",
      "  timers:\n",
      "    learn_throughput: 19.13\n",
      "    learn_time_ms: 209094.69\n",
      "    load_throughput: 2756826.002\n",
      "    load_time_ms: 1.451\n",
      "    sample_throughput: 18.183\n",
      "    sample_time_ms: 219983.129\n",
      "    update_time_ms: 19.181\n",
      "  timestamp: 1650257354\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-49-18\n",
      "  done: false\n",
      "  episode_len_mean: 517.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.92\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2334\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7914637327194214\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0125016700476408\n",
      "          model: {}\n",
      "          policy_loss: -0.044068124145269394\n",
      "          total_loss: 0.03848421946167946\n",
      "          vf_explained_var: 0.7079411149024963\n",
      "          vf_loss: 0.08255234360694885\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.5791095890411\n",
      "    ram_util_percent: 95.62020547945205\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11335137535268588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0455723987620846\n",
      "    mean_inference_ms: 2.4016976376759325\n",
      "    mean_raw_obs_processing_ms: 0.21851708618507304\n",
      "  time_since_restore: 39013.054607868195\n",
      "  time_this_iter_s: 219.67868494987488\n",
      "  time_total_s: 39013.054607868195\n",
      "  timers:\n",
      "    learn_throughput: 19.166\n",
      "    learn_time_ms: 208704.997\n",
      "    load_throughput: 5875403.957\n",
      "    load_time_ms: 0.681\n",
      "    sample_throughput: 18.188\n",
      "    sample_time_ms: 219919.833\n",
      "    update_time_ms: 9.311\n",
      "  timestamp: 1650257358\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 724000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-52-55\n",
      "  done: false\n",
      "  episode_len_mean: 2400.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 351\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014127913862466812\n",
      "          total_loss: 0.014134010300040245\n",
      "          vf_explained_var: -4.781189844038636e-08\n",
      "          vf_loss: 6.0976467466389295e-06\n",
      "    num_agent_steps_sampled: 724000\n",
      "    num_agent_steps_trained: 724000\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.98213058419243\n",
      "    ram_util_percent: 95.81374570446734\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1151847823859631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0351148950233178\n",
      "    mean_inference_ms: 2.446652935890865\n",
      "    mean_raw_obs_processing_ms: 0.1405959864310457\n",
      "  time_since_restore: 39222.99676513672\n",
      "  time_this_iter_s: 220.51327180862427\n",
      "  time_total_s: 39222.99676513672\n",
      "  timers:\n",
      "    learn_throughput: 19.117\n",
      "    learn_time_ms: 209241.219\n",
      "    load_throughput: 4123179.159\n",
      "    load_time_ms: 0.97\n",
      "    sample_throughput: 18.186\n",
      "    sample_time_ms: 219950.341\n",
      "    update_time_ms: 19.07\n",
      "  timestamp: 1650257575\n",
      "  timesteps_since_restore: 724000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 724000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-52-58\n",
      "  done: false\n",
      "  episode_len_mean: 515.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.87\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2342\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7481761574745178\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012530962005257607\n",
      "          model: {}\n",
      "          policy_loss: -0.0494077205657959\n",
      "          total_loss: 0.023065632209181786\n",
      "          vf_explained_var: 0.7546723484992981\n",
      "          vf_loss: 0.07247336953878403\n",
      "    num_agent_steps_sampled: 724000\n",
      "    num_agent_steps_trained: 724000\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.36597938144328\n",
      "    ram_util_percent: 95.81993127147766\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11322136707649393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0445853892110972\n",
      "    mean_inference_ms: 2.397368805603362\n",
      "    mean_raw_obs_processing_ms: 0.2181417859345083\n",
      "  time_since_restore: 39233.43646192551\n",
      "  time_this_iter_s: 220.381854057312\n",
      "  time_total_s: 39233.43646192551\n",
      "  timers:\n",
      "    learn_throughput: 19.147\n",
      "    learn_time_ms: 208911.405\n",
      "    load_throughput: 5848572.823\n",
      "    load_time_ms: 0.684\n",
      "    sample_throughput: 18.193\n",
      "    sample_time_ms: 219866.185\n",
      "    update_time_ms: 9.316\n",
      "  timestamp: 1650257578\n",
      "  timesteps_since_restore: 724000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-56-36\n",
      "  done: false\n",
      "  episode_len_mean: 2208.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 362\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0361208130406965e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.6603386198593973e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0002689202665351331\n",
      "          total_loss: 0.0006384413572959602\n",
      "          vf_explained_var: 0.2547260522842407\n",
      "          vf_loss: 0.000907367910258472\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.83938356164383\n",
      "    ram_util_percent: 95.90273972602739\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11501782009334123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0337606644895272\n",
      "    mean_inference_ms: 2.4405548320028942\n",
      "    mean_raw_obs_processing_ms: 0.1402092097378209\n",
      "  time_since_restore: 39443.52079415321\n",
      "  time_this_iter_s: 220.52402901649475\n",
      "  time_total_s: 39443.52079415321\n",
      "  timers:\n",
      "    learn_throughput: 19.116\n",
      "    learn_time_ms: 209248.616\n",
      "    load_throughput: 6812805.977\n",
      "    load_time_ms: 0.587\n",
      "    sample_throughput: 18.172\n",
      "    sample_time_ms: 220115.319\n",
      "    update_time_ms: 19.35\n",
      "  timestamp: 1650257796\n",
      "  timesteps_since_restore: 728000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_05-56-37\n",
      "  done: false\n",
      "  episode_len_mean: 510.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.82\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2351\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7756738662719727\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01101335696876049\n",
      "          model: {}\n",
      "          policy_loss: -0.045133963227272034\n",
      "          total_loss: 0.025734366849064827\n",
      "          vf_explained_var: 0.7266134023666382\n",
      "          vf_loss: 0.07086832821369171\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.86931034482758\n",
      "    ram_util_percent: 95.89793103448275\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1130742795320306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.043486774378171\n",
      "    mean_inference_ms: 2.392526328766111\n",
      "    mean_raw_obs_processing_ms: 0.21772726799597072\n",
      "  time_since_restore: 39452.65271306038\n",
      "  time_this_iter_s: 219.21625113487244\n",
      "  time_total_s: 39452.65271306038\n",
      "  timers:\n",
      "    learn_throughput: 19.155\n",
      "    learn_time_ms: 208824.517\n",
      "    load_throughput: 5911633.545\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 18.181\n",
      "    sample_time_ms: 220005.652\n",
      "    update_time_ms: 8.859\n",
      "  timestamp: 1650257797\n",
      "  timesteps_since_restore: 728000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 732000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-00-16\n",
      "  done: false\n",
      "  episode_len_mean: 2208.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 362\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.006544685922563076\n",
      "          total_loss: -0.00654439115896821\n",
      "          vf_explained_var: -1.0148492037842516e-05\n",
      "          vf_loss: 2.977102155909961e-07\n",
      "    num_agent_steps_sampled: 732000\n",
      "    num_agent_steps_trained: 732000\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.22405498281789\n",
      "    ram_util_percent: 95.93024054982817\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11501782009334123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0337606644895272\n",
      "    mean_inference_ms: 2.4405548320028942\n",
      "    mean_raw_obs_processing_ms: 0.1402092097378209\n",
      "  time_since_restore: 39663.36868906021\n",
      "  time_this_iter_s: 219.84789490699768\n",
      "  time_total_s: 39663.36868906021\n",
      "  timers:\n",
      "    learn_throughput: 19.099\n",
      "    learn_time_ms: 209436.648\n",
      "    load_throughput: 6802585.249\n",
      "    load_time_ms: 0.588\n",
      "    sample_throughput: 18.182\n",
      "    sample_time_ms: 219998.605\n",
      "    update_time_ms: 18.013\n",
      "  timestamp: 1650258016\n",
      "  timesteps_since_restore: 732000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 732000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-00-17\n",
      "  done: false\n",
      "  episode_len_mean: 511.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.8\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2359\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7389550805091858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013086499646306038\n",
      "          model: {}\n",
      "          policy_loss: -0.04795917123556137\n",
      "          total_loss: 0.014860974624752998\n",
      "          vf_explained_var: 0.7809234261512756\n",
      "          vf_loss: 0.06282014399766922\n",
      "    num_agent_steps_sampled: 732000\n",
      "    num_agent_steps_trained: 732000\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.96185567010309\n",
      "    ram_util_percent: 95.91443298969072\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11293797698465535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0424961370668115\n",
      "    mean_inference_ms: 2.3881638691350204\n",
      "    mean_raw_obs_processing_ms: 0.21735381483815489\n",
      "  time_since_restore: 39672.187395095825\n",
      "  time_this_iter_s: 219.53468203544617\n",
      "  time_total_s: 39672.187395095825\n",
      "  timers:\n",
      "    learn_throughput: 19.137\n",
      "    learn_time_ms: 209017.484\n",
      "    load_throughput: 6031281.59\n",
      "    load_time_ms: 0.663\n",
      "    sample_throughput: 18.198\n",
      "    sample_time_ms: 219808.974\n",
      "    update_time_ms: 9.33\n",
      "  timestamp: 1650258017\n",
      "  timesteps_since_restore: 732000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-03-57\n",
      "  done: false\n",
      "  episode_len_mean: 2208.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 362\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0006484256009571254\n",
      "          total_loss: -0.0006484256009571254\n",
      "          vf_explained_var: -0.00031739010591991246\n",
      "          vf_loss: 4.125992603132289e-11\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_agent_steps_trained: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.92294520547945\n",
      "    ram_util_percent: 95.91575342465754\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11501782009334123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0337606644895272\n",
      "    mean_inference_ms: 2.4405548320028942\n",
      "    mean_raw_obs_processing_ms: 0.1402092097378209\n",
      "  time_since_restore: 39884.40164899826\n",
      "  time_this_iter_s: 221.03295993804932\n",
      "  time_total_s: 39884.40164899826\n",
      "  timers:\n",
      "    learn_throughput: 19.069\n",
      "    learn_time_ms: 209764.38\n",
      "    load_throughput: 6742982.999\n",
      "    load_time_ms: 0.593\n",
      "    sample_throughput: 18.174\n",
      "    sample_time_ms: 220089.068\n",
      "    update_time_ms: 20.73\n",
      "  timestamp: 1650258237\n",
      "  timesteps_since_restore: 736000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-03-58\n",
      "  done: false\n",
      "  episode_len_mean: 519.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.9\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2367\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.753544807434082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012436321005225182\n",
      "          model: {}\n",
      "          policy_loss: -0.04741654172539711\n",
      "          total_loss: 0.04023097828030586\n",
      "          vf_explained_var: 0.7691596746444702\n",
      "          vf_loss: 0.08764752000570297\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_agent_steps_trained: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.95308219178084\n",
      "    ram_util_percent: 95.9376712328767\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11279920650880762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0414817403560186\n",
      "    mean_inference_ms: 2.3836871232618306\n",
      "    mean_raw_obs_processing_ms: 0.21697301296312702\n",
      "  time_since_restore: 39893.582257032394\n",
      "  time_this_iter_s: 221.3948619365692\n",
      "  time_total_s: 39893.582257032394\n",
      "  timers:\n",
      "    learn_throughput: 19.097\n",
      "    learn_time_ms: 209451.693\n",
      "    load_throughput: 5446087.126\n",
      "    load_time_ms: 0.734\n",
      "    sample_throughput: 18.191\n",
      "    sample_time_ms: 219891.37\n",
      "    update_time_ms: 10.277\n",
      "  timestamp: 1650258238\n",
      "  timesteps_since_restore: 736000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 06:05:39 (running for 11:06:58.78)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.9 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-07-36\n",
      "  done: false\n",
      "  episode_len_mean: 2208.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 364\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662232853184847e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.154940513492171e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.004571001511067152\n",
      "          total_loss: 0.004688506480306387\n",
      "          vf_explained_var: -0.07188404351472855\n",
      "          vf_loss: 0.00011750624980777502\n",
      "    num_agent_steps_sampled: 740000\n",
      "    num_agent_steps_trained: 740000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.066323024055\n",
      "    ram_util_percent: 95.89450171821305\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11497322739196364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.033413620686186\n",
      "    mean_inference_ms: 2.439001685571496\n",
      "    mean_raw_obs_processing_ms: 0.1401299300452437\n",
      "  time_since_restore: 40103.92151880264\n",
      "  time_this_iter_s: 219.51986980438232\n",
      "  time_total_s: 40103.92151880264\n",
      "  timers:\n",
      "    learn_throughput: 19.067\n",
      "    learn_time_ms: 209787.114\n",
      "    load_throughput: 6913308.06\n",
      "    load_time_ms: 0.579\n",
      "    sample_throughput: 18.15\n",
      "    sample_time_ms: 220382.634\n",
      "    update_time_ms: 21.262\n",
      "  timestamp: 1650258456\n",
      "  timesteps_since_restore: 740000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-07-38\n",
      "  done: false\n",
      "  episode_len_mean: 506.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.63\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2376\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8408021926879883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01445010770112276\n",
      "          model: {}\n",
      "          policy_loss: -0.0366068072617054\n",
      "          total_loss: 0.07426182925701141\n",
      "          vf_explained_var: 0.7024815678596497\n",
      "          vf_loss: 0.11086864024400711\n",
      "    num_agent_steps_sampled: 740000\n",
      "    num_agent_steps_trained: 740000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.64501718213059\n",
      "    ram_util_percent: 95.92852233676975\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1126432136539649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0403429957138952\n",
      "    mean_inference_ms: 2.3786652955339447\n",
      "    mean_raw_obs_processing_ms: 0.2165568780419601\n",
      "  time_since_restore: 40113.075289011\n",
      "  time_this_iter_s: 219.49303197860718\n",
      "  time_total_s: 40113.075289011\n",
      "  timers:\n",
      "    learn_throughput: 19.098\n",
      "    learn_time_ms: 209450.363\n",
      "    load_throughput: 5325086.015\n",
      "    load_time_ms: 0.751\n",
      "    sample_throughput: 18.158\n",
      "    sample_time_ms: 220294.197\n",
      "    update_time_ms: 9.891\n",
      "  timestamp: 1650258458\n",
      "  timesteps_since_restore: 740000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-11-17\n",
      "  done: false\n",
      "  episode_len_mean: 2208.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 364\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128096401691437\n",
      "          total_loss: 0.014143773354589939\n",
      "          vf_explained_var: 2.9417776659101946e-08\n",
      "          vf_loss: 1.5680614524171688e-05\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.04075342465754\n",
      "    ram_util_percent: 95.8417808219178\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11497322739196364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.033413620686186\n",
      "    mean_inference_ms: 2.439001685571496\n",
      "    mean_raw_obs_processing_ms: 0.1401299300452437\n",
      "  time_since_restore: 40324.577989816666\n",
      "  time_this_iter_s: 220.65647101402283\n",
      "  time_total_s: 40324.577989816666\n",
      "  timers:\n",
      "    learn_throughput: 19.05\n",
      "    learn_time_ms: 209975.989\n",
      "    load_throughput: 7036537.348\n",
      "    load_time_ms: 0.568\n",
      "    sample_throughput: 18.153\n",
      "    sample_time_ms: 220346.23\n",
      "    update_time_ms: 20.375\n",
      "  timestamp: 1650258677\n",
      "  timesteps_since_restore: 744000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-11-18\n",
      "  done: false\n",
      "  episode_len_mean: 508.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.65\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2384\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7347678542137146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013967075385153294\n",
      "          model: {}\n",
      "          policy_loss: -0.03981011360883713\n",
      "          total_loss: 0.032130636274814606\n",
      "          vf_explained_var: 0.6672537922859192\n",
      "          vf_loss: 0.07194074988365173\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.36130136986301\n",
      "    ram_util_percent: 95.86986301369862\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11250540608472571\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0393356437936978\n",
      "    mean_inference_ms: 2.374201086600414\n",
      "    mean_raw_obs_processing_ms: 0.21619081058883716\n",
      "  time_since_restore: 40333.35391139984\n",
      "  time_this_iter_s: 220.27862238883972\n",
      "  time_total_s: 40333.35391139984\n",
      "  timers:\n",
      "    learn_throughput: 19.085\n",
      "    learn_time_ms: 209592.982\n",
      "    load_throughput: 5391309.489\n",
      "    load_time_ms: 0.742\n",
      "    sample_throughput: 18.164\n",
      "    sample_time_ms: 220216.46\n",
      "    update_time_ms: 9.587\n",
      "  timestamp: 1650258678\n",
      "  timesteps_since_restore: 744000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 748000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-14-56\n",
      "  done: false\n",
      "  episode_len_mean: 2015.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 368\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.938484035913708e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1856168026835686e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.006772616412490606\n",
      "          total_loss: -0.00623450567945838\n",
      "          vf_explained_var: 0.22369709610939026\n",
      "          vf_loss: 0.0005381118971854448\n",
      "    num_agent_steps_sampled: 748000\n",
      "    num_agent_steps_trained: 748000\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.0041237113402\n",
      "    ram_util_percent: 95.56563573883162\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.114868402503661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0325610578868112\n",
      "    mean_inference_ms: 2.4352537987427674\n",
      "    mean_raw_obs_processing_ms: 0.13995979390809143\n",
      "  time_since_restore: 40543.23605775833\n",
      "  time_this_iter_s: 218.65806794166565\n",
      "  time_total_s: 40543.23605775833\n",
      "  timers:\n",
      "    learn_throughput: 19.052\n",
      "    learn_time_ms: 209955.099\n",
      "    load_throughput: 6666090.273\n",
      "    load_time_ms: 0.6\n",
      "    sample_throughput: 18.146\n",
      "    sample_time_ms: 220435.286\n",
      "    update_time_ms: 20.347\n",
      "  timestamp: 1650258896\n",
      "  timesteps_since_restore: 748000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 748000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-14-58\n",
      "  done: false\n",
      "  episode_len_mean: 506.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.61\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2393\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8469190001487732\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010263100266456604\n",
      "          model: {}\n",
      "          policy_loss: -0.0444575659930706\n",
      "          total_loss: 0.01604636386036873\n",
      "          vf_explained_var: 0.7997899651527405\n",
      "          vf_loss: 0.06050393730401993\n",
      "    num_agent_steps_sampled: 748000\n",
      "    num_agent_steps_trained: 748000\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.13938356164383\n",
      "    ram_util_percent: 95.56027397260273\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11234951433971899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0381961919280336\n",
      "    mean_inference_ms: 2.369115030937088\n",
      "    mean_raw_obs_processing_ms: 0.21577438012720657\n",
      "  time_since_restore: 40552.89493060112\n",
      "  time_this_iter_s: 219.5410192012787\n",
      "  time_total_s: 40552.89493060112\n",
      "  timers:\n",
      "    learn_throughput: 19.078\n",
      "    learn_time_ms: 209668.246\n",
      "    load_throughput: 6241988.243\n",
      "    load_time_ms: 0.641\n",
      "    sample_throughput: 18.163\n",
      "    sample_time_ms: 220225.161\n",
      "    update_time_ms: 9.284\n",
      "  timestamp: 1650258898\n",
      "  timesteps_since_restore: 748000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 2014.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 373\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4028945707277843e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.9124011226636933e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0010629019234329462\n",
      "          total_loss: 0.0012621836503967643\n",
      "          vf_explained_var: 0.09602905809879303\n",
      "          vf_loss: 0.00019928492838516831\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_agent_steps_trained: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.77577854671279\n",
      "    ram_util_percent: 95.5688581314879\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.114726919591833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0314016778443993\n",
      "    mean_inference_ms: 2.430153334068317\n",
      "    mean_raw_obs_processing_ms: 0.13974425453057193\n",
      "  time_since_restore: 40761.426043748856\n",
      "  time_this_iter_s: 218.1899859905243\n",
      "  time_total_s: 40761.426043748856\n",
      "  timers:\n",
      "    learn_throughput: 19.053\n",
      "    learn_time_ms: 209945.756\n",
      "    load_throughput: 6614317.366\n",
      "    load_time_ms: 0.605\n",
      "    sample_throughput: 18.152\n",
      "    sample_time_ms: 220367.348\n",
      "    update_time_ms: 20.66\n",
      "  timestamp: 1650259114\n",
      "  timesteps_since_restore: 752000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-18-35\n",
      "  done: false\n",
      "  episode_len_mean: 499.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.44\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2402\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.791854977607727\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011469818651676178\n",
      "          model: {}\n",
      "          policy_loss: -0.04573193937540054\n",
      "          total_loss: 0.03176136687397957\n",
      "          vf_explained_var: 0.7418899536132812\n",
      "          vf_loss: 0.07749330997467041\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_agent_steps_trained: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.6204152249135\n",
      "    ram_util_percent: 95.57231833910035\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11219619478855616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.037068867295475\n",
      "    mean_inference_ms: 2.364112866747122\n",
      "    mean_raw_obs_processing_ms: 0.2153602717121023\n",
      "  time_since_restore: 40770.56360459328\n",
      "  time_this_iter_s: 217.66867399215698\n",
      "  time_total_s: 40770.56360459328\n",
      "  timers:\n",
      "    learn_throughput: 19.081\n",
      "    learn_time_ms: 209637.768\n",
      "    load_throughput: 6196571.006\n",
      "    load_time_ms: 0.646\n",
      "    sample_throughput: 18.165\n",
      "    sample_time_ms: 220206.748\n",
      "    update_time_ms: 8.932\n",
      "  timestamp: 1650259115\n",
      "  timesteps_since_restore: 752000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 756000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-22-13\n",
      "  done: false\n",
      "  episode_len_mean: 2014.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 373\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.011180664412677288\n",
      "          total_loss: -0.011180647648870945\n",
      "          vf_explained_var: -3.6943984014214948e-06\n",
      "          vf_loss: 2.3878602206650612e-08\n",
      "    num_agent_steps_sampled: 756000\n",
      "    num_agent_steps_trained: 756000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.39862068965516\n",
      "    ram_util_percent: 95.72310344827585\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.114726919591833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0314016778443993\n",
      "    mean_inference_ms: 2.430153334068317\n",
      "    mean_raw_obs_processing_ms: 0.13974425453057193\n",
      "  time_since_restore: 40980.45183587074\n",
      "  time_this_iter_s: 219.0257921218872\n",
      "  time_total_s: 40980.45183587074\n",
      "  timers:\n",
      "    learn_throughput: 19.088\n",
      "    learn_time_ms: 209560.846\n",
      "    load_throughput: 6531149.175\n",
      "    load_time_ms: 0.612\n",
      "    sample_throughput: 18.16\n",
      "    sample_time_ms: 220264.227\n",
      "    update_time_ms: 17.167\n",
      "  timestamp: 1650259333\n",
      "  timesteps_since_restore: 756000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 189\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 756000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-22-15\n",
      "  done: false\n",
      "  episode_len_mean: 505.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.6\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2410\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7701616287231445\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012255776673555374\n",
      "          model: {}\n",
      "          policy_loss: -0.046137791126966476\n",
      "          total_loss: 0.02693963050842285\n",
      "          vf_explained_var: 0.7267382144927979\n",
      "          vf_loss: 0.07307742536067963\n",
      "    num_agent_steps_sampled: 756000\n",
      "    num_agent_steps_trained: 756000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.6968858131488\n",
      "    ram_util_percent: 95.73287197231835\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11205912056273032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0360591271433355\n",
      "    mean_inference_ms: 2.3596216607443337\n",
      "    mean_raw_obs_processing_ms: 0.21498669573987855\n",
      "  time_since_restore: 40989.58145260811\n",
      "  time_this_iter_s: 219.01784801483154\n",
      "  time_total_s: 40989.58145260811\n",
      "  timers:\n",
      "    learn_throughput: 19.121\n",
      "    learn_time_ms: 209194.205\n",
      "    load_throughput: 6275140.634\n",
      "    load_time_ms: 0.637\n",
      "    sample_throughput: 18.173\n",
      "    sample_time_ms: 220106.161\n",
      "    update_time_ms: 8.497\n",
      "  timestamp: 1650259335\n",
      "  timesteps_since_restore: 756000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 189\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 06:22:19 (running for 11:23:38.80)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.6 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-25-52\n",
      "  done: false\n",
      "  episode_len_mean: 2014.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 377\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4996974498633338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.5066891774631515e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.007309813518077135\n",
      "          total_loss: -0.00714569678530097\n",
      "          vf_explained_var: 0.19244109094142914\n",
      "          vf_loss: 0.00016411869728472084\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.84671280276817\n",
      "    ram_util_percent: 95.7993079584775\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11459452823217293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0303405096579967\n",
      "    mean_inference_ms: 2.4254423268797334\n",
      "    mean_raw_obs_processing_ms: 0.13954282670848506\n",
      "  time_since_restore: 41198.56111073494\n",
      "  time_this_iter_s: 218.10927486419678\n",
      "  time_total_s: 41198.56111073494\n",
      "  timers:\n",
      "    learn_throughput: 19.089\n",
      "    learn_time_ms: 209542.643\n",
      "    load_throughput: 6538785.564\n",
      "    load_time_ms: 0.612\n",
      "    sample_throughput: 18.204\n",
      "    sample_time_ms: 219731.044\n",
      "    update_time_ms: 16.972\n",
      "  timestamp: 1650259552\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-25-52\n",
      "  done: false\n",
      "  episode_len_mean: 502.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.58\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2419\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7729318737983704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011816374026238918\n",
      "          model: {}\n",
      "          policy_loss: -0.04667755588889122\n",
      "          total_loss: 0.024674488231539726\n",
      "          vf_explained_var: 0.7428115010261536\n",
      "          vf_loss: 0.0713520348072052\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.46678200692041\n",
      "    ram_util_percent: 95.79411764705883\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11190428144853984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.034922542028695\n",
      "    mean_inference_ms: 2.3545220131046687\n",
      "    mean_raw_obs_processing_ms: 0.2145733423008166\n",
      "  time_since_restore: 41207.457062482834\n",
      "  time_this_iter_s: 217.87560987472534\n",
      "  time_total_s: 41207.457062482834\n",
      "  timers:\n",
      "    learn_throughput: 19.125\n",
      "    learn_time_ms: 209152.241\n",
      "    load_throughput: 6134714.056\n",
      "    load_time_ms: 0.652\n",
      "    sample_throughput: 18.221\n",
      "    sample_time_ms: 219525.369\n",
      "    update_time_ms: 8.056\n",
      "  timestamp: 1650259552\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 764000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 2015.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 378\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.5792273760920982e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.908200272995598e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0006753724301233888\n",
      "          total_loss: -0.0005335723399184644\n",
      "          vf_explained_var: 0.09641646593809128\n",
      "          vf_loss: 0.0001418002211721614\n",
      "    num_agent_steps_sampled: 764000\n",
      "    num_agent_steps_trained: 764000\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.42241379310344\n",
      "    ram_util_percent: 95.8503448275862\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11455981471221505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.030063677763622\n",
      "    mean_inference_ms: 2.4242137654409124\n",
      "    mean_raw_obs_processing_ms: 0.13949037267073788\n",
      "  time_since_restore: 41417.492035865784\n",
      "  time_this_iter_s: 218.93092513084412\n",
      "  time_total_s: 41417.492035865784\n",
      "  timers:\n",
      "    learn_throughput: 19.09\n",
      "    learn_time_ms: 209532.138\n",
      "    load_throughput: 6574144.201\n",
      "    load_time_ms: 0.608\n",
      "    sample_throughput: 18.218\n",
      "    sample_time_ms: 219565.587\n",
      "    update_time_ms: 16.465\n",
      "  timestamp: 1650259771\n",
      "  timesteps_since_restore: 764000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 191\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 764000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 494.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.4\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2428\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7973552942276001\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010167315602302551\n",
      "          model: {}\n",
      "          policy_loss: -0.04394200071692467\n",
      "          total_loss: 0.005967050790786743\n",
      "          vf_explained_var: 0.7920660972595215\n",
      "          vf_loss: 0.04990905523300171\n",
      "    num_agent_steps_sampled: 764000\n",
      "    num_agent_steps_trained: 764000\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.07301038062283\n",
      "    ram_util_percent: 95.8287197231834\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1117489738686387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0337753486617247\n",
      "    mean_inference_ms: 2.3493909815390377\n",
      "    mean_raw_obs_processing_ms: 0.2141571280288364\n",
      "  time_since_restore: 41425.96072626114\n",
      "  time_this_iter_s: 218.50366377830505\n",
      "  time_total_s: 41425.96072626114\n",
      "  timers:\n",
      "    learn_throughput: 19.129\n",
      "    learn_time_ms: 209101.778\n",
      "    load_throughput: 6205280.172\n",
      "    load_time_ms: 0.645\n",
      "    sample_throughput: 18.236\n",
      "    sample_time_ms: 219345.248\n",
      "    update_time_ms: 8.12\n",
      "  timestamp: 1650259771\n",
      "  timesteps_since_restore: 764000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 191\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-33-09\n",
      "  done: false\n",
      "  episode_len_mean: 496.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.47\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2436\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7702049612998962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01297295093536377\n",
      "          model: {}\n",
      "          policy_loss: -0.04509330540895462\n",
      "          total_loss: 0.02783043310046196\n",
      "          vf_explained_var: 0.7546523213386536\n",
      "          vf_loss: 0.07292373478412628\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_agent_steps_trained: 768000\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.48235294117647\n",
      "    ram_util_percent: 95.88062283737023\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11160809612079343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0327312440294079\n",
      "    mean_inference_ms: 2.3447823556271254\n",
      "    mean_raw_obs_processing_ms: 0.21378575711539596\n",
      "  time_since_restore: 41643.84568238258\n",
      "  time_this_iter_s: 217.8849561214447\n",
      "  time_total_s: 41643.84568238258\n",
      "  timers:\n",
      "    learn_throughput: 19.134\n",
      "    learn_time_ms: 209053.496\n",
      "    load_throughput: 6176268.591\n",
      "    load_time_ms: 0.648\n",
      "    sample_throughput: 18.247\n",
      "    sample_time_ms: 219210.1\n",
      "    update_time_ms: 9.381\n",
      "  timestamp: 1650259989\n",
      "  timesteps_since_restore: 768000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-33-09\n",
      "  done: false\n",
      "  episode_len_mean: 2015.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 378\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0021758645307272673\n",
      "          total_loss: -0.0021758645307272673\n",
      "          vf_explained_var: 4.08603300456889e-05\n",
      "          vf_loss: 3.7912978312348855e-10\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_agent_steps_trained: 768000\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.33840830449827\n",
      "    ram_util_percent: 95.89377162629756\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11455981471221505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.030063677763622\n",
      "    mean_inference_ms: 2.4242137654409124\n",
      "    mean_raw_obs_processing_ms: 0.13949037267073788\n",
      "  time_since_restore: 41635.91361069679\n",
      "  time_this_iter_s: 218.4215748310089\n",
      "  time_total_s: 41635.91361069679\n",
      "  timers:\n",
      "    learn_throughput: 19.093\n",
      "    learn_time_ms: 209500.915\n",
      "    load_throughput: 6691080.801\n",
      "    load_time_ms: 0.598\n",
      "    sample_throughput: 18.234\n",
      "    sample_time_ms: 219375.627\n",
      "    update_time_ms: 15.275\n",
      "  timestamp: 1650259989\n",
      "  timesteps_since_restore: 768000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 772000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-36-47\n",
      "  done: false\n",
      "  episode_len_mean: 495.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.44\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2445\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.775657594203949\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011361013166606426\n",
      "          model: {}\n",
      "          policy_loss: -0.04031308740377426\n",
      "          total_loss: 0.025506628677248955\n",
      "          vf_explained_var: 0.7831122875213623\n",
      "          vf_loss: 0.06581971049308777\n",
      "    num_agent_steps_sampled: 772000\n",
      "    num_agent_steps_trained: 772000\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.63368055555556\n",
      "    ram_util_percent: 95.93194444444444\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11144651685684112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.031507380355892\n",
      "    mean_inference_ms: 2.3395557707184818\n",
      "    mean_raw_obs_processing_ms: 0.2133680748227484\n",
      "  time_since_restore: 41861.47438836098\n",
      "  time_this_iter_s: 217.62870597839355\n",
      "  time_total_s: 41861.47438836098\n",
      "  timers:\n",
      "    learn_throughput: 19.142\n",
      "    learn_time_ms: 208960.275\n",
      "    load_throughput: 6236883.271\n",
      "    load_time_ms: 0.641\n",
      "    sample_throughput: 18.259\n",
      "    sample_time_ms: 219068.659\n",
      "    update_time_ms: 9.533\n",
      "  timestamp: 1650260207\n",
      "  timesteps_since_restore: 772000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 772000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-36-48\n",
      "  done: false\n",
      "  episode_len_mean: 1920.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 387\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.3877278264557124e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.480609323950426e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.00024905320606194437\n",
      "          total_loss: 0.0004803766496479511\n",
      "          vf_explained_var: 0.08527328073978424\n",
      "          vf_loss: 0.0002313250588485971\n",
      "    num_agent_steps_sampled: 772000\n",
      "    num_agent_steps_trained: 772000\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.68379310344827\n",
      "    ram_util_percent: 95.9348275862069\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11420584919525653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0272920235840375\n",
      "    mean_inference_ms: 2.4119728779125125\n",
      "    mean_raw_obs_processing_ms: 0.13898982948163036\n",
      "  time_since_restore: 41854.60203051567\n",
      "  time_this_iter_s: 218.68841981887817\n",
      "  time_total_s: 41854.60203051567\n",
      "  timers:\n",
      "    learn_throughput: 19.1\n",
      "    learn_time_ms: 209429.225\n",
      "    load_throughput: 3085635.253\n",
      "    load_time_ms: 1.296\n",
      "    sample_throughput: 18.24\n",
      "    sample_time_ms: 219296.495\n",
      "    update_time_ms: 15.152\n",
      "  timestamp: 1650260208\n",
      "  timesteps_since_restore: 772000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 06:39:00 (running for 11:40:19.71)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.44 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 492.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.39\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2455\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7640417814254761\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011450561694800854\n",
      "          model: {}\n",
      "          policy_loss: -0.043881844729185104\n",
      "          total_loss: 0.018377413973212242\n",
      "          vf_explained_var: 0.7896828055381775\n",
      "          vf_loss: 0.062259260565042496\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.21862068965518\n",
      "    ram_util_percent: 95.97275862068965\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11126976167360475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.030152080826376\n",
      "    mean_inference_ms: 2.333762175683428\n",
      "    mean_raw_obs_processing_ms: 0.21291676708804133\n",
      "  time_since_restore: 42080.70175218582\n",
      "  time_this_iter_s: 219.22736382484436\n",
      "  time_total_s: 42080.70175218582\n",
      "  timers:\n",
      "    learn_throughput: 19.161\n",
      "    learn_time_ms: 208756.178\n",
      "    load_throughput: 6962078.181\n",
      "    load_time_ms: 0.575\n",
      "    sample_throughput: 18.268\n",
      "    sample_time_ms: 218965.634\n",
      "    update_time_ms: 9.156\n",
      "  timestamp: 1650260426\n",
      "  timesteps_since_restore: 776000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-40-28\n",
      "  done: false\n",
      "  episode_len_mean: 1822.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 389\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.5743146987353574e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.7934194705419854e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.00014984408335294574\n",
      "          total_loss: -8.388014248339459e-05\n",
      "          vf_explained_var: -0.00019708993204403669\n",
      "          vf_loss: 6.596697494387627e-05\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.85773195876288\n",
      "    ram_util_percent: 95.99003436426116\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11411617711831866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0266189173966713\n",
      "    mean_inference_ms: 2.4090248952458815\n",
      "    mean_raw_obs_processing_ms: 0.13887570034119442\n",
      "  time_since_restore: 42074.37790942192\n",
      "  time_this_iter_s: 219.77587890625\n",
      "  time_total_s: 42074.37790942192\n",
      "  timers:\n",
      "    learn_throughput: 19.107\n",
      "    learn_time_ms: 209346.264\n",
      "    load_throughput: 3093486.743\n",
      "    load_time_ms: 1.293\n",
      "    sample_throughput: 18.25\n",
      "    sample_time_ms: 219179.471\n",
      "    update_time_ms: 12.38\n",
      "  timestamp: 1650260428\n",
      "  timesteps_since_restore: 776000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 494.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.4\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2461\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6819967031478882\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011917653493583202\n",
      "          model: {}\n",
      "          policy_loss: -0.041339561343193054\n",
      "          total_loss: 0.010268062353134155\n",
      "          vf_explained_var: 0.8050622344017029\n",
      "          vf_loss: 0.05160762369632721\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_agent_steps_trained: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.88327645051196\n",
      "    ram_util_percent: 95.89556313993175\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11116369219996707\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.029336099852566\n",
      "    mean_inference_ms: 2.3303032967158566\n",
      "    mean_raw_obs_processing_ms: 0.2126462047492754\n",
      "  time_since_restore: 42302.815635204315\n",
      "  time_this_iter_s: 222.11388301849365\n",
      "  time_total_s: 42302.815635204315\n",
      "  timers:\n",
      "    learn_throughput: 19.134\n",
      "    learn_time_ms: 209053.076\n",
      "    load_throughput: 7193729.526\n",
      "    load_time_ms: 0.556\n",
      "    sample_throughput: 18.288\n",
      "    sample_time_ms: 218725.899\n",
      "    update_time_ms: 11.131\n",
      "  timestamp: 1650260648\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 195\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-44-10\n",
      "  done: false\n",
      "  episode_len_mean: 1822.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 389\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128293842077255\n",
      "          total_loss: -0.014128174632787704\n",
      "          vf_explained_var: -6.255924063225393e-07\n",
      "          vf_loss: 1.2308075270084373e-07\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_agent_steps_trained: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.69829351535837\n",
      "    ram_util_percent: 95.92047781569966\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11411617711831866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0266189173966713\n",
      "    mean_inference_ms: 2.4090248952458815\n",
      "    mean_raw_obs_processing_ms: 0.13887570034119442\n",
      "  time_since_restore: 42296.90155339241\n",
      "  time_this_iter_s: 222.5236439704895\n",
      "  time_total_s: 42296.90155339241\n",
      "  timers:\n",
      "    learn_throughput: 19.075\n",
      "    learn_time_ms: 209693.857\n",
      "    load_throughput: 2898420.289\n",
      "    load_time_ms: 1.38\n",
      "    sample_throughput: 18.262\n",
      "    sample_time_ms: 219039.923\n",
      "    update_time_ms: 11.769\n",
      "  timestamp: 1650260650\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 195\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-47-48\n",
      "  done: false\n",
      "  episode_len_mean: 496.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.45\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2470\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7387439608573914\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011929001659154892\n",
      "          model: {}\n",
      "          policy_loss: -0.04042370617389679\n",
      "          total_loss: 0.027226058766245842\n",
      "          vf_explained_var: 0.711940586566925\n",
      "          vf_loss: 0.06764976680278778\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_agent_steps_trained: 784000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.3958904109589\n",
      "    ram_util_percent: 95.91883561643836\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11100674432683214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0281294740288915\n",
      "    mean_inference_ms: 2.325190571257667\n",
      "    mean_raw_obs_processing_ms: 0.21224793031446146\n",
      "  time_since_restore: 42522.225798368454\n",
      "  time_this_iter_s: 219.4101631641388\n",
      "  time_total_s: 42522.225798368454\n",
      "  timers:\n",
      "    learn_throughput: 19.146\n",
      "    learn_time_ms: 208918.78\n",
      "    load_throughput: 7317029.09\n",
      "    load_time_ms: 0.547\n",
      "    sample_throughput: 18.258\n",
      "    sample_time_ms: 219086.376\n",
      "    update_time_ms: 12.507\n",
      "  timestamp: 1650260868\n",
      "  timesteps_since_restore: 784000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-47-49\n",
      "  done: false\n",
      "  episode_len_mean: 1824.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 396\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.397257981088089e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.629498962016719e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.020171133801341057\n",
      "          total_loss: 0.021248530596494675\n",
      "          vf_explained_var: 0.3516046106815338\n",
      "          vf_loss: 0.0010774029651656747\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_agent_steps_trained: 784000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.29896551724138\n",
      "    ram_util_percent: 95.90172413793104\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11377828719622561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0241000437785202\n",
      "    mean_inference_ms: 2.397985926205564\n",
      "    mean_raw_obs_processing_ms: 0.13845631708570402\n",
      "  time_since_restore: 42515.62769961357\n",
      "  time_this_iter_s: 218.7261462211609\n",
      "  time_total_s: 42515.62769961357\n",
      "  timers:\n",
      "    learn_throughput: 19.097\n",
      "    learn_time_ms: 209456.911\n",
      "    load_throughput: 2885459.549\n",
      "    load_time_ms: 1.386\n",
      "    sample_throughput: 18.229\n",
      "    sample_time_ms: 219429.094\n",
      "    update_time_ms: 10.914\n",
      "  timestamp: 1650260869\n",
      "  timesteps_since_restore: 784000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 788000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-51-29\n",
      "  done: false\n",
      "  episode_len_mean: 491.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.3\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2479\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7101911902427673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011940637603402138\n",
      "          model: {}\n",
      "          policy_loss: -0.04602792486548424\n",
      "          total_loss: 0.0388253889977932\n",
      "          vf_explained_var: 0.760082483291626\n",
      "          vf_loss: 0.08485330641269684\n",
      "    num_agent_steps_sampled: 788000\n",
      "    num_agent_steps_trained: 788000\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.56027397260273\n",
      "    ram_util_percent: 95.93321917808218\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11085105535250651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0269294170253722\n",
      "    mean_inference_ms: 2.320082291589373\n",
      "    mean_raw_obs_processing_ms: 0.21185143308743332\n",
      "  time_since_restore: 42742.990844249725\n",
      "  time_this_iter_s: 220.76504588127136\n",
      "  time_total_s: 42742.990844249725\n",
      "  timers:\n",
      "    learn_throughput: 19.136\n",
      "    learn_time_ms: 209025.44\n",
      "    load_throughput: 7170056.84\n",
      "    load_time_ms: 0.558\n",
      "    sample_throughput: 18.267\n",
      "    sample_time_ms: 218978.513\n",
      "    update_time_ms: 12.735\n",
      "  timestamp: 1650261089\n",
      "  timesteps_since_restore: 788000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 197\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 788000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-51-29\n",
      "  done: false\n",
      "  episode_len_mean: 1825.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 398\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.363872683646091e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.574723466337613e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0022079134359955788\n",
      "          total_loss: 0.0026080633979290724\n",
      "          vf_explained_var: 0.11783824861049652\n",
      "          vf_loss: 0.00040014873957261443\n",
      "    num_agent_steps_sampled: 788000\n",
      "    num_agent_steps_trained: 788000\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.88206896551725\n",
      "    ram_util_percent: 95.92103448275861\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11367791880808806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0233547389335589\n",
      "    mean_inference_ms: 2.394717154706059\n",
      "    mean_raw_obs_processing_ms: 0.13833025987356287\n",
      "  time_since_restore: 42735.56840658188\n",
      "  time_this_iter_s: 219.9407069683075\n",
      "  time_total_s: 42735.56840658188\n",
      "  timers:\n",
      "    learn_throughput: 19.083\n",
      "    learn_time_ms: 209615.655\n",
      "    load_throughput: 2934054.319\n",
      "    load_time_ms: 1.363\n",
      "    sample_throughput: 18.252\n",
      "    sample_time_ms: 219151.196\n",
      "    update_time_ms: 10.722\n",
      "  timestamp: 1650261089\n",
      "  timesteps_since_restore: 788000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 197\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 1825.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 398\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 5.738105380370897e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.428093780231864e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014127971604466438\n",
      "          total_loss: 0.014143900945782661\n",
      "          vf_explained_var: -1.6022754234512604e-09\n",
      "          vf_loss: 1.5932177120703273e-05\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.62698961937717\n",
      "    ram_util_percent: 95.95951557093426\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11367791880808806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0233547389335589\n",
      "    mean_inference_ms: 2.394717154706059\n",
      "    mean_raw_obs_processing_ms: 0.13833025987356287\n",
      "  time_since_restore: 42953.45036435127\n",
      "  time_this_iter_s: 217.88195776939392\n",
      "  time_total_s: 42953.45036435127\n",
      "  timers:\n",
      "    learn_throughput: 19.075\n",
      "    learn_time_ms: 209700.84\n",
      "    load_throughput: 2945024.575\n",
      "    load_time_ms: 1.358\n",
      "    sample_throughput: 18.249\n",
      "    sample_time_ms: 219188.859\n",
      "    update_time_ms: 10.712\n",
      "  timestamp: 1650261307\n",
      "  timesteps_since_restore: 792000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 494.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.33\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2488\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7740549445152283\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01256947498768568\n",
      "          model: {}\n",
      "          policy_loss: -0.04963342100381851\n",
      "          total_loss: 0.031988371163606644\n",
      "          vf_explained_var: 0.6795650124549866\n",
      "          vf_loss: 0.08162178844213486\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.96505190311419\n",
      "    ram_util_percent: 95.97058823529412\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1106970596005283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0257448123511508\n",
      "    mean_inference_ms: 2.3150510120453407\n",
      "    mean_raw_obs_processing_ms: 0.2114681872074307\n",
      "  time_since_restore: 42961.62988328934\n",
      "  time_this_iter_s: 218.63903903961182\n",
      "  time_total_s: 42961.62988328934\n",
      "  timers:\n",
      "    learn_throughput: 19.119\n",
      "    learn_time_ms: 209210.861\n",
      "    load_throughput: 7288737.51\n",
      "    load_time_ms: 0.549\n",
      "    sample_throughput: 18.265\n",
      "    sample_time_ms: 219002.109\n",
      "    update_time_ms: 12.786\n",
      "  timestamp: 1650261307\n",
      "  timesteps_since_restore: 792000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 06:55:40 (running for 11:56:59.97)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.33 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 796000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 494.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.33\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2495\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6923629641532898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011600439436733723\n",
      "          model: {}\n",
      "          policy_loss: -0.04689909145236015\n",
      "          total_loss: 0.03919743001461029\n",
      "          vf_explained_var: 0.7323119044303894\n",
      "          vf_loss: 0.08609651029109955\n",
      "    num_agent_steps_sampled: 796000\n",
      "    num_agent_steps_trained: 796000\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.07827586206896\n",
      "    ram_util_percent: 95.94275862068966\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11057614503553458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0248142327917744\n",
      "    mean_inference_ms: 2.311117086538854\n",
      "    mean_raw_obs_processing_ms: 0.21116355481071686\n",
      "  time_since_restore: 43180.7203142643\n",
      "  time_this_iter_s: 219.09043097496033\n",
      "  time_total_s: 43180.7203142643\n",
      "  timers:\n",
      "    learn_throughput: 19.114\n",
      "    learn_time_ms: 209275.869\n",
      "    load_throughput: 7283358.368\n",
      "    load_time_ms: 0.549\n",
      "    sample_throughput: 18.254\n",
      "    sample_time_ms: 219128.58\n",
      "    update_time_ms: 14.03\n",
      "  timestamp: 1650261526\n",
      "  timesteps_since_restore: 796000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 796000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_06-58-47\n",
      "  done: false\n",
      "  episode_len_mean: 1923.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 399\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.02465683696165e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.7209602082443044e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0028277134988456964\n",
      "          total_loss: 0.0028565304819494486\n",
      "          vf_explained_var: -0.004808914847671986\n",
      "          vf_loss: 2.880802821891848e-05\n",
      "    num_agent_steps_sampled: 796000\n",
      "    num_agent_steps_trained: 796000\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.90034364261169\n",
      "    ram_util_percent: 95.94329896907216\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11362477575654614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.022959281821458\n",
      "    mean_inference_ms: 2.3929800507792924\n",
      "    mean_raw_obs_processing_ms: 0.13826243738423355\n",
      "  time_since_restore: 43172.98669552803\n",
      "  time_this_iter_s: 219.5363311767578\n",
      "  time_total_s: 43172.98669552803\n",
      "  timers:\n",
      "    learn_throughput: 19.064\n",
      "    learn_time_ms: 209816.046\n",
      "    load_throughput: 2979226.48\n",
      "    load_time_ms: 1.343\n",
      "    sample_throughput: 18.248\n",
      "    sample_time_ms: 219200.286\n",
      "    update_time_ms: 13.754\n",
      "  timestamp: 1650261527\n",
      "  timesteps_since_restore: 796000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-02-25\n",
      "  done: false\n",
      "  episode_len_mean: 499.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.43\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2504\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7578663229942322\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015380611643195152\n",
      "          model: {}\n",
      "          policy_loss: -0.040098387748003006\n",
      "          total_loss: 0.06655116379261017\n",
      "          vf_explained_var: 0.5457643270492554\n",
      "          vf_loss: 0.10664955526590347\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_agent_steps_trained: 800000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.54172413793104\n",
      "    ram_util_percent: 95.92793103448275\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11041921368120863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0236073312523\n",
      "    mean_inference_ms: 2.3060083313810806\n",
      "    mean_raw_obs_processing_ms: 0.21077541333743988\n",
      "  time_since_restore: 43399.2611413002\n",
      "  time_this_iter_s: 218.54082703590393\n",
      "  time_total_s: 43399.2611413002\n",
      "  timers:\n",
      "    learn_throughput: 19.105\n",
      "    learn_time_ms: 209368.933\n",
      "    load_throughput: 6945361.815\n",
      "    load_time_ms: 0.576\n",
      "    sample_throughput: 18.25\n",
      "    sample_time_ms: 219174.093\n",
      "    update_time_ms: 14.279\n",
      "  timestamp: 1650261745\n",
      "  timesteps_since_restore: 800000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 200\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-02-25\n",
      "  done: false\n",
      "  episode_len_mean: 1923.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 399\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128018170595169\n",
      "          total_loss: 0.014136634767055511\n",
      "          vf_explained_var: -4.685053411890294e-08\n",
      "          vf_loss: 8.604897629993502e-06\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_agent_steps_trained: 800000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.96332179930796\n",
      "    ram_util_percent: 95.91418685121106\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11362477575654614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.022959281821458\n",
      "    mean_inference_ms: 2.3929800507792924\n",
      "    mean_raw_obs_processing_ms: 0.13826243738423355\n",
      "  time_since_restore: 43391.5138065815\n",
      "  time_this_iter_s: 218.5271110534668\n",
      "  time_total_s: 43391.5138065815\n",
      "  timers:\n",
      "    learn_throughput: 19.055\n",
      "    learn_time_ms: 209923.051\n",
      "    load_throughput: 2981715.037\n",
      "    load_time_ms: 1.342\n",
      "    sample_throughput: 18.244\n",
      "    sample_time_ms: 219252.866\n",
      "    update_time_ms: 13.689\n",
      "  timestamp: 1650261745\n",
      "  timesteps_since_restore: 800000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 200\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 804000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-06-03\n",
      "  done: false\n",
      "  episode_len_mean: 488.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2514\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7513169050216675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01113484799861908\n",
      "          model: {}\n",
      "          policy_loss: -0.04869704693555832\n",
      "          total_loss: 0.03038283810019493\n",
      "          vf_explained_var: 0.6570102572441101\n",
      "          vf_loss: 0.07907988876104355\n",
      "    num_agent_steps_sampled: 804000\n",
      "    num_agent_steps_trained: 804000\n",
      "    num_steps_sampled: 804000\n",
      "    num_steps_trained: 804000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.59377162629757\n",
      "    ram_util_percent: 95.91764705882352\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11024789996841369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0222841512743923\n",
      "    mean_inference_ms: 2.3003913954483632\n",
      "    mean_raw_obs_processing_ms: 0.21035471846087003\n",
      "  time_since_restore: 43617.023260354996\n",
      "  time_this_iter_s: 217.7621190547943\n",
      "  time_total_s: 43617.023260354996\n",
      "  timers:\n",
      "    learn_throughput: 19.105\n",
      "    learn_time_ms: 209367.119\n",
      "    load_throughput: 6839468.406\n",
      "    load_time_ms: 0.585\n",
      "    sample_throughput: 18.249\n",
      "    sample_time_ms: 219195.208\n",
      "    update_time_ms: 14.288\n",
      "  timestamp: 1650261963\n",
      "  timesteps_since_restore: 804000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 804000\n",
      "  training_iteration: 201\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 804000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-06-03\n",
      "  done: false\n",
      "  episode_len_mean: 1923.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 399\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128020964562893\n",
      "          total_loss: 0.014131508767604828\n",
      "          vf_explained_var: -3.973643192267673e-09\n",
      "          vf_loss: 3.489998562145047e-06\n",
      "    num_agent_steps_sampled: 804000\n",
      "    num_agent_steps_trained: 804000\n",
      "    num_steps_sampled: 804000\n",
      "    num_steps_trained: 804000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.17820069204153\n",
      "    ram_util_percent: 95.8916955017301\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11362477575654614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.022959281821458\n",
      "    mean_inference_ms: 2.3929800507792924\n",
      "    mean_raw_obs_processing_ms: 0.13826243738423355\n",
      "  time_since_restore: 43609.54057264328\n",
      "  time_this_iter_s: 218.02676606178284\n",
      "  time_total_s: 43609.54057264328\n",
      "  timers:\n",
      "    learn_throughput: 19.058\n",
      "    learn_time_ms: 209890.649\n",
      "    load_throughput: 2979491.023\n",
      "    load_time_ms: 1.343\n",
      "    sample_throughput: 18.24\n",
      "    sample_time_ms: 219298.595\n",
      "    update_time_ms: 13.289\n",
      "  timestamp: 1650261963\n",
      "  timesteps_since_restore: 804000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 804000\n",
      "  training_iteration: 201\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 808000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-09-42\n",
      "  done: false\n",
      "  episode_len_mean: 491.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 6.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2522\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6957476139068604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012953079305589199\n",
      "          model: {}\n",
      "          policy_loss: -0.041583072394132614\n",
      "          total_loss: 0.04166015610098839\n",
      "          vf_explained_var: 0.7016094326972961\n",
      "          vf_loss: 0.0832432359457016\n",
      "    num_agent_steps_sampled: 808000\n",
      "    num_agent_steps_trained: 808000\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 808000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.79862068965517\n",
      "    ram_util_percent: 95.88827586206897\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11010997547472337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0212251398313084\n",
      "    mean_inference_ms: 2.2958882058772203\n",
      "    mean_raw_obs_processing_ms: 0.2100167061518186\n",
      "  time_since_restore: 43836.396139621735\n",
      "  time_this_iter_s: 219.3728792667389\n",
      "  time_total_s: 43836.396139621735\n",
      "  timers:\n",
      "    learn_throughput: 19.088\n",
      "    learn_time_ms: 209551.493\n",
      "    load_throughput: 6299172.486\n",
      "    load_time_ms: 0.635\n",
      "    sample_throughput: 18.252\n",
      "    sample_time_ms: 219159.872\n",
      "    update_time_ms: 14.315\n",
      "  timestamp: 1650262182\n",
      "  timesteps_since_restore: 808000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 202\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 808000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-09-43\n",
      "  done: false\n",
      "  episode_len_mean: 1923.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 399\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.5795450651461068e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.8836596114210293e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.005566874053329229\n",
      "          total_loss: 0.005609000101685524\n",
      "          vf_explained_var: -0.010056625120341778\n",
      "          vf_loss: 4.212759813526645e-05\n",
      "    num_agent_steps_sampled: 808000\n",
      "    num_agent_steps_trained: 808000\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 808000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.02827586206898\n",
      "    ram_util_percent: 95.89241379310344\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11362477575654614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.022959281821458\n",
      "    mean_inference_ms: 2.3929800507792924\n",
      "    mean_raw_obs_processing_ms: 0.13826243738423355\n",
      "  time_since_restore: 43828.9988014698\n",
      "  time_this_iter_s: 219.45822882652283\n",
      "  time_total_s: 43828.9988014698\n",
      "  timers:\n",
      "    learn_throughput: 19.047\n",
      "    learn_time_ms: 210010.216\n",
      "    load_throughput: 2961765.35\n",
      "    load_time_ms: 1.351\n",
      "    sample_throughput: 18.245\n",
      "    sample_time_ms: 219243.227\n",
      "    update_time_ms: 13.309\n",
      "  timestamp: 1650262183\n",
      "  timesteps_since_restore: 808000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 202\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 07:12:21 (running for 12:13:40.89)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.35 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 812000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 502.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 6.55\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2529\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6798915863037109\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013096952810883522\n",
      "          model: {}\n",
      "          policy_loss: -0.03437827154994011\n",
      "          total_loss: 0.04067089036107063\n",
      "          vf_explained_var: 0.7188517451286316\n",
      "          vf_loss: 0.07504915446043015\n",
      "    num_agent_steps_sampled: 812000\n",
      "    num_agent_steps_trained: 812000\n",
      "    num_steps_sampled: 812000\n",
      "    num_steps_trained: 812000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.94152249134947\n",
      "    ram_util_percent: 95.74913494809688\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10998737042941478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0202821039356653\n",
      "    mean_inference_ms: 2.291866384133911\n",
      "    mean_raw_obs_processing_ms: 0.20971362561296789\n",
      "  time_since_restore: 44054.61778473854\n",
      "  time_this_iter_s: 218.22164511680603\n",
      "  time_total_s: 44054.61778473854\n",
      "  timers:\n",
      "    learn_throughput: 19.084\n",
      "    learn_time_ms: 209604.388\n",
      "    load_throughput: 5414974.664\n",
      "    load_time_ms: 0.739\n",
      "    sample_throughput: 18.239\n",
      "    sample_time_ms: 219313.377\n",
      "    update_time_ms: 15.008\n",
      "  timestamp: 1650262401\n",
      "  timesteps_since_restore: 812000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 812000\n",
      "  training_iteration: 203\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 812000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 1923.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 399\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128240756690502\n",
      "          total_loss: 0.014128305949270725\n",
      "          vf_explained_var: 2.4046948965406045e-07\n",
      "          vf_loss: 6.622491355301463e-08\n",
      "    num_agent_steps_sampled: 812000\n",
      "    num_agent_steps_trained: 812000\n",
      "    num_steps_sampled: 812000\n",
      "    num_steps_trained: 812000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.64742268041238\n",
      "    ram_util_percent: 95.76426116838489\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11362477575654614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.022959281821458\n",
      "    mean_inference_ms: 2.3929800507792924\n",
      "    mean_raw_obs_processing_ms: 0.13826243738423355\n",
      "  time_since_restore: 44047.98393249512\n",
      "  time_this_iter_s: 218.98513102531433\n",
      "  time_total_s: 44047.98393249512\n",
      "  timers:\n",
      "    learn_throughput: 19.041\n",
      "    learn_time_ms: 210070.323\n",
      "    load_throughput: 6100583.979\n",
      "    load_time_ms: 0.656\n",
      "    sample_throughput: 18.24\n",
      "    sample_time_ms: 219293.693\n",
      "    update_time_ms: 13.092\n",
      "  timestamp: 1650262402\n",
      "  timesteps_since_restore: 812000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 812000\n",
      "  training_iteration: 203\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 816000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-16-59\n",
      "  done: false\n",
      "  episode_len_mean: 502.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 6.51\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2538\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7548119425773621\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013307333923876286\n",
      "          model: {}\n",
      "          policy_loss: -0.038809146732091904\n",
      "          total_loss: 0.0466194711625576\n",
      "          vf_explained_var: 0.6489744782447815\n",
      "          vf_loss: 0.0854286178946495\n",
      "    num_agent_steps_sampled: 816000\n",
      "    num_agent_steps_trained: 816000\n",
      "    num_steps_sampled: 816000\n",
      "    num_steps_trained: 816000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.95137931034483\n",
      "    ram_util_percent: 95.69103448275862\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10983495632011117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0190978639879822\n",
      "    mean_inference_ms: 2.2867943006236118\n",
      "    mean_raw_obs_processing_ms: 0.20933133599847012\n",
      "  time_since_restore: 44272.58269906044\n",
      "  time_this_iter_s: 217.9649143218994\n",
      "  time_total_s: 44272.58269906044\n",
      "  timers:\n",
      "    learn_throughput: 19.096\n",
      "    learn_time_ms: 209470.015\n",
      "    load_throughput: 3505697.39\n",
      "    load_time_ms: 1.141\n",
      "    sample_throughput: 18.233\n",
      "    sample_time_ms: 219380.481\n",
      "    update_time_ms: 15.476\n",
      "  timestamp: 1650262619\n",
      "  timesteps_since_restore: 816000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 816000\n",
      "  training_iteration: 204\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 816000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-16-59\n",
      "  done: false\n",
      "  episode_len_mean: 1829.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 407\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.3698551676048264e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.583082509613031e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0009382558055222034\n",
      "          total_loss: -0.0006561838090419769\n",
      "          vf_explained_var: 0.16028165817260742\n",
      "          vf_loss: 0.00028207816649228334\n",
      "    num_agent_steps_sampled: 816000\n",
      "    num_agent_steps_trained: 816000\n",
      "    num_steps_sampled: 816000\n",
      "    num_steps_trained: 816000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.4610344827586\n",
      "    ram_util_percent: 95.68344827586206\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11313874682196219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0193871570960864\n",
      "    mean_inference_ms: 2.3771882207950323\n",
      "    mean_raw_obs_processing_ms: 0.13766153077716614\n",
      "  time_since_restore: 44265.64288854599\n",
      "  time_this_iter_s: 217.6589560508728\n",
      "  time_total_s: 44265.64288854599\n",
      "  timers:\n",
      "    learn_throughput: 19.065\n",
      "    learn_time_ms: 209804.042\n",
      "    load_throughput: 5582914.379\n",
      "    load_time_ms: 0.716\n",
      "    sample_throughput: 18.231\n",
      "    sample_time_ms: 219410.252\n",
      "    update_time_ms: 12.836\n",
      "  timestamp: 1650262619\n",
      "  timesteps_since_restore: 816000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 816000\n",
      "  training_iteration: 204\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 820000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-20-37\n",
      "  done: false\n",
      "  episode_len_mean: 498.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 6.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2548\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8087807297706604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013413956388831139\n",
      "          model: {}\n",
      "          policy_loss: -0.040750712156295776\n",
      "          total_loss: 0.03731321915984154\n",
      "          vf_explained_var: 0.6814266443252563\n",
      "          vf_loss: 0.07806394249200821\n",
      "    num_agent_steps_sampled: 820000\n",
      "    num_agent_steps_trained: 820000\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.93702422145329\n",
      "    ram_util_percent: 95.74290657439447\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10966725418422874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.017804486096596\n",
      "    mean_inference_ms: 2.281246049313684\n",
      "    mean_raw_obs_processing_ms: 0.2089204514960688\n",
      "  time_since_restore: 44490.6584687233\n",
      "  time_this_iter_s: 218.07576966285706\n",
      "  time_total_s: 44490.6584687233\n",
      "  timers:\n",
      "    learn_throughput: 19.127\n",
      "    learn_time_ms: 209128.051\n",
      "    load_throughput: 3494015.869\n",
      "    load_time_ms: 1.145\n",
      "    sample_throughput: 18.249\n",
      "    sample_time_ms: 219185.924\n",
      "    update_time_ms: 14.539\n",
      "  timestamp: 1650262837\n",
      "  timesteps_since_restore: 820000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 205\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 820000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-20-38\n",
      "  done: false\n",
      "  episode_len_mean: 1537.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 424\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0247203254951551e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.2051211959721637e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0006715314229950309\n",
      "          total_loss: 0.0002402092795819044\n",
      "          vf_explained_var: 0.3914128243923187\n",
      "          vf_loss: 0.0009117387817241251\n",
      "    num_agent_steps_sampled: 820000\n",
      "    num_agent_steps_trained: 820000\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.55397923875432\n",
      "    ram_util_percent: 95.73460207612457\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11213263094555465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0120678230947533\n",
      "    mean_inference_ms: 2.344766125210456\n",
      "    mean_raw_obs_processing_ms: 0.13651346118273974\n",
      "  time_since_restore: 44483.934054374695\n",
      "  time_this_iter_s: 218.29116582870483\n",
      "  time_total_s: 44483.934054374695\n",
      "  timers:\n",
      "    learn_throughput: 19.105\n",
      "    learn_time_ms: 209368.536\n",
      "    load_throughput: 6317436.457\n",
      "    load_time_ms: 0.633\n",
      "    sample_throughput: 18.252\n",
      "    sample_time_ms: 219154.583\n",
      "    update_time_ms: 12.535\n",
      "  timestamp: 1650262838\n",
      "  timesteps_since_restore: 820000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 205\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 824000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-24-15\n",
      "  done: false\n",
      "  episode_len_mean: 494.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 6.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2557\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.804049015045166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012710402719676495\n",
      "          model: {}\n",
      "          policy_loss: -0.05183906480669975\n",
      "          total_loss: 0.023910140618681908\n",
      "          vf_explained_var: 0.7072761654853821\n",
      "          vf_loss: 0.0757492184638977\n",
      "    num_agent_steps_sampled: 824000\n",
      "    num_agent_steps_trained: 824000\n",
      "    num_steps_sampled: 824000\n",
      "    num_steps_trained: 824000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.6771626297578\n",
      "    ram_util_percent: 95.78892733564014\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10951608122931386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0166404676880634\n",
      "    mean_inference_ms: 2.276244424632492\n",
      "    mean_raw_obs_processing_ms: 0.20854338824354218\n",
      "  time_since_restore: 44708.64165067673\n",
      "  time_this_iter_s: 217.98318195343018\n",
      "  time_total_s: 44708.64165067673\n",
      "  timers:\n",
      "    learn_throughput: 19.132\n",
      "    learn_time_ms: 209070.172\n",
      "    load_throughput: 3504672.14\n",
      "    load_time_ms: 1.141\n",
      "    sample_throughput: 18.286\n",
      "    sample_time_ms: 218748.944\n",
      "    update_time_ms: 13.302\n",
      "  timestamp: 1650263055\n",
      "  timesteps_since_restore: 824000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 824000\n",
      "  training_iteration: 206\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 824000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-24-16\n",
      "  done: false\n",
      "  episode_len_mean: 1537.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 424\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128053560853004\n",
      "          total_loss: 0.014129122719168663\n",
      "          vf_explained_var: -2.2463900961611216e-07\n",
      "          vf_loss: 1.0692313026083866e-06\n",
      "    num_agent_steps_sampled: 824000\n",
      "    num_agent_steps_trained: 824000\n",
      "    num_steps_sampled: 824000\n",
      "    num_steps_trained: 824000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.61758620689655\n",
      "    ram_util_percent: 95.81827586206894\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11213263094555465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0120678230947533\n",
      "    mean_inference_ms: 2.344766125210456\n",
      "    mean_raw_obs_processing_ms: 0.13651346118273974\n",
      "  time_since_restore: 44702.572674274445\n",
      "  time_this_iter_s: 218.63861989974976\n",
      "  time_total_s: 44702.572674274445\n",
      "  timers:\n",
      "    learn_throughput: 19.096\n",
      "    learn_time_ms: 209464.968\n",
      "    load_throughput: 6288547.547\n",
      "    load_time_ms: 0.636\n",
      "    sample_throughput: 18.297\n",
      "    sample_time_ms: 218612.124\n",
      "    update_time_ms: 12.824\n",
      "  timestamp: 1650263056\n",
      "  timesteps_since_restore: 824000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 824000\n",
      "  training_iteration: 206\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 828000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-27-55\n",
      "  done: false\n",
      "  episode_len_mean: 490.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 6.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2565\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7281914353370667\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014464497566223145\n",
      "          model: {}\n",
      "          policy_loss: -0.04228048399090767\n",
      "          total_loss: 0.03145996481180191\n",
      "          vf_explained_var: 0.7316138744354248\n",
      "          vf_loss: 0.07374044507741928\n",
      "    num_agent_steps_sampled: 828000\n",
      "    num_agent_steps_trained: 828000\n",
      "    num_steps_sampled: 828000\n",
      "    num_steps_trained: 828000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.07191780821918\n",
      "    ram_util_percent: 95.91849315068492\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10938704600711265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0156393590885613\n",
      "    mean_inference_ms: 2.2719162469549365\n",
      "    mean_raw_obs_processing_ms: 0.20821986443305415\n",
      "  time_since_restore: 44928.908861637115\n",
      "  time_this_iter_s: 220.26721096038818\n",
      "  time_total_s: 44928.908861637115\n",
      "  timers:\n",
      "    learn_throughput: 19.139\n",
      "    learn_time_ms: 208998.243\n",
      "    load_throughput: 3536363.56\n",
      "    load_time_ms: 1.131\n",
      "    sample_throughput: 18.29\n",
      "    sample_time_ms: 218703.413\n",
      "    update_time_ms: 14.316\n",
      "  timestamp: 1650263275\n",
      "  timesteps_since_restore: 828000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 828000\n",
      "  training_iteration: 207\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 828000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-27-56\n",
      "  done: false\n",
      "  episode_len_mean: 1537.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 424\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128177426755428\n",
      "          total_loss: 0.014128248207271099\n",
      "          vf_explained_var: 1.7730138779370463e-06\n",
      "          vf_loss: 7.429565584970987e-08\n",
      "    num_agent_steps_sampled: 828000\n",
      "    num_agent_steps_trained: 828000\n",
      "    num_steps_sampled: 828000\n",
      "    num_steps_trained: 828000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.5896551724138\n",
      "    ram_util_percent: 95.92793103448275\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11213263094555465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0120678230947533\n",
      "    mean_inference_ms: 2.344766125210456\n",
      "    mean_raw_obs_processing_ms: 0.13651346118273974\n",
      "  time_since_restore: 44921.65152645111\n",
      "  time_this_iter_s: 219.07885217666626\n",
      "  time_total_s: 44921.65152645111\n",
      "  timers:\n",
      "    learn_throughput: 19.104\n",
      "    learn_time_ms: 209377.815\n",
      "    load_throughput: 6249428.593\n",
      "    load_time_ms: 0.64\n",
      "    sample_throughput: 18.289\n",
      "    sample_time_ms: 218709.384\n",
      "    update_time_ms: 12.456\n",
      "  timestamp: 1650263276\n",
      "  timesteps_since_restore: 828000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 828000\n",
      "  training_iteration: 207\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 07:29:01 (running for 12:30:21.55)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.22 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 832000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 495.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2573\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7638368606567383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01295720785856247\n",
      "          model: {}\n",
      "          policy_loss: -0.043068401515483856\n",
      "          total_loss: 0.04436508193612099\n",
      "          vf_explained_var: 0.6539268493652344\n",
      "          vf_loss: 0.08743348717689514\n",
      "    num_agent_steps_sampled: 832000\n",
      "    num_agent_steps_trained: 832000\n",
      "    num_steps_sampled: 832000\n",
      "    num_steps_trained: 832000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.1594501718213\n",
      "    ram_util_percent: 95.94192439862543\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10925512811748299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0146242057449546\n",
      "    mean_inference_ms: 2.267529556550743\n",
      "    mean_raw_obs_processing_ms: 0.20789357708350192\n",
      "  time_since_restore: 45148.63158631325\n",
      "  time_this_iter_s: 219.7227246761322\n",
      "  time_total_s: 45148.63158631325\n",
      "  timers:\n",
      "    learn_throughput: 19.127\n",
      "    learn_time_ms: 209130.926\n",
      "    load_throughput: 3508409.87\n",
      "    load_time_ms: 1.14\n",
      "    sample_throughput: 18.298\n",
      "    sample_time_ms: 218608.547\n",
      "    update_time_ms: 14.607\n",
      "  timestamp: 1650263495\n",
      "  timesteps_since_restore: 832000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 832000\n",
      "  training_iteration: 208\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 832000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 1633.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 434\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.519149284554971e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -9.658695754763077e-29\n",
      "          model: {}\n",
      "          policy_loss: -0.0012690359726548195\n",
      "          total_loss: -0.0004841178306378424\n",
      "          vf_explained_var: 0.28685513138771057\n",
      "          vf_loss: 0.0007849220419302583\n",
      "    num_agent_steps_sampled: 832000\n",
      "    num_agent_steps_trained: 832000\n",
      "    num_steps_sampled: 832000\n",
      "    num_steps_trained: 832000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.5405498281787\n",
      "    ram_util_percent: 95.94810996563574\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11153542988661255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0077279798807706\n",
      "    mean_inference_ms: 2.325317002530392\n",
      "    mean_raw_obs_processing_ms: 0.1358169570905583\n",
      "  time_since_restore: 45141.55468130112\n",
      "  time_this_iter_s: 219.9031548500061\n",
      "  time_total_s: 45141.55468130112\n",
      "  timers:\n",
      "    learn_throughput: 19.089\n",
      "    learn_time_ms: 209542.97\n",
      "    load_throughput: 6207805.817\n",
      "    load_time_ms: 0.644\n",
      "    sample_throughput: 18.293\n",
      "    sample_time_ms: 218659.755\n",
      "    update_time_ms: 11.752\n",
      "  timestamp: 1650263495\n",
      "  timesteps_since_restore: 832000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 832000\n",
      "  training_iteration: 208\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 836000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 484.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2584\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8658689856529236\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010124590247869492\n",
      "          model: {}\n",
      "          policy_loss: -0.04516056925058365\n",
      "          total_loss: 0.04218228533864021\n",
      "          vf_explained_var: 0.667041540145874\n",
      "          vf_loss: 0.08734285086393356\n",
      "    num_agent_steps_sampled: 836000\n",
      "    num_agent_steps_trained: 836000\n",
      "    num_steps_sampled: 836000\n",
      "    num_steps_trained: 836000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.37972508591065\n",
      "    ram_util_percent: 95.94501718213058\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10907482054636032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0132461802778803\n",
      "    mean_inference_ms: 2.2615808642017625\n",
      "    mean_raw_obs_processing_ms: 0.20745481353090076\n",
      "  time_since_restore: 45367.79049324989\n",
      "  time_this_iter_s: 219.1589069366455\n",
      "  time_total_s: 45367.79049324989\n",
      "  timers:\n",
      "    learn_throughput: 19.124\n",
      "    learn_time_ms: 209158.725\n",
      "    load_throughput: 2857155.313\n",
      "    load_time_ms: 1.4\n",
      "    sample_throughput: 18.288\n",
      "    sample_time_ms: 218724.79\n",
      "    update_time_ms: 13.554\n",
      "  timestamp: 1650263714\n",
      "  timesteps_since_restore: 836000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 836000\n",
      "  training_iteration: 209\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 836000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-35-15\n",
      "  done: false\n",
      "  episode_len_mean: 1634.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 435\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.9201503825437114e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.553148131853216e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.006136463489383459\n",
      "          total_loss: 0.006845492869615555\n",
      "          vf_explained_var: 0.09586598724126816\n",
      "          vf_loss: 0.0007090268773026764\n",
      "    num_agent_steps_sampled: 836000\n",
      "    num_agent_steps_trained: 836000\n",
      "    num_steps_sampled: 836000\n",
      "    num_steps_trained: 836000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.39725085910652\n",
      "    ram_util_percent: 95.94329896907216\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1114743275100485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0072836368098776\n",
      "    mean_inference_ms: 2.3233244744405326\n",
      "    mean_raw_obs_processing_ms: 0.13574519643028757\n",
      "  time_since_restore: 45360.68520832062\n",
      "  time_this_iter_s: 219.13052701950073\n",
      "  time_total_s: 45360.68520832062\n",
      "  timers:\n",
      "    learn_throughput: 19.091\n",
      "    learn_time_ms: 209525.231\n",
      "    load_throughput: 6300591.858\n",
      "    load_time_ms: 0.635\n",
      "    sample_throughput: 18.281\n",
      "    sample_time_ms: 218800.771\n",
      "    update_time_ms: 7.59\n",
      "  timestamp: 1650263715\n",
      "  timesteps_since_restore: 836000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 836000\n",
      "  training_iteration: 209\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-38-53\n",
      "  done: false\n",
      "  episode_len_mean: 481.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2593\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7935060858726501\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01143475528806448\n",
      "          model: {}\n",
      "          policy_loss: -0.04817028343677521\n",
      "          total_loss: 0.01402541995048523\n",
      "          vf_explained_var: 0.7647796869277954\n",
      "          vf_loss: 0.06219570338726044\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.20622837370242\n",
      "    ram_util_percent: 95.91107266435985\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10893103415392481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0121411777469733\n",
      "    mean_inference_ms: 2.256786172804462\n",
      "    mean_raw_obs_processing_ms: 0.20710423991920954\n",
      "  time_since_restore: 45586.083255290985\n",
      "  time_this_iter_s: 218.29276204109192\n",
      "  time_total_s: 45586.083255290985\n",
      "  timers:\n",
      "    learn_throughput: 19.125\n",
      "    learn_time_ms: 209145.05\n",
      "    load_throughput: 2372780.064\n",
      "    load_time_ms: 1.686\n",
      "    sample_throughput: 18.287\n",
      "    sample_time_ms: 218738.157\n",
      "    update_time_ms: 13.511\n",
      "  timestamp: 1650263933\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 210\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-38-53\n",
      "  done: false\n",
      "  episode_len_mean: 1634.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 435\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0083342981903997e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.143498353251182e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.01412813551723957\n",
      "          total_loss: 0.01421199832111597\n",
      "          vf_explained_var: -4.4991892877987993e-08\n",
      "          vf_loss: 8.386849367525429e-05\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.1603448275862\n",
      "    ram_util_percent: 95.90172413793104\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1114743275100485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0072836368098776\n",
      "    mean_inference_ms: 2.3233244744405326\n",
      "    mean_raw_obs_processing_ms: 0.13574519643028757\n",
      "  time_since_restore: 45578.68683338165\n",
      "  time_this_iter_s: 218.00162506103516\n",
      "  time_total_s: 45578.68683338165\n",
      "  timers:\n",
      "    learn_throughput: 19.098\n",
      "    learn_time_ms: 209446.949\n",
      "    load_throughput: 6274905.936\n",
      "    load_time_ms: 0.637\n",
      "    sample_throughput: 18.281\n",
      "    sample_time_ms: 218801.492\n",
      "    update_time_ms: 7.94\n",
      "  timestamp: 1650263933\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 210\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 844000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 481.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2601\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7397216558456421\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011194686405360699\n",
      "          model: {}\n",
      "          policy_loss: -0.043874021619558334\n",
      "          total_loss: 0.029311727732419968\n",
      "          vf_explained_var: 0.6665341854095459\n",
      "          vf_loss: 0.0731857493519783\n",
      "    num_agent_steps_sampled: 844000\n",
      "    num_agent_steps_trained: 844000\n",
      "    num_steps_sampled: 844000\n",
      "    num_steps_trained: 844000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.16643835616438\n",
      "    ram_util_percent: 95.92808219178082\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10880571230842467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0111840716717488\n",
      "    mean_inference_ms: 2.2526193162162556\n",
      "    mean_raw_obs_processing_ms: 0.20679707940890452\n",
      "  time_since_restore: 45806.88868308067\n",
      "  time_this_iter_s: 220.8054277896881\n",
      "  time_total_s: 45806.88868308067\n",
      "  timers:\n",
      "    learn_throughput: 19.098\n",
      "    learn_time_ms: 209449.563\n",
      "    load_throughput: 2369261.707\n",
      "    load_time_ms: 1.688\n",
      "    sample_throughput: 18.288\n",
      "    sample_time_ms: 218724.806\n",
      "    update_time_ms: 13.774\n",
      "  timestamp: 1650264154\n",
      "  timesteps_since_restore: 844000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 844000\n",
      "  training_iteration: 211\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 844000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 1729.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 440\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4940432826356217e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.198752700878263e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0010209048632532358\n",
      "          total_loss: 0.0011069676838815212\n",
      "          vf_explained_var: -0.06666699796915054\n",
      "          vf_loss: 8.606095798313618e-05\n",
      "    num_agent_steps_sampled: 844000\n",
      "    num_agent_steps_trained: 844000\n",
      "    num_steps_sampled: 844000\n",
      "    num_steps_trained: 844000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.13780068728524\n",
      "    ram_util_percent: 95.90859106529209\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11115927005549957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0050016334296836\n",
      "    mean_inference_ms: 2.3130886816595506\n",
      "    mean_raw_obs_processing_ms: 0.13537235297438077\n",
      "  time_since_restore: 45799.47217321396\n",
      "  time_this_iter_s: 220.7853398323059\n",
      "  time_total_s: 45799.47217321396\n",
      "  timers:\n",
      "    learn_throughput: 19.074\n",
      "    learn_time_ms: 209711.96\n",
      "    load_throughput: 6311494.997\n",
      "    load_time_ms: 0.634\n",
      "    sample_throughput: 18.287\n",
      "    sample_time_ms: 218734.895\n",
      "    update_time_ms: 7.879\n",
      "  timestamp: 1650264154\n",
      "  timesteps_since_restore: 844000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 844000\n",
      "  training_iteration: 211\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 07:45:42 (running for 12:47:02.16)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.16 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 848000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-46-16\n",
      "  done: false\n",
      "  episode_len_mean: 1729.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 440\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128108508884907\n",
      "          total_loss: 0.014136175625026226\n",
      "          vf_explained_var: -1.4721706520504085e-07\n",
      "          vf_loss: 8.069729119597469e-06\n",
      "    num_agent_steps_sampled: 848000\n",
      "    num_agent_steps_trained: 848000\n",
      "    num_steps_sampled: 848000\n",
      "    num_steps_trained: 848000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.8179054054054\n",
      "    ram_util_percent: 95.72533783783783\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11115927005549957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0050016334296836\n",
      "    mean_inference_ms: 2.3130886816595506\n",
      "    mean_raw_obs_processing_ms: 0.13537235297438077\n",
      "  time_since_restore: 46021.65890312195\n",
      "  time_this_iter_s: 222.1867299079895\n",
      "  time_total_s: 46021.65890312195\n",
      "  timers:\n",
      "    learn_throughput: 19.049\n",
      "    learn_time_ms: 209983.734\n",
      "    load_throughput: 6378199.513\n",
      "    load_time_ms: 0.627\n",
      "    sample_throughput: 18.264\n",
      "    sample_time_ms: 219006.591\n",
      "    update_time_ms: 11.426\n",
      "  timestamp: 1650264376\n",
      "  timesteps_since_restore: 848000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 848000\n",
      "  training_iteration: 212\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 848000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-46-17\n",
      "  done: false\n",
      "  episode_len_mean: 480.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2611\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7795727849006653\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012954498641192913\n",
      "          model: {}\n",
      "          policy_loss: -0.04220397025346756\n",
      "          total_loss: 0.04954542592167854\n",
      "          vf_explained_var: 0.5564517378807068\n",
      "          vf_loss: 0.09174938499927521\n",
      "    num_agent_steps_sampled: 848000\n",
      "    num_agent_steps_trained: 848000\n",
      "    num_steps_sampled: 848000\n",
      "    num_steps_trained: 848000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.11050847457626\n",
      "    ram_util_percent: 95.72745762711862\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10864877287479366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0099949391578198\n",
      "    mean_inference_ms: 2.2474507187876167\n",
      "    mean_raw_obs_processing_ms: 0.20642014949870444\n",
      "  time_since_restore: 46029.938488960266\n",
      "  time_this_iter_s: 223.0498058795929\n",
      "  time_total_s: 46029.938488960266\n",
      "  timers:\n",
      "    learn_throughput: 19.066\n",
      "    learn_time_ms: 209802.751\n",
      "    load_throughput: 2328810.416\n",
      "    load_time_ms: 1.718\n",
      "    sample_throughput: 18.261\n",
      "    sample_time_ms: 219050.412\n",
      "    update_time_ms: 12.954\n",
      "  timestamp: 1650264377\n",
      "  timesteps_since_restore: 848000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 848000\n",
      "  training_iteration: 212\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 852000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-49-55\n",
      "  done: false\n",
      "  episode_len_mean: 1729.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 440\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412828266620636\n",
      "          total_loss: 0.01413189060986042\n",
      "          vf_explained_var: 6.96028408242455e-08\n",
      "          vf_loss: 3.611639385781018e-06\n",
      "    num_agent_steps_sampled: 852000\n",
      "    num_agent_steps_trained: 852000\n",
      "    num_steps_sampled: 852000\n",
      "    num_steps_trained: 852000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.13137931034484\n",
      "    ram_util_percent: 95.76896551724137\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11115927005549957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0050016334296836\n",
      "    mean_inference_ms: 2.3130886816595506\n",
      "    mean_raw_obs_processing_ms: 0.13537235297438077\n",
      "  time_since_restore: 46240.53655934334\n",
      "  time_this_iter_s: 218.87765622138977\n",
      "  time_total_s: 46240.53655934334\n",
      "  timers:\n",
      "    learn_throughput: 19.054\n",
      "    learn_time_ms: 209931.608\n",
      "    load_throughput: 6429776.568\n",
      "    load_time_ms: 0.622\n",
      "    sample_throughput: 18.234\n",
      "    sample_time_ms: 219365.47\n",
      "    update_time_ms: 12.487\n",
      "  timestamp: 1650264595\n",
      "  timesteps_since_restore: 852000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 852000\n",
      "  training_iteration: 213\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 852000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-49-56\n",
      "  done: false\n",
      "  episode_len_mean: 485.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.03\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2619\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7570745944976807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012488276697695255\n",
      "          model: {}\n",
      "          policy_loss: -0.04871169850230217\n",
      "          total_loss: 0.022959977388381958\n",
      "          vf_explained_var: 0.730767011642456\n",
      "          vf_loss: 0.07167167961597443\n",
      "    num_agent_steps_sampled: 852000\n",
      "    num_agent_steps_trained: 852000\n",
      "    num_steps_sampled: 852000\n",
      "    num_steps_trained: 852000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.80830449826989\n",
      "    ram_util_percent: 95.77093425605536\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10852453868865411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0090516158510652\n",
      "    mean_inference_ms: 2.2433405495350507\n",
      "    mean_raw_obs_processing_ms: 0.206121120465845\n",
      "  time_since_restore: 46248.72993969917\n",
      "  time_this_iter_s: 218.79145073890686\n",
      "  time_total_s: 46248.72993969917\n",
      "  timers:\n",
      "    learn_throughput: 19.063\n",
      "    learn_time_ms: 209832.123\n",
      "    load_throughput: 2430635.141\n",
      "    load_time_ms: 1.646\n",
      "    sample_throughput: 18.226\n",
      "    sample_time_ms: 219464.625\n",
      "    update_time_ms: 11.852\n",
      "  timestamp: 1650264596\n",
      "  timesteps_since_restore: 852000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 852000\n",
      "  training_iteration: 213\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 856000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-53-35\n",
      "  done: false\n",
      "  episode_len_mean: 1632.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 443\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.601809277567825e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.8188293926110517e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0009216968319378793\n",
      "          total_loss: -0.0006471521337516606\n",
      "          vf_explained_var: -0.00042902794666588306\n",
      "          vf_loss: 0.00027454778319224715\n",
      "    num_agent_steps_sampled: 856000\n",
      "    num_agent_steps_trained: 856000\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 856000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.53344827586207\n",
      "    ram_util_percent: 95.95241379310343\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11097288350431184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0036513116642722\n",
      "    mean_inference_ms: 2.3069976621791533\n",
      "    mean_raw_obs_processing_ms: 0.13515281395110137\n",
      "  time_since_restore: 46460.467606544495\n",
      "  time_this_iter_s: 219.93104720115662\n",
      "  time_total_s: 46460.467606544495\n",
      "  timers:\n",
      "    learn_throughput: 19.033\n",
      "    learn_time_ms: 210158.572\n",
      "    load_throughput: 7028872.596\n",
      "    load_time_ms: 0.569\n",
      "    sample_throughput: 18.238\n",
      "    sample_time_ms: 219317.477\n",
      "    update_time_ms: 13.651\n",
      "  timestamp: 1650264815\n",
      "  timesteps_since_restore: 856000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 214\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 856000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-53-36\n",
      "  done: false\n",
      "  episode_len_mean: 485.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.03\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2627\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7498422265052795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011800641193985939\n",
      "          model: {}\n",
      "          policy_loss: -0.04570282623171806\n",
      "          total_loss: 0.04108104109764099\n",
      "          vf_explained_var: 0.7524429559707642\n",
      "          vf_loss: 0.08678387105464935\n",
      "    num_agent_steps_sampled: 856000\n",
      "    num_agent_steps_trained: 856000\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 856000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.90068493150685\n",
      "    ram_util_percent: 95.93047945205478\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10840287340686902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0081291144266382\n",
      "    mean_inference_ms: 2.2393147913542117\n",
      "    mean_raw_obs_processing_ms: 0.2058336592164553\n",
      "  time_since_restore: 46469.20139455795\n",
      "  time_this_iter_s: 220.4714548587799\n",
      "  time_total_s: 46469.20139455795\n",
      "  timers:\n",
      "    learn_throughput: 19.043\n",
      "    learn_time_ms: 210047.168\n",
      "    load_throughput: 2870746.381\n",
      "    load_time_ms: 1.393\n",
      "    sample_throughput: 18.221\n",
      "    sample_time_ms: 219523.776\n",
      "    update_time_ms: 11.16\n",
      "  timestamp: 1650264816\n",
      "  timesteps_since_restore: 856000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 214\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 860000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-57-15\n",
      "  done: false\n",
      "  episode_len_mean: 1632.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 443\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128316193819046\n",
      "          total_loss: -0.014127549715340137\n",
      "          vf_explained_var: 3.2237781510957575e-07\n",
      "          vf_loss: 7.625377520525944e-07\n",
      "    num_agent_steps_sampled: 860000\n",
      "    num_agent_steps_trained: 860000\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.13801369863015\n",
      "    ram_util_percent: 95.97739726027397\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11097288350431184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0036513116642722\n",
      "    mean_inference_ms: 2.3069976621791533\n",
      "    mean_raw_obs_processing_ms: 0.13515281395110137\n",
      "  time_since_restore: 46680.74740552902\n",
      "  time_this_iter_s: 220.2797989845276\n",
      "  time_total_s: 46680.74740552902\n",
      "  timers:\n",
      "    learn_throughput: 19.015\n",
      "    learn_time_ms: 210363.28\n",
      "    load_throughput: 7170363.279\n",
      "    load_time_ms: 0.558\n",
      "    sample_throughput: 18.219\n",
      "    sample_time_ms: 219548.786\n",
      "    update_time_ms: 16.915\n",
      "  timestamp: 1650265035\n",
      "  timesteps_since_restore: 860000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 215\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 860000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_07-57-17\n",
      "  done: false\n",
      "  episode_len_mean: 486.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.15\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2635\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7398545742034912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013361969962716103\n",
      "          model: {}\n",
      "          policy_loss: -0.04379433020949364\n",
      "          total_loss: 0.03092406876385212\n",
      "          vf_explained_var: 0.7457917332649231\n",
      "          vf_loss: 0.07471839338541031\n",
      "    num_agent_steps_sampled: 860000\n",
      "    num_agent_steps_trained: 860000\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.70650684931506\n",
      "    ram_util_percent: 95.97739726027397\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10828204550504691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.007227063894503\n",
      "    mean_inference_ms: 2.2353753394325766\n",
      "    mean_raw_obs_processing_ms: 0.20555417935481357\n",
      "  time_since_restore: 46689.90101265907\n",
      "  time_this_iter_s: 220.69961810112\n",
      "  time_total_s: 46689.90101265907\n",
      "  timers:\n",
      "    learn_throughput: 19.022\n",
      "    learn_time_ms: 210281.776\n",
      "    load_throughput: 2855404.725\n",
      "    load_time_ms: 1.401\n",
      "    sample_throughput: 18.201\n",
      "    sample_time_ms: 219764.279\n",
      "    update_time_ms: 10.56\n",
      "  timestamp: 1650265037\n",
      "  timesteps_since_restore: 860000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 215\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 864000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-00-54\n",
      "  done: false\n",
      "  episode_len_mean: 1532.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 447\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.9565610636507057e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.2406090133968285e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.005992541555315256\n",
      "          total_loss: -0.005952294450253248\n",
      "          vf_explained_var: -0.04147212207317352\n",
      "          vf_loss: 4.024895315524191e-05\n",
      "    num_agent_steps_sampled: 864000\n",
      "    num_agent_steps_trained: 864000\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.4006896551724\n",
      "    ram_util_percent: 95.58551724137932\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11074372614218934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0019910005794948\n",
      "    mean_inference_ms: 2.299500274228482\n",
      "    mean_raw_obs_processing_ms: 0.13488891496675784\n",
      "  time_since_restore: 46899.25750541687\n",
      "  time_this_iter_s: 218.5100998878479\n",
      "  time_total_s: 46899.25750541687\n",
      "  timers:\n",
      "    learn_throughput: 19.024\n",
      "    learn_time_ms: 210256.658\n",
      "    load_throughput: 7270732.828\n",
      "    load_time_ms: 0.55\n",
      "    sample_throughput: 18.194\n",
      "    sample_time_ms: 219855.143\n",
      "    update_time_ms: 17.963\n",
      "  timestamp: 1650265254\n",
      "  timesteps_since_restore: 864000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 216\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 864000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-00-55\n",
      "  done: false\n",
      "  episode_len_mean: 490.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.22\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2643\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7648522853851318\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012311743572354317\n",
      "          model: {}\n",
      "          policy_loss: -0.05167802423238754\n",
      "          total_loss: 0.014599345624446869\n",
      "          vf_explained_var: 0.7403032183647156\n",
      "          vf_loss: 0.06627736985683441\n",
      "    num_agent_steps_sampled: 864000\n",
      "    num_agent_steps_trained: 864000\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.89758620689655\n",
      "    ram_util_percent: 95.58310344827588\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1081637356591279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0063314109962105\n",
      "    mean_inference_ms: 2.231475981296902\n",
      "    mean_raw_obs_processing_ms: 0.2052744646587201\n",
      "  time_since_restore: 46908.360340833664\n",
      "  time_this_iter_s: 218.45932817459106\n",
      "  time_total_s: 46908.360340833664\n",
      "  timers:\n",
      "    learn_throughput: 19.023\n",
      "    learn_time_ms: 210269.495\n",
      "    load_throughput: 2825779.155\n",
      "    load_time_ms: 1.416\n",
      "    sample_throughput: 18.177\n",
      "    sample_time_ms: 220055.22\n",
      "    update_time_ms: 9.978\n",
      "  timestamp: 1650265255\n",
      "  timesteps_since_restore: 864000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 216\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 08:02:22 (running for 13:03:42.56)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.22 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 868000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 1533.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 450\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.8697301052934244e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.1599165931798866e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0007087829289957881\n",
      "          total_loss: 0.0008480548858642578\n",
      "          vf_explained_var: 0.06432121247053146\n",
      "          vf_loss: 0.00013927483814768493\n",
      "    num_agent_steps_sampled: 868000\n",
      "    num_agent_steps_trained: 868000\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 868000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.6242214532872\n",
      "    ram_util_percent: 95.61695501730105\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1105780943714114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0007915428837904\n",
      "    mean_inference_ms: 2.294076518877305\n",
      "    mean_raw_obs_processing_ms: 0.13470171748041407\n",
      "  time_since_restore: 47116.35773944855\n",
      "  time_this_iter_s: 217.10023403167725\n",
      "  time_total_s: 47116.35773944855\n",
      "  timers:\n",
      "    learn_throughput: 19.046\n",
      "    learn_time_ms: 210013.443\n",
      "    load_throughput: 7417639.049\n",
      "    load_time_ms: 0.539\n",
      "    sample_throughput: 18.198\n",
      "    sample_time_ms: 219801.859\n",
      "    update_time_ms: 18.854\n",
      "  timestamp: 1650265471\n",
      "  timesteps_since_restore: 868000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 217\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 868000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-04-33\n",
      "  done: false\n",
      "  episode_len_mean: 497.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2651\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7211757898330688\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013898300006985664\n",
      "          model: {}\n",
      "          policy_loss: -0.043179936707019806\n",
      "          total_loss: 0.04304749518632889\n",
      "          vf_explained_var: 0.6623077988624573\n",
      "          vf_loss: 0.0862274318933487\n",
      "    num_agent_steps_sampled: 868000\n",
      "    num_agent_steps_trained: 868000\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 868000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.49448275862068\n",
      "    ram_util_percent: 95.59241379310346\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10804559658234911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0054320372201406\n",
      "    mean_inference_ms: 2.2275947666683122\n",
      "    mean_raw_obs_processing_ms: 0.20499033391918553\n",
      "  time_since_restore: 47125.58220076561\n",
      "  time_this_iter_s: 217.2218599319458\n",
      "  time_total_s: 47125.58220076561\n",
      "  timers:\n",
      "    learn_throughput: 19.054\n",
      "    learn_time_ms: 209926.531\n",
      "    load_throughput: 2812373.816\n",
      "    load_time_ms: 1.422\n",
      "    sample_throughput: 18.175\n",
      "    sample_time_ms: 220080.247\n",
      "    update_time_ms: 9.064\n",
      "  timestamp: 1650265473\n",
      "  timesteps_since_restore: 868000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 217\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 872000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-08-09\n",
      "  done: false\n",
      "  episode_len_mean: 1533.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 450\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0065642716363072395\n",
      "          total_loss: -0.0065642716363072395\n",
      "          vf_explained_var: 1.0464780643815175e-05\n",
      "          vf_loss: 2.69731037505494e-09\n",
      "    num_agent_steps_sampled: 872000\n",
      "    num_agent_steps_trained: 872000\n",
      "    num_steps_sampled: 872000\n",
      "    num_steps_trained: 872000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.38827586206897\n",
      "    ram_util_percent: 95.71689655172415\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1105780943714114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0007915428837904\n",
      "    mean_inference_ms: 2.294076518877305\n",
      "    mean_raw_obs_processing_ms: 0.13470171748041407\n",
      "  time_since_restore: 47334.28814935684\n",
      "  time_this_iter_s: 217.93040990829468\n",
      "  time_total_s: 47334.28814935684\n",
      "  timers:\n",
      "    learn_throughput: 19.068\n",
      "    learn_time_ms: 209777.141\n",
      "    load_throughput: 7404217.309\n",
      "    load_time_ms: 0.54\n",
      "    sample_throughput: 18.215\n",
      "    sample_time_ms: 219597.888\n",
      "    update_time_ms: 20.285\n",
      "  timestamp: 1650265689\n",
      "  timesteps_since_restore: 872000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 872000\n",
      "  training_iteration: 218\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 872000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 498.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2660\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7463346719741821\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011239388957619667\n",
      "          model: {}\n",
      "          policy_loss: -0.037734080106019974\n",
      "          total_loss: 0.03578978404402733\n",
      "          vf_explained_var: 0.7511497735977173\n",
      "          vf_loss: 0.0735238716006279\n",
      "    num_agent_steps_sampled: 872000\n",
      "    num_agent_steps_trained: 872000\n",
      "    num_steps_sampled: 872000\n",
      "    num_steps_trained: 872000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.07577854671281\n",
      "    ram_util_percent: 95.72595155709342\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10791379533087463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0044349034917937\n",
      "    mean_inference_ms: 2.2232991817726817\n",
      "    mean_raw_obs_processing_ms: 0.2046755767618866\n",
      "  time_since_restore: 47343.37803149223\n",
      "  time_this_iter_s: 217.79583072662354\n",
      "  time_total_s: 47343.37803149223\n",
      "  timers:\n",
      "    learn_throughput: 19.081\n",
      "    learn_time_ms: 209637.09\n",
      "    load_throughput: 2817805.845\n",
      "    load_time_ms: 1.42\n",
      "    sample_throughput: 18.196\n",
      "    sample_time_ms: 219829.312\n",
      "    update_time_ms: 9.032\n",
      "  timestamp: 1650265690\n",
      "  timesteps_since_restore: 872000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 872000\n",
      "  training_iteration: 218\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 876000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-11-49\n",
      "  done: false\n",
      "  episode_len_mean: 1534.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 455\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.8841870252023492e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.8472609334150683e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0009823833825066686\n",
      "          total_loss: -0.0006198172923177481\n",
      "          vf_explained_var: 0.09543202817440033\n",
      "          vf_loss: 0.0003625695244409144\n",
      "    num_agent_steps_sampled: 876000\n",
      "    num_agent_steps_trained: 876000\n",
      "    num_steps_sampled: 876000\n",
      "    num_steps_trained: 876000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.55463917525773\n",
      "    ram_util_percent: 95.85223367697593\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11030776249330793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9988125290575502\n",
      "    mean_inference_ms: 2.285182646600756\n",
      "    mean_raw_obs_processing_ms: 0.13439179629958345\n",
      "  time_since_restore: 47554.36300110817\n",
      "  time_this_iter_s: 220.07485175132751\n",
      "  time_total_s: 47554.36300110817\n",
      "  timers:\n",
      "    learn_throughput: 19.066\n",
      "    learn_time_ms: 209796.659\n",
      "    load_throughput: 7162098.613\n",
      "    load_time_ms: 0.558\n",
      "    sample_throughput: 18.228\n",
      "    sample_time_ms: 219441.856\n",
      "    update_time_ms: 20.654\n",
      "  timestamp: 1650265909\n",
      "  timesteps_since_restore: 876000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 876000\n",
      "  training_iteration: 219\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 876000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-11-49\n",
      "  done: false\n",
      "  episode_len_mean: 490.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.2\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2669\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8407629132270813\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011338741518557072\n",
      "          model: {}\n",
      "          policy_loss: -0.048529792577028275\n",
      "          total_loss: 0.01681572198867798\n",
      "          vf_explained_var: 0.7639493942260742\n",
      "          vf_loss: 0.06534550338983536\n",
      "    num_agent_steps_sampled: 876000\n",
      "    num_agent_steps_trained: 876000\n",
      "    num_steps_sampled: 876000\n",
      "    num_steps_trained: 876000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.20276816608997\n",
      "    ram_util_percent: 95.84394463667819\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10778554606479483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0034619213481133\n",
      "    mean_inference_ms: 2.219097452972217\n",
      "    mean_raw_obs_processing_ms: 0.2043742413763017\n",
      "  time_since_restore: 47562.47331237793\n",
      "  time_this_iter_s: 219.0952808856964\n",
      "  time_total_s: 47562.47331237793\n",
      "  timers:\n",
      "    learn_throughput: 19.085\n",
      "    learn_time_ms: 209588.703\n",
      "    load_throughput: 2425890.484\n",
      "    load_time_ms: 1.649\n",
      "    sample_throughput: 18.217\n",
      "    sample_time_ms: 219578.512\n",
      "    update_time_ms: 9.488\n",
      "  timestamp: 1650265909\n",
      "  timesteps_since_restore: 876000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 876000\n",
      "  training_iteration: 219\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-15-28\n",
      "  done: false\n",
      "  episode_len_mean: 1534.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 455\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128383249044418\n",
      "          total_loss: -0.014112480916082859\n",
      "          vf_explained_var: 4.749144366655855e-08\n",
      "          vf_loss: 1.5902240193099715e-05\n",
      "    num_agent_steps_sampled: 880000\n",
      "    num_agent_steps_trained: 880000\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.10657439446365\n",
      "    ram_util_percent: 95.69861591695502\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11030776249330793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9988125290575502\n",
      "    mean_inference_ms: 2.285182646600756\n",
      "    mean_raw_obs_processing_ms: 0.13439179629958345\n",
      "  time_since_restore: 47773.59516119957\n",
      "  time_this_iter_s: 219.23216009140015\n",
      "  time_total_s: 47773.59516119957\n",
      "  timers:\n",
      "    learn_throughput: 19.054\n",
      "    learn_time_ms: 209925.647\n",
      "    load_throughput: 7164239.474\n",
      "    load_time_ms: 0.558\n",
      "    sample_throughput: 18.227\n",
      "    sample_time_ms: 219458.19\n",
      "    update_time_ms: 22.309\n",
      "  timestamp: 1650266128\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 220\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-15-29\n",
      "  done: false\n",
      "  episode_len_mean: 495.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2677\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7526640295982361\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013319462537765503\n",
      "          model: {}\n",
      "          policy_loss: -0.04174378886818886\n",
      "          total_loss: 0.08225462585687637\n",
      "          vf_explained_var: 0.614880383014679\n",
      "          vf_loss: 0.12399841099977493\n",
      "    num_agent_steps_sampled: 880000\n",
      "    num_agent_steps_trained: 880000\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.77827586206898\n",
      "    ram_util_percent: 95.69310344827586\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1076750799272873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0026159802484231\n",
      "    mean_inference_ms: 2.2154330998798115\n",
      "    mean_raw_obs_processing_ms: 0.20410823258833333\n",
      "  time_since_restore: 47781.49456334114\n",
      "  time_this_iter_s: 219.02125096321106\n",
      "  time_total_s: 47781.49456334114\n",
      "  timers:\n",
      "    learn_throughput: 19.08\n",
      "    learn_time_ms: 209647.038\n",
      "    load_throughput: 3026957.745\n",
      "    load_time_ms: 1.321\n",
      "    sample_throughput: 18.22\n",
      "    sample_time_ms: 219540.526\n",
      "    update_time_ms: 9.426\n",
      "  timestamp: 1650266129\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 220\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 08:19:02 (running for 13:20:22.61)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.29 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 884000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-19-05\n",
      "  done: false\n",
      "  episode_len_mean: 508.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.56\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2685\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6801813840866089\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012810426764190197\n",
      "          model: {}\n",
      "          policy_loss: -0.05158991366624832\n",
      "          total_loss: 0.0450759157538414\n",
      "          vf_explained_var: 0.6530236601829529\n",
      "          vf_loss: 0.09666582942008972\n",
      "    num_agent_steps_sampled: 884000\n",
      "    num_agent_steps_trained: 884000\n",
      "    num_steps_sampled: 884000\n",
      "    num_steps_trained: 884000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.50416666666666\n",
      "    ram_util_percent: 95.72118055555556\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10756221426944798\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0017477366534107\n",
      "    mean_inference_ms: 2.211659034371308\n",
      "    mean_raw_obs_processing_ms: 0.20383062181888506\n",
      "  time_since_restore: 47998.14117407799\n",
      "  time_this_iter_s: 216.64661073684692\n",
      "  time_total_s: 47998.14117407799\n",
      "  timers:\n",
      "    learn_throughput: 19.117\n",
      "    learn_time_ms: 209232.513\n",
      "    load_throughput: 3040782.977\n",
      "    load_time_ms: 1.315\n",
      "    sample_throughput: 18.215\n",
      "    sample_time_ms: 219597.185\n",
      "    update_time_ms: 9.861\n",
      "  timestamp: 1650266345\n",
      "  timesteps_since_restore: 884000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 884000\n",
      "  training_iteration: 221\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 884000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-19-05\n",
      "  done: false\n",
      "  episode_len_mean: 1534.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 455\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0141279436647892\n",
      "          total_loss: -0.014123537577688694\n",
      "          vf_explained_var: 1.9765668923810154e-07\n",
      "          vf_loss: 4.405883828439983e-06\n",
      "    num_agent_steps_sampled: 884000\n",
      "    num_agent_steps_trained: 884000\n",
      "    num_steps_sampled: 884000\n",
      "    num_steps_trained: 884000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.50659722222221\n",
      "    ram_util_percent: 95.72534722222223\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11030776249330793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9988125290575502\n",
      "    mean_inference_ms: 2.285182646600756\n",
      "    mean_raw_obs_processing_ms: 0.13439179629958345\n",
      "  time_since_restore: 47990.4033472538\n",
      "  time_this_iter_s: 216.80818605422974\n",
      "  time_total_s: 47990.4033472538\n",
      "  timers:\n",
      "    learn_throughput: 19.089\n",
      "    learn_time_ms: 209542.809\n",
      "    load_throughput: 7176804.551\n",
      "    load_time_ms: 0.557\n",
      "    sample_throughput: 18.217\n",
      "    sample_time_ms: 219580.849\n",
      "    update_time_ms: 22.188\n",
      "  timestamp: 1650266345\n",
      "  timesteps_since_restore: 884000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 884000\n",
      "  training_iteration: 221\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 888000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-22-44\n",
      "  done: false\n",
      "  episode_len_mean: 515.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.71\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2692\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6629250645637512\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01430347841233015\n",
      "          model: {}\n",
      "          policy_loss: -0.04581500589847565\n",
      "          total_loss: 0.038841988891363144\n",
      "          vf_explained_var: 0.7319844961166382\n",
      "          vf_loss: 0.08465699106454849\n",
      "    num_agent_steps_sampled: 888000\n",
      "    num_agent_steps_trained: 888000\n",
      "    num_steps_sampled: 888000\n",
      "    num_steps_trained: 888000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.52758620689656\n",
      "    ram_util_percent: 95.75068965517241\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10746256672329867\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.000982776511234\n",
      "    mean_inference_ms: 2.2083327530189787\n",
      "    mean_raw_obs_processing_ms: 0.2035844532932569\n",
      "  time_since_restore: 48217.15212225914\n",
      "  time_this_iter_s: 219.01094818115234\n",
      "  time_total_s: 48217.15212225914\n",
      "  timers:\n",
      "    learn_throughput: 19.152\n",
      "    learn_time_ms: 208860.578\n",
      "    load_throughput: 2121308.399\n",
      "    load_time_ms: 1.886\n",
      "    sample_throughput: 18.253\n",
      "    sample_time_ms: 219147.021\n",
      "    update_time_ms: 9.852\n",
      "  timestamp: 1650266564\n",
      "  timesteps_since_restore: 888000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 888000\n",
      "  training_iteration: 222\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 888000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-22-45\n",
      "  done: false\n",
      "  episode_len_mean: 1631.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 457\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1670927467122295e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.2440290525468702e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0007030880660749972\n",
      "          total_loss: -0.0006318306550383568\n",
      "          vf_explained_var: 0.09758249670267105\n",
      "          vf_loss: 7.125975389499217e-05\n",
      "    num_agent_steps_sampled: 888000\n",
      "    num_agent_steps_trained: 888000\n",
      "    num_steps_sampled: 888000\n",
      "    num_steps_trained: 888000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.35137931034484\n",
      "    ram_util_percent: 95.77103448275862\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11019339567122496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9979752632318621\n",
      "    mean_inference_ms: 2.2814230868909395\n",
      "    mean_raw_obs_processing_ms: 0.13425772568526118\n",
      "  time_since_restore: 48209.639218091965\n",
      "  time_this_iter_s: 219.23587083816528\n",
      "  time_total_s: 48209.639218091965\n",
      "  timers:\n",
      "    learn_throughput: 19.116\n",
      "    learn_time_ms: 209243.48\n",
      "    load_throughput: 7072428.969\n",
      "    load_time_ms: 0.566\n",
      "    sample_throughput: 18.249\n",
      "    sample_time_ms: 219194.956\n",
      "    update_time_ms: 18.397\n",
      "  timestamp: 1650266565\n",
      "  timesteps_since_restore: 888000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 888000\n",
      "  training_iteration: 222\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 892000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-26-24\n",
      "  done: false\n",
      "  episode_len_mean: 1631.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 457\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128169044852257\n",
      "          total_loss: 0.014128358103334904\n",
      "          vf_explained_var: 3.293637291790219e-07\n",
      "          vf_loss: 1.928600994460794e-07\n",
      "    num_agent_steps_sampled: 892000\n",
      "    num_agent_steps_trained: 892000\n",
      "    num_steps_sampled: 892000\n",
      "    num_steps_trained: 892000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.78137931034482\n",
      "    ram_util_percent: 95.94241379310343\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11019339567122496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9979752632318621\n",
      "    mean_inference_ms: 2.2814230868909395\n",
      "    mean_raw_obs_processing_ms: 0.13425772568526118\n",
      "  time_since_restore: 48428.4922952652\n",
      "  time_this_iter_s: 218.85307717323303\n",
      "  time_total_s: 48428.4922952652\n",
      "  timers:\n",
      "    learn_throughput: 19.112\n",
      "    learn_time_ms: 209290.497\n",
      "    load_throughput: 7130744.645\n",
      "    load_time_ms: 0.561\n",
      "    sample_throughput: 18.278\n",
      "    sample_time_ms: 218840.951\n",
      "    update_time_ms: 18.114\n",
      "  timestamp: 1650266784\n",
      "  timesteps_since_restore: 892000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 892000\n",
      "  training_iteration: 223\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 892000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-26-24\n",
      "  done: false\n",
      "  episode_len_mean: 517.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.73\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2700\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.704956591129303\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01445632055401802\n",
      "          model: {}\n",
      "          policy_loss: -0.05348431318998337\n",
      "          total_loss: 0.040754321962594986\n",
      "          vf_explained_var: 0.6199550032615662\n",
      "          vf_loss: 0.09423863142728806\n",
      "    num_agent_steps_sampled: 892000\n",
      "    num_agent_steps_trained: 892000\n",
      "    num_steps_sampled: 892000\n",
      "    num_steps_trained: 892000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.47079037800687\n",
      "    ram_util_percent: 95.92164948453608\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10734845502019447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0001074979216042\n",
      "    mean_inference_ms: 2.204527204040744\n",
      "    mean_raw_obs_processing_ms: 0.20330343876159465\n",
      "  time_since_restore: 48436.873848199844\n",
      "  time_this_iter_s: 219.72172594070435\n",
      "  time_total_s: 48436.873848199844\n",
      "  timers:\n",
      "    learn_throughput: 19.14\n",
      "    learn_time_ms: 208982.48\n",
      "    load_throughput: 2145533.787\n",
      "    load_time_ms: 1.864\n",
      "    sample_throughput: 18.286\n",
      "    sample_time_ms: 218751.656\n",
      "    update_time_ms: 10.235\n",
      "  timestamp: 1650266784\n",
      "  timesteps_since_restore: 892000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 892000\n",
      "  training_iteration: 223\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 896000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-30-05\n",
      "  done: false\n",
      "  episode_len_mean: 514.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.71\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2709\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8348440527915955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012012124992907047\n",
      "          model: {}\n",
      "          policy_loss: -0.05058085918426514\n",
      "          total_loss: 0.03576739877462387\n",
      "          vf_explained_var: 0.6680687069892883\n",
      "          vf_loss: 0.08634825795888901\n",
      "    num_agent_steps_sampled: 896000\n",
      "    num_agent_steps_trained: 896000\n",
      "    num_steps_sampled: 896000\n",
      "    num_steps_trained: 896000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.61099656357388\n",
      "    ram_util_percent: 95.96872852233676\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10722263967695209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9991366131748607\n",
      "    mean_inference_ms: 2.2002876999310526\n",
      "    mean_raw_obs_processing_ms: 0.20298851776961058\n",
      "  time_since_restore: 48657.84716010094\n",
      "  time_this_iter_s: 220.97331190109253\n",
      "  time_total_s: 48657.84716010094\n",
      "  timers:\n",
      "    learn_throughput: 19.13\n",
      "    learn_time_ms: 209094.482\n",
      "    load_throughput: 2321174.338\n",
      "    load_time_ms: 1.723\n",
      "    sample_throughput: 18.281\n",
      "    sample_time_ms: 218810.324\n",
      "    update_time_ms: 11.044\n",
      "  timestamp: 1650267005\n",
      "  timesteps_since_restore: 896000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 896000\n",
      "  training_iteration: 224\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 896000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-30-05\n",
      "  done: false\n",
      "  episode_len_mean: 1727.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 459\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.450269757613633e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7185381042558407e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.008295317180454731\n",
      "          total_loss: -0.008288899436593056\n",
      "          vf_explained_var: -0.03225846588611603\n",
      "          vf_loss: 6.421869784389855e-06\n",
      "    num_agent_steps_sampled: 896000\n",
      "    num_agent_steps_trained: 896000\n",
      "    num_steps_sampled: 896000\n",
      "    num_steps_trained: 896000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.76621160409556\n",
      "    ram_util_percent: 95.97952218430034\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11007384232654584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9971020397096743\n",
      "    mean_inference_ms: 2.277499025653307\n",
      "    mean_raw_obs_processing_ms: 0.13411561674584505\n",
      "  time_since_restore: 48650.13323903084\n",
      "  time_this_iter_s: 221.64094376564026\n",
      "  time_total_s: 48650.13323903084\n",
      "  timers:\n",
      "    learn_throughput: 19.091\n",
      "    learn_time_ms: 209517.711\n",
      "    load_throughput: 7212594.471\n",
      "    load_time_ms: 0.555\n",
      "    sample_throughput: 18.279\n",
      "    sample_time_ms: 218833.646\n",
      "    update_time_ms: 17.018\n",
      "  timestamp: 1650267005\n",
      "  timesteps_since_restore: 896000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 896000\n",
      "  training_iteration: 224\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 900000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-33-44\n",
      "  done: false\n",
      "  episode_len_mean: 516.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.82\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2717\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7275933623313904\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013408195227384567\n",
      "          model: {}\n",
      "          policy_loss: -0.0450051985681057\n",
      "          total_loss: 0.024985356256365776\n",
      "          vf_explained_var: 0.7620596289634705\n",
      "          vf_loss: 0.06999054551124573\n",
      "    num_agent_steps_sampled: 900000\n",
      "    num_agent_steps_trained: 900000\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.31137931034482\n",
      "    ram_util_percent: 95.93206896551723\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10710885964072656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9982648072261184\n",
      "    mean_inference_ms: 2.196488604204394\n",
      "    mean_raw_obs_processing_ms: 0.20270143911716676\n",
      "  time_since_restore: 48876.1787071228\n",
      "  time_this_iter_s: 218.33154702186584\n",
      "  time_total_s: 48876.1787071228\n",
      "  timers:\n",
      "    learn_throughput: 19.147\n",
      "    learn_time_ms: 208905.696\n",
      "    load_throughput: 2337017.649\n",
      "    load_time_ms: 1.712\n",
      "    sample_throughput: 18.277\n",
      "    sample_time_ms: 218850.4\n",
      "    update_time_ms: 11.628\n",
      "  timestamp: 1650267224\n",
      "  timesteps_since_restore: 900000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 225\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 900000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-33-45\n",
      "  done: false\n",
      "  episode_len_mean: 1534.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 466\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.893944205602372e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5867375009428903e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0007992761093191803\n",
      "          total_loss: 0.0012087621726095676\n",
      "          vf_explained_var: 0.35360920429229736\n",
      "          vf_loss: 0.0004094822215847671\n",
      "    num_agent_steps_sampled: 900000\n",
      "    num_agent_steps_trained: 900000\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.37724137931035\n",
      "    ram_util_percent: 95.94103448275862\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10967292602951254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9941738856552889\n",
      "    mean_inference_ms: 2.2643065271829723\n",
      "    mean_raw_obs_processing_ms: 0.133652079693634\n",
      "  time_since_restore: 48869.42568016052\n",
      "  time_this_iter_s: 219.29244112968445\n",
      "  time_total_s: 48869.42568016052\n",
      "  timers:\n",
      "    learn_throughput: 19.098\n",
      "    learn_time_ms: 209441.462\n",
      "    load_throughput: 7058741.165\n",
      "    load_time_ms: 0.567\n",
      "    sample_throughput: 18.265\n",
      "    sample_time_ms: 218997.844\n",
      "    update_time_ms: 13.671\n",
      "  timestamp: 1650267225\n",
      "  timesteps_since_restore: 900000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 225\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 08:35:43 (running for 13:37:03.39)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.82 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 904000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-37-23\n",
      "  done: false\n",
      "  episode_len_mean: 517.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 6.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2725\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6933665871620178\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015535252168774605\n",
      "          model: {}\n",
      "          policy_loss: -0.039883580058813095\n",
      "          total_loss: 0.0499209389090538\n",
      "          vf_explained_var: 0.6849396228790283\n",
      "          vf_loss: 0.0898045226931572\n",
      "    num_agent_steps_sampled: 904000\n",
      "    num_agent_steps_trained: 904000\n",
      "    num_steps_sampled: 904000\n",
      "    num_steps_trained: 904000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.48310344827586\n",
      "    ram_util_percent: 95.9503448275862\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10699571169627983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9973958717231233\n",
      "    mean_inference_ms: 2.192715816288618\n",
      "    mean_raw_obs_processing_ms: 0.2024124830160055\n",
      "  time_since_restore: 49095.0172598362\n",
      "  time_this_iter_s: 218.83855271339417\n",
      "  time_total_s: 49095.0172598362\n",
      "  timers:\n",
      "    learn_throughput: 19.14\n",
      "    learn_time_ms: 208990.225\n",
      "    load_throughput: 2350342.663\n",
      "    load_time_ms: 1.702\n",
      "    sample_throughput: 18.297\n",
      "    sample_time_ms: 218619.463\n",
      "    update_time_ms: 11.697\n",
      "  timestamp: 1650267443\n",
      "  timesteps_since_restore: 904000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 904000\n",
      "  training_iteration: 226\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 904000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-37-23\n",
      "  done: false\n",
      "  episode_len_mean: 1534.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 466\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128077775239944\n",
      "          total_loss: 0.014128210954368114\n",
      "          vf_explained_var: 1.3312985629454488e-06\n",
      "          vf_loss: 1.3346998173346947e-07\n",
      "    num_agent_steps_sampled: 904000\n",
      "    num_agent_steps_trained: 904000\n",
      "    num_steps_sampled: 904000\n",
      "    num_steps_trained: 904000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.60934256055363\n",
      "    ram_util_percent: 95.9235294117647\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10967292602951254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9941738856552889\n",
      "    mean_inference_ms: 2.2643065271829723\n",
      "    mean_raw_obs_processing_ms: 0.133652079693634\n",
      "  time_since_restore: 49087.54732322693\n",
      "  time_this_iter_s: 218.12164306640625\n",
      "  time_total_s: 49087.54732322693\n",
      "  timers:\n",
      "    learn_throughput: 19.094\n",
      "    learn_time_ms: 209493.382\n",
      "    load_throughput: 7151108.648\n",
      "    load_time_ms: 0.559\n",
      "    sample_throughput: 18.28\n",
      "    sample_time_ms: 218821.31\n",
      "    update_time_ms: 12.28\n",
      "  timestamp: 1650267443\n",
      "  timesteps_since_restore: 904000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 904000\n",
      "  training_iteration: 226\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 908000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-41-01\n",
      "  done: false\n",
      "  episode_len_mean: 514.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.77\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2733\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6945250034332275\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013669697567820549\n",
      "          model: {}\n",
      "          policy_loss: -0.04160305857658386\n",
      "          total_loss: 0.03444591164588928\n",
      "          vf_explained_var: 0.7340407371520996\n",
      "          vf_loss: 0.07604897022247314\n",
      "    num_agent_steps_sampled: 908000\n",
      "    num_agent_steps_trained: 908000\n",
      "    num_steps_sampled: 908000\n",
      "    num_steps_trained: 908000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.87896551724137\n",
      "    ram_util_percent: 95.88896551724136\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10688457801840417\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9965303189421445\n",
      "    mean_inference_ms: 2.1889493465069396\n",
      "    mean_raw_obs_processing_ms: 0.20212602693949713\n",
      "  time_since_restore: 49313.80603957176\n",
      "  time_this_iter_s: 218.78877973556519\n",
      "  time_total_s: 49313.80603957176\n",
      "  timers:\n",
      "    learn_throughput: 19.117\n",
      "    learn_time_ms: 209235.391\n",
      "    load_throughput: 2339233.418\n",
      "    load_time_ms: 1.71\n",
      "    sample_throughput: 18.297\n",
      "    sample_time_ms: 218616.755\n",
      "    update_time_ms: 11.683\n",
      "  timestamp: 1650267661\n",
      "  timesteps_since_restore: 908000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 908000\n",
      "  training_iteration: 227\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 908000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-41-01\n",
      "  done: false\n",
      "  episode_len_mean: 1630.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 467\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.390662964000674e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.423924941396031e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.001162420492619276\n",
      "          total_loss: 0.00116545963101089\n",
      "          vf_explained_var: -0.016583193093538284\n",
      "          vf_loss: 3.0385306217795005e-06\n",
      "    num_agent_steps_sampled: 908000\n",
      "    num_agent_steps_trained: 908000\n",
      "    num_steps_sampled: 908000\n",
      "    num_steps_trained: 908000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.28724137931034\n",
      "    ram_util_percent: 95.87137931034482\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10961766248241914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9937704313100878\n",
      "    mean_inference_ms: 2.26248590273554\n",
      "    mean_raw_obs_processing_ms: 0.1335876985188628\n",
      "  time_since_restore: 49306.184883356094\n",
      "  time_this_iter_s: 218.63756012916565\n",
      "  time_total_s: 49306.184883356094\n",
      "  timers:\n",
      "    learn_throughput: 19.074\n",
      "    learn_time_ms: 209705.338\n",
      "    load_throughput: 7015939.447\n",
      "    load_time_ms: 0.57\n",
      "    sample_throughput: 18.28\n",
      "    sample_time_ms: 218813.013\n",
      "    update_time_ms: 11.763\n",
      "  timestamp: 1650267661\n",
      "  timesteps_since_restore: 908000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 908000\n",
      "  training_iteration: 227\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 912000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-44-43\n",
      "  done: false\n",
      "  episode_len_mean: 519.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.92\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2741\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7289995551109314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01655813865363598\n",
      "          model: {}\n",
      "          policy_loss: -0.041161857545375824\n",
      "          total_loss: 0.028725827112793922\n",
      "          vf_explained_var: 0.7662211656570435\n",
      "          vf_loss: 0.0698876827955246\n",
      "    num_agent_steps_sampled: 912000\n",
      "    num_agent_steps_trained: 912000\n",
      "    num_steps_sampled: 912000\n",
      "    num_steps_trained: 912000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.58835616438357\n",
      "    ram_util_percent: 95.811301369863\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10677157139630841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9956610319960169\n",
      "    mean_inference_ms: 2.185166464553467\n",
      "    mean_raw_obs_processing_ms: 0.20183674543862182\n",
      "  time_since_restore: 49534.976138830185\n",
      "  time_this_iter_s: 221.17009925842285\n",
      "  time_total_s: 49534.976138830185\n",
      "  timers:\n",
      "    learn_throughput: 19.079\n",
      "    learn_time_ms: 209659.551\n",
      "    load_throughput: 2295984.235\n",
      "    load_time_ms: 1.742\n",
      "    sample_throughput: 18.284\n",
      "    sample_time_ms: 218776.054\n",
      "    update_time_ms: 14.217\n",
      "  timestamp: 1650267883\n",
      "  timesteps_since_restore: 912000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 912000\n",
      "  training_iteration: 228\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 912000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-44-43\n",
      "  done: false\n",
      "  episode_len_mean: 1630.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 467\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.178589006263068e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.7872649800850993e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.01412828080356121\n",
      "          total_loss: 0.014129050076007843\n",
      "          vf_explained_var: 5.457990823742875e-07\n",
      "          vf_loss: 7.727882120889262e-07\n",
      "    num_agent_steps_sampled: 912000\n",
      "    num_agent_steps_trained: 912000\n",
      "    num_steps_sampled: 912000\n",
      "    num_steps_trained: 912000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.9318493150685\n",
      "    ram_util_percent: 95.80547945205478\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10961766248241914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9937704313100878\n",
      "    mean_inference_ms: 2.26248590273554\n",
      "    mean_raw_obs_processing_ms: 0.1335876985188628\n",
      "  time_since_restore: 49527.36926627159\n",
      "  time_this_iter_s: 221.18438291549683\n",
      "  time_total_s: 49527.36926627159\n",
      "  timers:\n",
      "    learn_throughput: 19.038\n",
      "    learn_time_ms: 210108.647\n",
      "    load_throughput: 7193729.526\n",
      "    load_time_ms: 0.556\n",
      "    sample_throughput: 18.269\n",
      "    sample_time_ms: 218945.936\n",
      "    update_time_ms: 11.259\n",
      "  timestamp: 1650267883\n",
      "  timesteps_since_restore: 912000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 912000\n",
      "  training_iteration: 228\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 916000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-48-22\n",
      "  done: false\n",
      "  episode_len_mean: 1630.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 467\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.178589006263068e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.7872649800850993e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128250069916248\n",
      "          total_loss: 0.014128562994301319\n",
      "          vf_explained_var: 4.0966978076539817e-07\n",
      "          vf_loss: 3.1670813882556104e-07\n",
      "    num_agent_steps_sampled: 916000\n",
      "    num_agent_steps_trained: 916000\n",
      "    num_steps_sampled: 916000\n",
      "    num_steps_trained: 916000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.1051724137931\n",
      "    ram_util_percent: 95.96206896551725\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10961766248241914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9937704313100878\n",
      "    mean_inference_ms: 2.26248590273554\n",
      "    mean_raw_obs_processing_ms: 0.1335876985188628\n",
      "  time_since_restore: 49746.650195360184\n",
      "  time_this_iter_s: 219.28092908859253\n",
      "  time_total_s: 49746.650195360184\n",
      "  timers:\n",
      "    learn_throughput: 19.039\n",
      "    learn_time_ms: 210091.163\n",
      "    load_throughput: 7439677.176\n",
      "    load_time_ms: 0.538\n",
      "    sample_throughput: 18.241\n",
      "    sample_time_ms: 219292.054\n",
      "    update_time_ms: 12.675\n",
      "  timestamp: 1650268102\n",
      "  timesteps_since_restore: 916000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 916000\n",
      "  training_iteration: 229\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 916000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-48-22\n",
      "  done: false\n",
      "  episode_len_mean: 519.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.93\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2749\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7115717530250549\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013679045252501965\n",
      "          model: {}\n",
      "          policy_loss: -0.04872626066207886\n",
      "          total_loss: 0.03356599062681198\n",
      "          vf_explained_var: 0.6978660225868225\n",
      "          vf_loss: 0.08229225873947144\n",
      "    num_agent_steps_sampled: 916000\n",
      "    num_agent_steps_trained: 916000\n",
      "    num_steps_sampled: 916000\n",
      "    num_steps_trained: 916000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.77903780068728\n",
      "    ram_util_percent: 95.98178694158075\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10665735570086288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9947880300260955\n",
      "    mean_inference_ms: 2.1813387381266933\n",
      "    mean_raw_obs_processing_ms: 0.20154592615986622\n",
      "  time_since_restore: 49754.556701660156\n",
      "  time_this_iter_s: 219.5805628299713\n",
      "  time_total_s: 49754.556701660156\n",
      "  timers:\n",
      "    learn_throughput: 19.07\n",
      "    learn_time_ms: 209755.941\n",
      "    load_throughput: 3177202.159\n",
      "    load_time_ms: 1.259\n",
      "    sample_throughput: 18.251\n",
      "    sample_time_ms: 219166.782\n",
      "    update_time_ms: 13.739\n",
      "  timestamp: 1650268102\n",
      "  timesteps_since_restore: 916000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 916000\n",
      "  training_iteration: 229\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-52-02\n",
      "  done: false\n",
      "  episode_len_mean: 1726.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 471\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.49225889895798e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.9428557307435e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0005454565398395061\n",
      "          total_loss: -0.00037533167051151395\n",
      "          vf_explained_var: -0.0001418254105374217\n",
      "          vf_loss: 0.00017012734315358102\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_agent_steps_trained: 920000\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.39554794520548\n",
      "    ram_util_percent: 95.93698630136986\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10938730975015533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9920845274025629\n",
      "    mean_inference_ms: 2.2548663912587994\n",
      "    mean_raw_obs_processing_ms: 0.13331125926089943\n",
      "  time_since_restore: 49966.01280999184\n",
      "  time_this_iter_s: 219.36261463165283\n",
      "  time_total_s: 49966.01280999184\n",
      "  timers:\n",
      "    learn_throughput: 19.038\n",
      "    learn_time_ms: 210111.495\n",
      "    load_throughput: 7412395.511\n",
      "    load_time_ms: 0.54\n",
      "    sample_throughput: 18.242\n",
      "    sample_time_ms: 219273.454\n",
      "    update_time_ms: 11.912\n",
      "  timestamp: 1650268322\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 230\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-52-03\n",
      "  done: false\n",
      "  episode_len_mean: 518.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.9\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2757\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7447956800460815\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013471889309585094\n",
      "          model: {}\n",
      "          policy_loss: -0.0429605096578598\n",
      "          total_loss: 0.050667375326156616\n",
      "          vf_explained_var: 0.6632189154624939\n",
      "          vf_loss: 0.09362787753343582\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_agent_steps_trained: 920000\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.17594501718212\n",
      "    ram_util_percent: 95.94536082474227\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10654191241965165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9939096480010634\n",
      "    mean_inference_ms: 2.177487602676726\n",
      "    mean_raw_obs_processing_ms: 0.20125434872815964\n",
      "  time_since_restore: 49974.844373703\n",
      "  time_this_iter_s: 220.28767204284668\n",
      "  time_total_s: 49974.844373703\n",
      "  timers:\n",
      "    learn_throughput: 19.057\n",
      "    learn_time_ms: 209892.866\n",
      "    load_throughput: 3131421.319\n",
      "    load_time_ms: 1.277\n",
      "    sample_throughput: 18.244\n",
      "    sample_time_ms: 219252.715\n",
      "    update_time_ms: 13.729\n",
      "  timestamp: 1650268323\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 230\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 08:52:24 (running for 13:53:44.17)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.9 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 924000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-55-41\n",
      "  done: false\n",
      "  episode_len_mean: 1726.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 471\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.965147622502517e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.5000877284581697e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128079637885094\n",
      "          total_loss: 0.01412956789135933\n",
      "          vf_explained_var: 3.112259605586587e-07\n",
      "          vf_loss: 1.490750150878739e-06\n",
      "    num_agent_steps_sampled: 924000\n",
      "    num_agent_steps_trained: 924000\n",
      "    num_steps_sampled: 924000\n",
      "    num_steps_trained: 924000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.89758620689655\n",
      "    ram_util_percent: 95.98310344827586\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10938730975015533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9920845274025629\n",
      "    mean_inference_ms: 2.2548663912587994\n",
      "    mean_raw_obs_processing_ms: 0.13331125926089943\n",
      "  time_since_restore: 50185.38452100754\n",
      "  time_this_iter_s: 219.3717110157013\n",
      "  time_total_s: 50185.38452100754\n",
      "  timers:\n",
      "    learn_throughput: 19.019\n",
      "    learn_time_ms: 210319.828\n",
      "    load_throughput: 7361010.881\n",
      "    load_time_ms: 0.543\n",
      "    sample_throughput: 18.237\n",
      "    sample_time_ms: 219338.226\n",
      "    update_time_ms: 13.039\n",
      "  timestamp: 1650268541\n",
      "  timesteps_since_restore: 924000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 924000\n",
      "  training_iteration: 231\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 924000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-55-42\n",
      "  done: false\n",
      "  episode_len_mean: 520.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 7.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2766\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7227705717086792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014605464413762093\n",
      "          model: {}\n",
      "          policy_loss: -0.05567137151956558\n",
      "          total_loss: 0.04465434327721596\n",
      "          vf_explained_var: 0.5682449340820312\n",
      "          vf_loss: 0.10032571852207184\n",
      "    num_agent_steps_sampled: 924000\n",
      "    num_agent_steps_trained: 924000\n",
      "    num_steps_sampled: 924000\n",
      "    num_steps_trained: 924000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.55743944636677\n",
      "    ram_util_percent: 95.97681660899654\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10641149546750374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9929212615411498\n",
      "    mean_inference_ms: 2.173152834470329\n",
      "    mean_raw_obs_processing_ms: 0.20092282415844503\n",
      "  time_since_restore: 50193.73199391365\n",
      "  time_this_iter_s: 218.88762021064758\n",
      "  time_total_s: 50193.73199391365\n",
      "  timers:\n",
      "    learn_throughput: 19.042\n",
      "    learn_time_ms: 210060.646\n",
      "    load_throughput: 3105396.661\n",
      "    load_time_ms: 1.288\n",
      "    sample_throughput: 18.228\n",
      "    sample_time_ms: 219446.766\n",
      "    update_time_ms: 12.789\n",
      "  timestamp: 1650268542\n",
      "  timesteps_since_restore: 924000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 924000\n",
      "  training_iteration: 231\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 928000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 1726.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 471\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.965147622502517e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.5000877284581697e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014127833768725395\n",
      "          total_loss: 0.014128130860626698\n",
      "          vf_explained_var: 9.718120281831943e-07\n",
      "          vf_loss: 2.961317306926503e-07\n",
      "    num_agent_steps_sampled: 928000\n",
      "    num_agent_steps_trained: 928000\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 928000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.46883561643835\n",
      "    ram_util_percent: 95.94623287671232\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10938730975015533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9920845274025629\n",
      "    mean_inference_ms: 2.2548663912587994\n",
      "    mean_raw_obs_processing_ms: 0.13331125926089943\n",
      "  time_since_restore: 50406.68020319939\n",
      "  time_this_iter_s: 221.29568219184875\n",
      "  time_total_s: 50406.68020319939\n",
      "  timers:\n",
      "    learn_throughput: 19.001\n",
      "    learn_time_ms: 210518.585\n",
      "    load_throughput: 7588753.392\n",
      "    load_time_ms: 0.527\n",
      "    sample_throughput: 18.218\n",
      "    sample_time_ms: 219561.56\n",
      "    update_time_ms: 14.491\n",
      "  timestamp: 1650268762\n",
      "  timesteps_since_restore: 928000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 232\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 928000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_08-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 524.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 7.11\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2774\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.714302122592926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015019229613244534\n",
      "          model: {}\n",
      "          policy_loss: -0.04636983573436737\n",
      "          total_loss: 0.03986617922782898\n",
      "          vf_explained_var: 0.69247967004776\n",
      "          vf_loss: 0.08623602241277695\n",
      "    num_agent_steps_sampled: 928000\n",
      "    num_agent_steps_trained: 928000\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 928000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.11331058020478\n",
      "    ram_util_percent: 95.96894197952217\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10629543902341364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9920432745804267\n",
      "    mean_inference_ms: 2.1693079659196046\n",
      "    mean_raw_obs_processing_ms: 0.20062781405246\n",
      "  time_since_restore: 50415.1793820858\n",
      "  time_this_iter_s: 221.44738817214966\n",
      "  time_total_s: 50415.1793820858\n",
      "  timers:\n",
      "    learn_throughput: 19.022\n",
      "    learn_time_ms: 210277.632\n",
      "    load_throughput: 3125995.156\n",
      "    load_time_ms: 1.28\n",
      "    sample_throughput: 18.212\n",
      "    sample_time_ms: 219639.236\n",
      "    update_time_ms: 12.369\n",
      "  timestamp: 1650268763\n",
      "  timesteps_since_restore: 928000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 232\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 932000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-03-02\n",
      "  done: false\n",
      "  episode_len_mean: 1823.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 473\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.006023620848021e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.617842452347912e-27\n",
      "          model: {}\n",
      "          policy_loss: 5.0956523409695365e-06\n",
      "          total_loss: 9.6126961580012e-05\n",
      "          vf_explained_var: -0.04954458400607109\n",
      "          vf_loss: 9.103029151447117e-05\n",
      "    num_agent_steps_sampled: 932000\n",
      "    num_agent_steps_trained: 932000\n",
      "    num_steps_sampled: 932000\n",
      "    num_steps_trained: 932000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.80068493150686\n",
      "    ram_util_percent: 95.91849315068492\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10926642866322892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9911990891684416\n",
      "    mean_inference_ms: 2.2508604822887923\n",
      "    mean_raw_obs_processing_ms: 0.13316282728665407\n",
      "  time_since_restore: 50626.3536901474\n",
      "  time_this_iter_s: 219.6734869480133\n",
      "  time_total_s: 50626.3536901474\n",
      "  timers:\n",
      "    learn_throughput: 18.996\n",
      "    learn_time_ms: 210567.664\n",
      "    load_throughput: 7587037.489\n",
      "    load_time_ms: 0.527\n",
      "    sample_throughput: 18.198\n",
      "    sample_time_ms: 219808.963\n",
      "    update_time_ms: 16.434\n",
      "  timestamp: 1650268982\n",
      "  timesteps_since_restore: 932000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 932000\n",
      "  training_iteration: 233\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 932000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-03-02\n",
      "  done: false\n",
      "  episode_len_mean: 515.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.94\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2784\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8097423911094666\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012969068251550198\n",
      "          model: {}\n",
      "          policy_loss: -0.04729582369327545\n",
      "          total_loss: 0.018718218430876732\n",
      "          vf_explained_var: 0.6894978880882263\n",
      "          vf_loss: 0.06601404398679733\n",
      "    num_agent_steps_sampled: 932000\n",
      "    num_agent_steps_trained: 932000\n",
      "    num_steps_sampled: 932000\n",
      "    num_steps_trained: 932000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.36632302405498\n",
      "    ram_util_percent: 95.89553264604811\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10615177866071254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.990966569923807\n",
      "    mean_inference_ms: 2.164603162114123\n",
      "    mean_raw_obs_processing_ms: 0.20027563636067414\n",
      "  time_since_restore: 50634.42191076279\n",
      "  time_this_iter_s: 219.2425286769867\n",
      "  time_total_s: 50634.42191076279\n",
      "  timers:\n",
      "    learn_throughput: 19.031\n",
      "    learn_time_ms: 210187.8\n",
      "    load_throughput: 2968840.757\n",
      "    load_time_ms: 1.347\n",
      "    sample_throughput: 18.191\n",
      "    sample_time_ms: 219892.1\n",
      "    update_time_ms: 12.086\n",
      "  timestamp: 1650268982\n",
      "  timesteps_since_restore: 932000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 932000\n",
      "  training_iteration: 233\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 936000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-06-41\n",
      "  done: false\n",
      "  episode_len_mean: 509.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.78\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2792\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.715114951133728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013219212181866169\n",
      "          model: {}\n",
      "          policy_loss: -0.04137671738862991\n",
      "          total_loss: 0.026779023930430412\n",
      "          vf_explained_var: 0.7572773098945618\n",
      "          vf_loss: 0.06815574318170547\n",
      "    num_agent_steps_sampled: 936000\n",
      "    num_agent_steps_trained: 936000\n",
      "    num_steps_sampled: 936000\n",
      "    num_steps_trained: 936000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.32206896551725\n",
      "    ram_util_percent: 95.9510344827586\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10603906371688644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9901262599204392\n",
      "    mean_inference_ms: 2.160924117324025\n",
      "    mean_raw_obs_processing_ms: 0.20000195204130874\n",
      "  time_since_restore: 50853.39265370369\n",
      "  time_this_iter_s: 218.9707429409027\n",
      "  time_total_s: 50853.39265370369\n",
      "  timers:\n",
      "    learn_throughput: 19.047\n",
      "    learn_time_ms: 210010.704\n",
      "    load_throughput: 2803491.745\n",
      "    load_time_ms: 1.427\n",
      "    sample_throughput: 18.202\n",
      "    sample_time_ms: 219762.034\n",
      "    update_time_ms: 12.158\n",
      "  timestamp: 1650269201\n",
      "  timesteps_since_restore: 936000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 936000\n",
      "  training_iteration: 234\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 936000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-06-41\n",
      "  done: false\n",
      "  episode_len_mean: 1823.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 473\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.004921184852719307\n",
      "          total_loss: -0.004921184852719307\n",
      "          vf_explained_var: -1.7070256944862194e-05\n",
      "          vf_loss: 1.9118422400765667e-09\n",
      "    num_agent_steps_sampled: 936000\n",
      "    num_agent_steps_trained: 936000\n",
      "    num_steps_sampled: 936000\n",
      "    num_steps_trained: 936000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.36931034482758\n",
      "    ram_util_percent: 95.97827586206895\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10926642866322892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9911990891684416\n",
      "    mean_inference_ms: 2.2508604822887923\n",
      "    mean_raw_obs_processing_ms: 0.13316282728665407\n",
      "  time_since_restore: 50845.34270310402\n",
      "  time_this_iter_s: 218.98901295661926\n",
      "  time_total_s: 50845.34270310402\n",
      "  timers:\n",
      "    learn_throughput: 19.018\n",
      "    learn_time_ms: 210323.148\n",
      "    load_throughput: 4328152.104\n",
      "    load_time_ms: 0.924\n",
      "    sample_throughput: 18.197\n",
      "    sample_time_ms: 219816.251\n",
      "    update_time_ms: 17.791\n",
      "  timestamp: 1650269201\n",
      "  timesteps_since_restore: 936000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 936000\n",
      "  training_iteration: 234\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 09:09:04 (running for 14:10:24.50)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.78 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 940000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-10-19\n",
      "  done: false\n",
      "  episode_len_mean: 505.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.71\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2801\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8151314854621887\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01161706168204546\n",
      "          model: {}\n",
      "          policy_loss: -0.051049016416072845\n",
      "          total_loss: 0.021744951605796814\n",
      "          vf_explained_var: 0.7346255779266357\n",
      "          vf_loss: 0.07279397547245026\n",
      "    num_agent_steps_sampled: 940000\n",
      "    num_agent_steps_trained: 940000\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.3588850174216\n",
      "    ram_util_percent: 95.92369337979093\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1059156671611786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9892063587999903\n",
      "    mean_inference_ms: 2.1568988783865115\n",
      "    mean_raw_obs_processing_ms: 0.19970485625040937\n",
      "  time_since_restore: 51070.92236471176\n",
      "  time_this_iter_s: 217.5297110080719\n",
      "  time_total_s: 51070.92236471176\n",
      "  timers:\n",
      "    learn_throughput: 19.055\n",
      "    learn_time_ms: 209917.913\n",
      "    load_throughput: 2650973.502\n",
      "    load_time_ms: 1.509\n",
      "    sample_throughput: 18.213\n",
      "    sample_time_ms: 219625.309\n",
      "    update_time_ms: 12.305\n",
      "  timestamp: 1650269419\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 235\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 940000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-10-21\n",
      "  done: false\n",
      "  episode_len_mean: 1722.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 480\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.7819535760984184e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.8838813085044238e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.016682198271155357\n",
      "          total_loss: 0.01742500439286232\n",
      "          vf_explained_var: 0.1170026957988739\n",
      "          vf_loss: 0.00074279640102759\n",
      "    num_agent_steps_sampled: 940000\n",
      "    num_agent_steps_trained: 940000\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.8093103448276\n",
      "    ram_util_percent: 95.92655172413792\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10885863803260225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9882198137631213\n",
      "    mean_inference_ms: 2.2373790214373286\n",
      "    mean_raw_obs_processing_ms: 0.13266953261797648\n",
      "  time_since_restore: 51064.53053307533\n",
      "  time_this_iter_s: 219.18782997131348\n",
      "  time_total_s: 51064.53053307533\n",
      "  timers:\n",
      "    learn_throughput: 19.018\n",
      "    learn_time_ms: 210324.531\n",
      "    load_throughput: 2767375.835\n",
      "    load_time_ms: 1.445\n",
      "    sample_throughput: 18.215\n",
      "    sample_time_ms: 219599.395\n",
      "    update_time_ms: 17.876\n",
      "  timestamp: 1650269421\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 235\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 944000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-13-58\n",
      "  done: false\n",
      "  episode_len_mean: 511.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2809\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7102628350257874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013646573759615421\n",
      "          model: {}\n",
      "          policy_loss: -0.045513734221458435\n",
      "          total_loss: 0.031056325882673264\n",
      "          vf_explained_var: 0.7472409605979919\n",
      "          vf_loss: 0.0765700563788414\n",
      "    num_agent_steps_sampled: 944000\n",
      "    num_agent_steps_trained: 944000\n",
      "    num_steps_sampled: 944000\n",
      "    num_steps_trained: 944000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.8948275862069\n",
      "    ram_util_percent: 95.84551724137931\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10580623018458858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.988397297334513\n",
      "    mean_inference_ms: 2.153345577112971\n",
      "    mean_raw_obs_processing_ms: 0.1994368229980843\n",
      "  time_since_restore: 51290.37452483177\n",
      "  time_this_iter_s: 219.45216012001038\n",
      "  time_total_s: 51290.37452483177\n",
      "  timers:\n",
      "    learn_throughput: 19.051\n",
      "    learn_time_ms: 209967.178\n",
      "    load_throughput: 2648713.472\n",
      "    load_time_ms: 1.51\n",
      "    sample_throughput: 18.219\n",
      "    sample_time_ms: 219545.041\n",
      "    update_time_ms: 13.592\n",
      "  timestamp: 1650269638\n",
      "  timesteps_since_restore: 944000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 944000\n",
      "  training_iteration: 236\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 944000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-14-00\n",
      "  done: false\n",
      "  episode_len_mean: 1724.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 484\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.184101101644552e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.5234004748780955e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.00030949755455367267\n",
      "          total_loss: 0.0005054078064858913\n",
      "          vf_explained_var: 0.07303476333618164\n",
      "          vf_loss: 0.0001959111395990476\n",
      "    num_agent_steps_sampled: 944000\n",
      "    num_agent_steps_trained: 944000\n",
      "    num_steps_sampled: 944000\n",
      "    num_steps_trained: 944000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.25841924398625\n",
      "    ram_util_percent: 95.84123711340206\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10863483149873122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9865842540768611\n",
      "    mean_inference_ms: 2.2299722573503082\n",
      "    mean_raw_obs_processing_ms: 0.13239745682744447\n",
      "  time_since_restore: 51284.203077077866\n",
      "  time_this_iter_s: 219.67254400253296\n",
      "  time_total_s: 51284.203077077866\n",
      "  timers:\n",
      "    learn_throughput: 19.009\n",
      "    learn_time_ms: 210425.246\n",
      "    load_throughput: 2756826.002\n",
      "    load_time_ms: 1.451\n",
      "    sample_throughput: 18.21\n",
      "    sample_time_ms: 219655.755\n",
      "    update_time_ms: 17.924\n",
      "  timestamp: 1650269640\n",
      "  timesteps_since_restore: 944000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 944000\n",
      "  training_iteration: 236\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 948000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-17-37\n",
      "  done: false\n",
      "  episode_len_mean: 515.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 6.92\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2816\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6890104413032532\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012439939193427563\n",
      "          model: {}\n",
      "          policy_loss: -0.038819871842861176\n",
      "          total_loss: 0.039424292743206024\n",
      "          vf_explained_var: 0.7414678335189819\n",
      "          vf_loss: 0.0782441720366478\n",
      "    num_agent_steps_sampled: 948000\n",
      "    num_agent_steps_trained: 948000\n",
      "    num_steps_sampled: 948000\n",
      "    num_steps_trained: 948000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.30824742268041\n",
      "    ram_util_percent: 95.77766323024055\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10571283945313308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.987706909440029\n",
      "    mean_inference_ms: 2.1503192971169374\n",
      "    mean_raw_obs_processing_ms: 0.19920616746724668\n",
      "  time_since_restore: 51509.24363064766\n",
      "  time_this_iter_s: 218.86910581588745\n",
      "  time_total_s: 51509.24363064766\n",
      "  timers:\n",
      "    learn_throughput: 19.056\n",
      "    learn_time_ms: 209908.243\n",
      "    load_throughput: 2682123.034\n",
      "    load_time_ms: 1.491\n",
      "    sample_throughput: 18.21\n",
      "    sample_time_ms: 219660.983\n",
      "    update_time_ms: 14.74\n",
      "  timestamp: 1650269857\n",
      "  timesteps_since_restore: 948000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 948000\n",
      "  training_iteration: 237\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 948000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-17-39\n",
      "  done: false\n",
      "  episode_len_mean: 1724.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 484\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.329142083911374e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2430768727823652e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128164388239384\n",
      "          total_loss: 0.014135737903416157\n",
      "          vf_explained_var: -3.14686872115999e-08\n",
      "          vf_loss: 7.566030490124831e-06\n",
      "    num_agent_steps_sampled: 948000\n",
      "    num_agent_steps_trained: 948000\n",
      "    num_steps_sampled: 948000\n",
      "    num_steps_trained: 948000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.29278350515463\n",
      "    ram_util_percent: 95.76735395189004\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10863483149873122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9865842540768611\n",
      "    mean_inference_ms: 2.2299722573503082\n",
      "    mean_raw_obs_processing_ms: 0.13239745682744447\n",
      "  time_since_restore: 51503.01257419586\n",
      "  time_this_iter_s: 218.80949711799622\n",
      "  time_total_s: 51503.01257419586\n",
      "  timers:\n",
      "    learn_throughput: 19.011\n",
      "    learn_time_ms: 210399.987\n",
      "    load_throughput: 2752980.867\n",
      "    load_time_ms: 1.453\n",
      "    sample_throughput: 18.199\n",
      "    sample_time_ms: 219792.254\n",
      "    update_time_ms: 18.47\n",
      "  timestamp: 1650269859\n",
      "  timesteps_since_restore: 948000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 948000\n",
      "  training_iteration: 237\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 952000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-21-17\n",
      "  done: false\n",
      "  episode_len_mean: 514.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 6.87\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2824\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7283499836921692\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01333761215209961\n",
      "          model: {}\n",
      "          policy_loss: -0.04849127307534218\n",
      "          total_loss: 0.008073808625340462\n",
      "          vf_explained_var: 0.7939251065254211\n",
      "          vf_loss: 0.05656508728861809\n",
      "    num_agent_steps_sampled: 952000\n",
      "    num_agent_steps_trained: 952000\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 952000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.35773195876288\n",
      "    ram_util_percent: 95.90790378006874\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10560599143362806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9869231387129819\n",
      "    mean_inference_ms: 2.146875055382367\n",
      "    mean_raw_obs_processing_ms: 0.1989435189662071\n",
      "  time_since_restore: 51728.361825704575\n",
      "  time_this_iter_s: 219.11819505691528\n",
      "  time_total_s: 51728.361825704575\n",
      "  timers:\n",
      "    learn_throughput: 19.082\n",
      "    learn_time_ms: 209618.006\n",
      "    load_throughput: 1996265.721\n",
      "    load_time_ms: 2.004\n",
      "    sample_throughput: 18.207\n",
      "    sample_time_ms: 219692.377\n",
      "    update_time_ms: 15.369\n",
      "  timestamp: 1650270077\n",
      "  timesteps_since_restore: 952000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 238\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 952000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-21-18\n",
      "  done: false\n",
      "  episode_len_mean: 1824.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 489\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.201941061011848e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3247694974707235e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.002325240522623062\n",
      "          total_loss: 0.0025217921938747168\n",
      "          vf_explained_var: 0.16063852608203888\n",
      "          vf_loss: 0.0001965511910384521\n",
      "    num_agent_steps_sampled: 952000\n",
      "    num_agent_steps_trained: 952000\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 952000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.40206896551724\n",
      "    ram_util_percent: 95.90689655172413\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10834920624473267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9844946386747767\n",
      "    mean_inference_ms: 2.2205066146792594\n",
      "    mean_raw_obs_processing_ms: 0.1320509406061403\n",
      "  time_since_restore: 51721.901918411255\n",
      "  time_this_iter_s: 218.88934421539307\n",
      "  time_total_s: 51721.901918411255\n",
      "  timers:\n",
      "    learn_throughput: 19.041\n",
      "    learn_time_ms: 210073.01\n",
      "    load_throughput: 2750498.549\n",
      "    load_time_ms: 1.454\n",
      "    sample_throughput: 18.192\n",
      "    sample_time_ms: 219873.169\n",
      "    update_time_ms: 18.126\n",
      "  timestamp: 1650270078\n",
      "  timesteps_since_restore: 952000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 238\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 956000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-24-55\n",
      "  done: false\n",
      "  episode_len_mean: 518.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 6.99\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2832\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7386587262153625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016385285183787346\n",
      "          model: {}\n",
      "          policy_loss: -0.036366961896419525\n",
      "          total_loss: 0.03836594149470329\n",
      "          vf_explained_var: 0.7371506094932556\n",
      "          vf_loss: 0.07473289966583252\n",
      "    num_agent_steps_sampled: 956000\n",
      "    num_agent_steps_trained: 956000\n",
      "    num_steps_sampled: 956000\n",
      "    num_steps_trained: 956000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.41280276816609\n",
      "    ram_util_percent: 95.90311418685121\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1055004585609321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9861516375346439\n",
      "    mean_inference_ms: 2.1434810566131417\n",
      "    mean_raw_obs_processing_ms: 0.19868194674016998\n",
      "  time_since_restore: 51947.02905488014\n",
      "  time_this_iter_s: 218.66722917556763\n",
      "  time_total_s: 51947.02905488014\n",
      "  timers:\n",
      "    learn_throughput: 19.094\n",
      "    learn_time_ms: 209486.237\n",
      "    load_throughput: 2005788.341\n",
      "    load_time_ms: 1.994\n",
      "    sample_throughput: 18.228\n",
      "    sample_time_ms: 219438.294\n",
      "    update_time_ms: 18.003\n",
      "  timestamp: 1650270295\n",
      "  timesteps_since_restore: 956000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 956000\n",
      "  training_iteration: 239\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 956000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-24-57\n",
      "  done: false\n",
      "  episode_len_mean: 1726.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 492\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.3660788296145804e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.283268977962588e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0003441224107518792\n",
      "          total_loss: -0.0002252190315630287\n",
      "          vf_explained_var: -0.0001792209077393636\n",
      "          vf_loss: 0.00011890291352756321\n",
      "    num_agent_steps_sampled: 956000\n",
      "    num_agent_steps_trained: 956000\n",
      "    num_steps_sampled: 956000\n",
      "    num_steps_trained: 956000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.53758620689656\n",
      "    ram_util_percent: 95.93310344827586\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10818619325192332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9833025378373695\n",
      "    mean_inference_ms: 2.2151017348200215\n",
      "    mean_raw_obs_processing_ms: 0.1318496943711386\n",
      "  time_since_restore: 51940.797033548355\n",
      "  time_this_iter_s: 218.89511513710022\n",
      "  time_total_s: 51940.797033548355\n",
      "  timers:\n",
      "    learn_throughput: 19.046\n",
      "    learn_time_ms: 210013.441\n",
      "    load_throughput: 2701253.603\n",
      "    load_time_ms: 1.481\n",
      "    sample_throughput: 18.218\n",
      "    sample_time_ms: 219561.606\n",
      "    update_time_ms: 16.389\n",
      "  timestamp: 1650270297\n",
      "  timesteps_since_restore: 956000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 956000\n",
      "  training_iteration: 239\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 09:25:45 (running for 14:27:04.91)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.99 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-28-36\n",
      "  done: false\n",
      "  episode_len_mean: 506.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 6.71\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2842\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7446844577789307\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01299135759472847\n",
      "          model: {}\n",
      "          policy_loss: -0.044875677675008774\n",
      "          total_loss: 0.05250956863164902\n",
      "          vf_explained_var: 0.632796049118042\n",
      "          vf_loss: 0.09738525748252869\n",
      "    num_agent_steps_sampled: 960000\n",
      "    num_agent_steps_trained: 960000\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.93231292517007\n",
      "    ram_util_percent: 95.95646258503402\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10537420449650323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9852178124574942\n",
      "    mean_inference_ms: 2.1393643532227418\n",
      "    mean_raw_obs_processing_ms: 0.19837352406902817\n",
      "  time_since_restore: 52167.75084900856\n",
      "  time_this_iter_s: 220.72179412841797\n",
      "  time_total_s: 52167.75084900856\n",
      "  timers:\n",
      "    learn_throughput: 19.095\n",
      "    learn_time_ms: 209481.544\n",
      "    load_throughput: 2021521.815\n",
      "    load_time_ms: 1.979\n",
      "    sample_throughput: 18.234\n",
      "    sample_time_ms: 219376.367\n",
      "    update_time_ms: 19.086\n",
      "  timestamp: 1650270516\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 240\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-28-37\n",
      "  done: false\n",
      "  episode_len_mean: 1726.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 492\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128255657851696\n",
      "          total_loss: -0.014128215610980988\n",
      "          vf_explained_var: 1.597724917701271e-06\n",
      "          vf_loss: 5.02924315526343e-08\n",
      "    num_agent_steps_sampled: 960000\n",
      "    num_agent_steps_trained: 960000\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.65445205479452\n",
      "    ram_util_percent: 95.98527397260273\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10818619325192332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9833025378373695\n",
      "    mean_inference_ms: 2.2151017348200215\n",
      "    mean_raw_obs_processing_ms: 0.1318496943711386\n",
      "  time_since_restore: 52160.61597752571\n",
      "  time_this_iter_s: 219.81894397735596\n",
      "  time_total_s: 52160.61597752571\n",
      "  timers:\n",
      "    learn_throughput: 19.044\n",
      "    learn_time_ms: 210036.152\n",
      "    load_throughput: 2576432.937\n",
      "    load_time_ms: 1.553\n",
      "    sample_throughput: 18.222\n",
      "    sample_time_ms: 219514.829\n",
      "    update_time_ms: 14.942\n",
      "  timestamp: 1650270517\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 240\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 964000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-32-15\n",
      "  done: false\n",
      "  episode_len_mean: 511.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 6.73\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2849\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6374042630195618\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013396915048360825\n",
      "          model: {}\n",
      "          policy_loss: -0.04910121485590935\n",
      "          total_loss: 0.01918129250407219\n",
      "          vf_explained_var: 0.6678736805915833\n",
      "          vf_loss: 0.06828250735998154\n",
      "    num_agent_steps_sampled: 964000\n",
      "    num_agent_steps_trained: 964000\n",
      "    num_steps_sampled: 964000\n",
      "    num_steps_trained: 964000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.17508650519031\n",
      "    ram_util_percent: 95.98373702422145\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10528687814514896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9845726594740569\n",
      "    mean_inference_ms: 2.136521180561087\n",
      "    mean_raw_obs_processing_ms: 0.1981584453045337\n",
      "  time_since_restore: 52386.656620025635\n",
      "  time_this_iter_s: 218.90577101707458\n",
      "  time_total_s: 52386.656620025635\n",
      "  timers:\n",
      "    learn_throughput: 19.091\n",
      "    learn_time_ms: 209526.495\n",
      "    load_throughput: 2035007.945\n",
      "    load_time_ms: 1.966\n",
      "    sample_throughput: 18.237\n",
      "    sample_time_ms: 219332.442\n",
      "    update_time_ms: 19.883\n",
      "  timestamp: 1650270735\n",
      "  timesteps_since_restore: 964000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 964000\n",
      "  training_iteration: 241\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 964000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 1821.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 498\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.2963190696428037e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.30826417027743e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0005119549459777772\n",
      "          total_loss: 2.216549364675302e-05\n",
      "          vf_explained_var: 0.3210386335849762\n",
      "          vf_loss: 0.0005341291544027627\n",
      "    num_agent_steps_sampled: 964000\n",
      "    num_agent_steps_trained: 964000\n",
      "    num_steps_sampled: 964000\n",
      "    num_steps_trained: 964000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.53965517241379\n",
      "    ram_util_percent: 95.96931034482758\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10785068781314269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9808530546739543\n",
      "    mean_inference_ms: 2.20399067200732\n",
      "    mean_raw_obs_processing_ms: 0.131437044322136\n",
      "  time_since_restore: 52380.238807201385\n",
      "  time_this_iter_s: 219.62282967567444\n",
      "  time_total_s: 52380.238807201385\n",
      "  timers:\n",
      "    learn_throughput: 19.042\n",
      "    learn_time_ms: 210056.887\n",
      "    load_throughput: 2567482.745\n",
      "    load_time_ms: 1.558\n",
      "    sample_throughput: 18.22\n",
      "    sample_time_ms: 219536.048\n",
      "    update_time_ms: 14.099\n",
      "  timestamp: 1650270737\n",
      "  timesteps_since_restore: 964000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 964000\n",
      "  training_iteration: 241\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 968000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 514.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.0\n",
      "  episode_reward_mean: 6.84\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2857\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6669914126396179\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013220935128629208\n",
      "          model: {}\n",
      "          policy_loss: -0.04112127050757408\n",
      "          total_loss: 0.024445680901408195\n",
      "          vf_explained_var: 0.7527902722358704\n",
      "          vf_loss: 0.06556694954633713\n",
      "    num_agent_steps_sampled: 968000\n",
      "    num_agent_steps_trained: 968000\n",
      "    num_steps_sampled: 968000\n",
      "    num_steps_trained: 968000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.9857638888889\n",
      "    ram_util_percent: 95.98402777777778\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10518962967937627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9838472688532335\n",
      "    mean_inference_ms: 2.133316425381618\n",
      "    mean_raw_obs_processing_ms: 0.19791782415738357\n",
      "  time_since_restore: 52603.96726202965\n",
      "  time_this_iter_s: 217.31064200401306\n",
      "  time_total_s: 52603.96726202965\n",
      "  timers:\n",
      "    learn_throughput: 19.131\n",
      "    learn_time_ms: 209082.208\n",
      "    load_throughput: 3022812.872\n",
      "    load_time_ms: 1.323\n",
      "    sample_throughput: 18.231\n",
      "    sample_time_ms: 219409.626\n",
      "    update_time_ms: 20.442\n",
      "  timestamp: 1650270953\n",
      "  timesteps_since_restore: 968000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 968000\n",
      "  training_iteration: 242\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 968000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-35-54\n",
      "  done: false\n",
      "  episode_len_mean: 1528.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 503\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.26835526314676e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.635547664429256e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.004375709220767021\n",
      "          total_loss: 0.0045578693971037865\n",
      "          vf_explained_var: 0.19888463616371155\n",
      "          vf_loss: 0.00018215831369161606\n",
      "    num_agent_steps_sampled: 968000\n",
      "    num_agent_steps_trained: 968000\n",
      "    num_steps_sampled: 968000\n",
      "    num_steps_trained: 968000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.51833910034601\n",
      "    ram_util_percent: 95.97474048442906\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10761570874354068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9791500953111115\n",
      "    mean_inference_ms: 2.196237876674435\n",
      "    mean_raw_obs_processing_ms: 0.13115777798587933\n",
      "  time_since_restore: 52597.598331213\n",
      "  time_this_iter_s: 217.35952401161194\n",
      "  time_total_s: 52597.598331213\n",
      "  timers:\n",
      "    learn_throughput: 19.082\n",
      "    learn_time_ms: 209618.121\n",
      "    load_throughput: 2434055.74\n",
      "    load_time_ms: 1.643\n",
      "    sample_throughput: 18.215\n",
      "    sample_time_ms: 219598.51\n",
      "    update_time_ms: 13.314\n",
      "  timestamp: 1650270954\n",
      "  timesteps_since_restore: 968000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 968000\n",
      "  training_iteration: 242\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 972000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-39-32\n",
      "  done: false\n",
      "  episode_len_mean: 524.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.0\n",
      "  episode_reward_mean: 7.03\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2863\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6341341137886047\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01615549437701702\n",
      "          model: {}\n",
      "          policy_loss: -0.036414049565792084\n",
      "          total_loss: 0.06673996895551682\n",
      "          vf_explained_var: 0.7054804563522339\n",
      "          vf_loss: 0.1031540259718895\n",
      "    num_agent_steps_sampled: 972000\n",
      "    num_agent_steps_trained: 972000\n",
      "    num_steps_sampled: 972000\n",
      "    num_steps_trained: 972000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.90172413793104\n",
      "    ram_util_percent: 95.97172413793103\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10511649548091775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9833048109718043\n",
      "    mean_inference_ms: 2.1309385624079806\n",
      "    mean_raw_obs_processing_ms: 0.1977347930822372\n",
      "  time_since_restore: 52822.971668958664\n",
      "  time_this_iter_s: 219.0044069290161\n",
      "  time_total_s: 52822.971668958664\n",
      "  timers:\n",
      "    learn_throughput: 19.133\n",
      "    learn_time_ms: 209061.499\n",
      "    load_throughput: 3206102.926\n",
      "    load_time_ms: 1.248\n",
      "    sample_throughput: 18.267\n",
      "    sample_time_ms: 218969.126\n",
      "    update_time_ms: 20.943\n",
      "  timestamp: 1650271172\n",
      "  timesteps_since_restore: 972000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 972000\n",
      "  training_iteration: 243\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 972000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-39-33\n",
      "  done: false\n",
      "  episode_len_mean: 1528.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 503\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014127995818853378\n",
      "          total_loss: 0.014150734059512615\n",
      "          vf_explained_var: -4.838871703327641e-08\n",
      "          vf_loss: 2.2739704945706762e-05\n",
      "    num_agent_steps_sampled: 972000\n",
      "    num_agent_steps_trained: 972000\n",
      "    num_steps_sampled: 972000\n",
      "    num_steps_trained: 972000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.17024221453286\n",
      "    ram_util_percent: 95.95640138408305\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10761570874354068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9791500953111115\n",
      "    mean_inference_ms: 2.196237876674435\n",
      "    mean_raw_obs_processing_ms: 0.13115777798587933\n",
      "  time_since_restore: 52816.39949417114\n",
      "  time_this_iter_s: 218.80116295814514\n",
      "  time_total_s: 52816.39949417114\n",
      "  timers:\n",
      "    learn_throughput: 19.09\n",
      "    learn_time_ms: 209528.937\n",
      "    load_throughput: 2088146.867\n",
      "    load_time_ms: 1.916\n",
      "    sample_throughput: 18.253\n",
      "    sample_time_ms: 219146.31\n",
      "    update_time_ms: 11.2\n",
      "  timestamp: 1650271173\n",
      "  timesteps_since_restore: 972000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 972000\n",
      "  training_iteration: 243\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 09:42:25 (running for 14:43:45.25)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.03 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 976000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 522.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.0\n",
      "  episode_reward_mean: 7.0\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2871\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7686481475830078\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014269912615418434\n",
      "          model: {}\n",
      "          policy_loss: -0.050878122448921204\n",
      "          total_loss: 0.0371551439166069\n",
      "          vf_explained_var: 0.7088993191719055\n",
      "          vf_loss: 0.08803325146436691\n",
      "    num_agent_steps_sampled: 976000\n",
      "    num_agent_steps_trained: 976000\n",
      "    num_steps_sampled: 976000\n",
      "    num_steps_trained: 976000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.82602739726028\n",
      "    ram_util_percent: 95.8845890410959\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10501567633825395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9825633977255452\n",
      "    mean_inference_ms: 2.127696120723055\n",
      "    mean_raw_obs_processing_ms: 0.19748465511600372\n",
      "  time_since_restore: 53044.35055685043\n",
      "  time_this_iter_s: 221.3788878917694\n",
      "  time_total_s: 53044.35055685043\n",
      "  timers:\n",
      "    learn_throughput: 19.114\n",
      "    learn_time_ms: 209272.113\n",
      "    load_throughput: 3457152.63\n",
      "    load_time_ms: 1.157\n",
      "    sample_throughput: 18.265\n",
      "    sample_time_ms: 218995.139\n",
      "    update_time_ms: 24.042\n",
      "  timestamp: 1650271393\n",
      "  timesteps_since_restore: 976000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 976000\n",
      "  training_iteration: 244\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 976000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-43-14\n",
      "  done: false\n",
      "  episode_len_mean: 1626.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 506\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.66009830772907e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.9566847344831667e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.001541369827464223\n",
      "          total_loss: 0.001643044757656753\n",
      "          vf_explained_var: 0.046839337795972824\n",
      "          vf_loss: 0.00010167513391934335\n",
      "    num_agent_steps_sampled: 976000\n",
      "    num_agent_steps_trained: 976000\n",
      "    num_steps_sampled: 976000\n",
      "    num_steps_trained: 976000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.34965753424657\n",
      "    ram_util_percent: 95.89760273972603\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10747316582612303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9781177312842847\n",
      "    mean_inference_ms: 2.191533886301649\n",
      "    mean_raw_obs_processing_ms: 0.1309880604550027\n",
      "  time_since_restore: 53037.37289214134\n",
      "  time_this_iter_s: 220.97339797019958\n",
      "  time_total_s: 53037.37289214134\n",
      "  timers:\n",
      "    learn_throughput: 19.074\n",
      "    learn_time_ms: 209706.016\n",
      "    load_throughput: 2621440.0\n",
      "    load_time_ms: 1.526\n",
      "    sample_throughput: 18.257\n",
      "    sample_time_ms: 219094.6\n",
      "    update_time_ms: 10.188\n",
      "  timestamp: 1650271394\n",
      "  timesteps_since_restore: 976000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 976000\n",
      "  training_iteration: 244\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 980000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 1626.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 506\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.004893649835139513\n",
      "          total_loss: 0.004893649835139513\n",
      "          vf_explained_var: -2.333938482479425e-06\n",
      "          vf_loss: 1.516296310732912e-09\n",
      "    num_agent_steps_sampled: 980000\n",
      "    num_agent_steps_trained: 980000\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.54965986394559\n",
      "    ram_util_percent: 95.68401360544217\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10747316582612303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9781177312842847\n",
      "    mean_inference_ms: 2.191533886301649\n",
      "    mean_raw_obs_processing_ms: 0.1309880604550027\n",
      "  time_since_restore: 53258.17493915558\n",
      "  time_this_iter_s: 220.80204701423645\n",
      "  time_total_s: 53258.17493915558\n",
      "  timers:\n",
      "    learn_throughput: 19.06\n",
      "    learn_time_ms: 209866.61\n",
      "    load_throughput: 3961375.142\n",
      "    load_time_ms: 1.01\n",
      "    sample_throughput: 18.243\n",
      "    sample_time_ms: 219262.772\n",
      "    update_time_ms: 11.225\n",
      "  timestamp: 1650271615\n",
      "  timesteps_since_restore: 980000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 245\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 980000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 531.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.0\n",
      "  episode_reward_mean: 7.16\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2878\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6141775846481323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012970104813575745\n",
      "          model: {}\n",
      "          policy_loss: -0.0357874259352684\n",
      "          total_loss: 0.05928720533847809\n",
      "          vf_explained_var: 0.7110850214958191\n",
      "          vf_loss: 0.09507464617490768\n",
      "    num_agent_steps_sampled: 980000\n",
      "    num_agent_steps_trained: 980000\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.56824324324324\n",
      "    ram_util_percent: 95.68344594594595\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10492719061993792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9819148906522003\n",
      "    mean_inference_ms: 2.1248659923590436\n",
      "    mean_raw_obs_processing_ms: 0.19726145011193608\n",
      "  time_since_restore: 53266.34000873566\n",
      "  time_this_iter_s: 221.9894518852234\n",
      "  time_total_s: 53266.34000873566\n",
      "  timers:\n",
      "    learn_throughput: 19.076\n",
      "    learn_time_ms: 209690.568\n",
      "    load_throughput: 3699577.94\n",
      "    load_time_ms: 1.081\n",
      "    sample_throughput: 18.244\n",
      "    sample_time_ms: 219245.64\n",
      "    update_time_ms: 24.453\n",
      "  timestamp: 1650271615\n",
      "  timesteps_since_restore: 980000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 245\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 984000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 1626.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 506\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.003016112372279167\n",
      "          total_loss: 0.003016112372279167\n",
      "          vf_explained_var: 7.008352440607268e-07\n",
      "          vf_loss: 6.632309146326065e-10\n",
      "    num_agent_steps_sampled: 984000\n",
      "    num_agent_steps_trained: 984000\n",
      "    num_steps_sampled: 984000\n",
      "    num_steps_trained: 984000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.46310344827587\n",
      "    ram_util_percent: 95.88758620689654\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10747316582612303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9781177312842847\n",
      "    mean_inference_ms: 2.191533886301649\n",
      "    mean_raw_obs_processing_ms: 0.1309880604550027\n",
      "  time_since_restore: 53477.02756214142\n",
      "  time_this_iter_s: 218.85262298583984\n",
      "  time_total_s: 53477.02756214142\n",
      "  timers:\n",
      "    learn_throughput: 19.063\n",
      "    learn_time_ms: 209825.55\n",
      "    load_throughput: 3975926.251\n",
      "    load_time_ms: 1.006\n",
      "    sample_throughput: 18.232\n",
      "    sample_time_ms: 219391.317\n",
      "    update_time_ms: 14.665\n",
      "  timestamp: 1650271834\n",
      "  timesteps_since_restore: 984000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 984000\n",
      "  training_iteration: 246\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 984000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 536.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.31\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2887\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7802949547767639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01318148709833622\n",
      "          model: {}\n",
      "          policy_loss: -0.04164594039320946\n",
      "          total_loss: 0.04976320266723633\n",
      "          vf_explained_var: 0.7446553707122803\n",
      "          vf_loss: 0.09140913933515549\n",
      "    num_agent_steps_sampled: 984000\n",
      "    num_agent_steps_trained: 984000\n",
      "    num_steps_sampled: 984000\n",
      "    num_steps_trained: 984000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.35862068965517\n",
      "    ram_util_percent: 95.85655172413793\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10481295879070515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9810720559961351\n",
      "    mean_inference_ms: 2.1212022019789374\n",
      "    mean_raw_obs_processing_ms: 0.19697054424827581\n",
      "  time_since_restore: 53484.8525698185\n",
      "  time_this_iter_s: 218.51256108283997\n",
      "  time_total_s: 53484.8525698185\n",
      "  timers:\n",
      "    learn_throughput: 19.084\n",
      "    learn_time_ms: 209600.763\n",
      "    load_throughput: 3704070.296\n",
      "    load_time_ms: 1.08\n",
      "    sample_throughput: 18.21\n",
      "    sample_time_ms: 219660.79\n",
      "    update_time_ms: 23.179\n",
      "  timestamp: 1650271834\n",
      "  timesteps_since_restore: 984000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 984000\n",
      "  training_iteration: 246\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 988000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-54-13\n",
      "  done: false\n",
      "  episode_len_mean: 1723.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 517\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4275077175952118e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.372203349612735e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0013640878023579717\n",
      "          total_loss: -0.0009660403011366725\n",
      "          vf_explained_var: 0.2613983750343323\n",
      "          vf_loss: 0.00039804959669709206\n",
      "    num_agent_steps_sampled: 988000\n",
      "    num_agent_steps_trained: 988000\n",
      "    num_steps_sampled: 988000\n",
      "    num_steps_trained: 988000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.4553633217993\n",
      "    ram_util_percent: 95.97058823529412\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10693079503835651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.974191228191891\n",
      "    mean_inference_ms: 2.1736498846271717\n",
      "    mean_raw_obs_processing_ms: 0.130309633079832\n",
      "  time_since_restore: 53696.09335517883\n",
      "  time_this_iter_s: 219.06579303741455\n",
      "  time_total_s: 53696.09335517883\n",
      "  timers:\n",
      "    learn_throughput: 19.059\n",
      "    learn_time_ms: 209876.826\n",
      "    load_throughput: 3281029.452\n",
      "    load_time_ms: 1.219\n",
      "    sample_throughput: 18.237\n",
      "    sample_time_ms: 219336.245\n",
      "    update_time_ms: 14.191\n",
      "  timestamp: 1650272053\n",
      "  timesteps_since_restore: 988000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 988000\n",
      "  training_iteration: 247\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 988000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-54-13\n",
      "  done: false\n",
      "  episode_len_mean: 536.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.35\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2895\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7469574213027954\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014096706174314022\n",
      "          model: {}\n",
      "          policy_loss: -0.04273589700460434\n",
      "          total_loss: 0.03098367340862751\n",
      "          vf_explained_var: 0.7212414145469666\n",
      "          vf_loss: 0.0737195611000061\n",
      "    num_agent_steps_sampled: 988000\n",
      "    num_agent_steps_trained: 988000\n",
      "    num_steps_sampled: 988000\n",
      "    num_steps_trained: 988000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.99827586206897\n",
      "    ram_util_percent: 95.97999999999999\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10471281361035613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9803329571641984\n",
      "    mean_inference_ms: 2.1179841135969983\n",
      "    mean_raw_obs_processing_ms: 0.19671264094367452\n",
      "  time_since_restore: 53704.068093538284\n",
      "  time_this_iter_s: 219.2155237197876\n",
      "  time_total_s: 53704.068093538284\n",
      "  timers:\n",
      "    learn_throughput: 19.071\n",
      "    learn_time_ms: 209745.817\n",
      "    load_throughput: 3692087.762\n",
      "    load_time_ms: 1.083\n",
      "    sample_throughput: 18.226\n",
      "    sample_time_ms: 219460.743\n",
      "    update_time_ms: 21.885\n",
      "  timestamp: 1650272053\n",
      "  timesteps_since_restore: 988000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 988000\n",
      "  training_iteration: 247\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 992000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-57-52\n",
      "  done: false\n",
      "  episode_len_mean: 545.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.57\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2902\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6561132073402405\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01655813492834568\n",
      "          model: {}\n",
      "          policy_loss: -0.0427798368036747\n",
      "          total_loss: 0.062018461525440216\n",
      "          vf_explained_var: 0.714232861995697\n",
      "          vf_loss: 0.10479830205440521\n",
      "    num_agent_steps_sampled: 992000\n",
      "    num_agent_steps_trained: 992000\n",
      "    num_steps_sampled: 992000\n",
      "    num_steps_trained: 992000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.71280276816609\n",
      "    ram_util_percent: 95.96955017301038\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10462401745896077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9796786465752941\n",
      "    mean_inference_ms: 2.1151406282418974\n",
      "    mean_raw_obs_processing_ms: 0.19648269887265396\n",
      "  time_since_restore: 53922.68009233475\n",
      "  time_this_iter_s: 218.611998796463\n",
      "  time_total_s: 53922.68009233475\n",
      "  timers:\n",
      "    learn_throughput: 19.068\n",
      "    learn_time_ms: 209772.857\n",
      "    load_throughput: 7537273.013\n",
      "    load_time_ms: 0.531\n",
      "    sample_throughput: 18.222\n",
      "    sample_time_ms: 219520.698\n",
      "    update_time_ms: 19.473\n",
      "  timestamp: 1650272272\n",
      "  timesteps_since_restore: 992000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 992000\n",
      "  training_iteration: 248\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 992000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_09-57-52\n",
      "  done: false\n",
      "  episode_len_mean: 1722.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 522\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1021660796977127e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.025315302541808e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0005843233084306121\n",
      "          total_loss: -0.0004113017348572612\n",
      "          vf_explained_var: -0.0003500765305943787\n",
      "          vf_loss: 0.00017302355263382196\n",
      "    num_agent_steps_sampled: 992000\n",
      "    num_agent_steps_trained: 992000\n",
      "    num_steps_sampled: 992000\n",
      "    num_steps_trained: 992000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.03275862068965\n",
      "    ram_util_percent: 95.98137931034482\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10668014157486606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.972374269433125\n",
      "    mean_inference_ms: 2.1653784434190064\n",
      "    mean_raw_obs_processing_ms: 0.12999658110547244\n",
      "  time_since_restore: 53915.496888160706\n",
      "  time_this_iter_s: 219.40353298187256\n",
      "  time_total_s: 53915.496888160706\n",
      "  timers:\n",
      "    learn_throughput: 19.047\n",
      "    learn_time_ms: 210002.877\n",
      "    load_throughput: 3274881.124\n",
      "    load_time_ms: 1.221\n",
      "    sample_throughput: 18.239\n",
      "    sample_time_ms: 219314.51\n",
      "    update_time_ms: 13.472\n",
      "  timestamp: 1650272272\n",
      "  timesteps_since_restore: 992000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 992000\n",
      "  training_iteration: 248\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 09:59:05 (running for 15:00:25.46)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.57 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 996000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-01-32\n",
      "  done: false\n",
      "  episode_len_mean: 552.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.61\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2909\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7111895084381104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01160995103418827\n",
      "          model: {}\n",
      "          policy_loss: -0.044581785798072815\n",
      "          total_loss: 0.030979832634329796\n",
      "          vf_explained_var: 0.7611109018325806\n",
      "          vf_loss: 0.07556161284446716\n",
      "    num_agent_steps_sampled: 996000\n",
      "    num_agent_steps_trained: 996000\n",
      "    num_steps_sampled: 996000\n",
      "    num_steps_trained: 996000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.98862068965516\n",
      "    ram_util_percent: 95.97724137931036\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10453466815779734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9790130206040167\n",
      "    mean_inference_ms: 2.112261012618666\n",
      "    mean_raw_obs_processing_ms: 0.1962495710778483\n",
      "  time_since_restore: 54142.62257218361\n",
      "  time_this_iter_s: 219.9424798488617\n",
      "  time_total_s: 54142.62257218361\n",
      "  timers:\n",
      "    learn_throughput: 19.052\n",
      "    learn_time_ms: 209953.013\n",
      "    load_throughput: 3667070.884\n",
      "    load_time_ms: 1.091\n",
      "    sample_throughput: 18.224\n",
      "    sample_time_ms: 219488.162\n",
      "    update_time_ms: 17.881\n",
      "  timestamp: 1650272492\n",
      "  timesteps_since_restore: 996000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 996000\n",
      "  training_iteration: 249\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 996000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-01-33\n",
      "  done: false\n",
      "  episode_len_mean: 1722.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 522\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0537047341590132e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.610847144094311e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412790734320879\n",
      "          total_loss: 0.014127925969660282\n",
      "          vf_explained_var: -1.3254021951070172e-06\n",
      "          vf_loss: 2.6026425459235725e-08\n",
      "    num_agent_steps_sampled: 996000\n",
      "    num_agent_steps_trained: 996000\n",
      "    num_steps_sampled: 996000\n",
      "    num_steps_trained: 996000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.37044673539519\n",
      "    ram_util_percent: 95.99072164948453\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10668014157486606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.972374269433125\n",
      "    mean_inference_ms: 2.1653784434190064\n",
      "    mean_raw_obs_processing_ms: 0.12999658110547244\n",
      "  time_since_restore: 54136.003278017044\n",
      "  time_this_iter_s: 220.5063898563385\n",
      "  time_total_s: 54136.003278017044\n",
      "  timers:\n",
      "    learn_throughput: 19.03\n",
      "    learn_time_ms: 210190.542\n",
      "    load_throughput: 2874533.71\n",
      "    load_time_ms: 1.392\n",
      "    sample_throughput: 18.231\n",
      "    sample_time_ms: 219410.925\n",
      "    update_time_ms: 13.515\n",
      "  timestamp: 1650272493\n",
      "  timesteps_since_restore: 996000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 996000\n",
      "  training_iteration: 249\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1000000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-05-10\n",
      "  done: false\n",
      "  episode_len_mean: 550.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.56\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2916\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7174215316772461\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013390048407018185\n",
      "          model: {}\n",
      "          policy_loss: -0.04397011175751686\n",
      "          total_loss: 0.03010973334312439\n",
      "          vf_explained_var: 0.7399919033050537\n",
      "          vf_loss: 0.07407984882593155\n",
      "    num_agent_steps_sampled: 1000000\n",
      "    num_agent_steps_trained: 1000000\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.92941176470589\n",
      "    ram_util_percent: 95.96885813148789\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10444519177933163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9783453536805001\n",
      "    mean_inference_ms: 2.109381425871207\n",
      "    mean_raw_obs_processing_ms: 0.196017543241584\n",
      "  time_since_restore: 54361.16332697868\n",
      "  time_this_iter_s: 218.54075479507446\n",
      "  time_total_s: 54361.16332697868\n",
      "  timers:\n",
      "    learn_throughput: 19.069\n",
      "    learn_time_ms: 209762.633\n",
      "    load_throughput: 3673092.215\n",
      "    load_time_ms: 1.089\n",
      "    sample_throughput: 18.213\n",
      "    sample_time_ms: 219623.747\n",
      "    update_time_ms: 17.266\n",
      "  timestamp: 1650272710\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 250\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1000000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-05-12\n",
      "  done: false\n",
      "  episode_len_mean: 1722.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 525\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.926449109032483e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.7611299341185735e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.00029500824166461825\n",
      "          total_loss: 0.0003604902303777635\n",
      "          vf_explained_var: -8.504185825586319e-05\n",
      "          vf_loss: 6.548391684191301e-05\n",
      "    num_agent_steps_sampled: 1000000\n",
      "    num_agent_steps_trained: 1000000\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.78625429553264\n",
      "    ram_util_percent: 95.98694158075601\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10652791081364249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9712708675202666\n",
      "    mean_inference_ms: 2.1603651085830817\n",
      "    mean_raw_obs_processing_ms: 0.12980475711257336\n",
      "  time_since_restore: 54355.40671801567\n",
      "  time_this_iter_s: 219.4034399986267\n",
      "  time_total_s: 54355.40671801567\n",
      "  timers:\n",
      "    learn_throughput: 19.035\n",
      "    learn_time_ms: 210137.888\n",
      "    load_throughput: 3023412.085\n",
      "    load_time_ms: 1.323\n",
      "    sample_throughput: 18.214\n",
      "    sample_time_ms: 219611.959\n",
      "    update_time_ms: 13.655\n",
      "  timestamp: 1650272712\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 250\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1004000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-08-48\n",
      "  done: false\n",
      "  episode_len_mean: 554.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.6\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2923\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7448339462280273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015039660036563873\n",
      "          model: {}\n",
      "          policy_loss: -0.04323316365480423\n",
      "          total_loss: 0.05044084042310715\n",
      "          vf_explained_var: 0.7030999660491943\n",
      "          vf_loss: 0.09367401152849197\n",
      "    num_agent_steps_sampled: 1004000\n",
      "    num_agent_steps_trained: 1004000\n",
      "    num_steps_sampled: 1004000\n",
      "    num_steps_trained: 1004000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.02083333333333\n",
      "    ram_util_percent: 95.88958333333332\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10435646687124947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9776837908581584\n",
      "    mean_inference_ms: 2.106530777985441\n",
      "    mean_raw_obs_processing_ms: 0.1957887452103892\n",
      "  time_since_restore: 54578.467410326004\n",
      "  time_this_iter_s: 217.30408334732056\n",
      "  time_total_s: 54578.467410326004\n",
      "  timers:\n",
      "    learn_throughput: 19.091\n",
      "    learn_time_ms: 209520.203\n",
      "    load_throughput: 3671082.908\n",
      "    load_time_ms: 1.09\n",
      "    sample_throughput: 18.222\n",
      "    sample_time_ms: 219512.394\n",
      "    update_time_ms: 17.859\n",
      "  timestamp: 1650272928\n",
      "  timesteps_since_restore: 1004000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1004000\n",
      "  training_iteration: 251\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1004000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-08-50\n",
      "  done: false\n",
      "  episode_len_mean: 1722.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 525\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128301292657852\n",
      "          total_loss: -0.014127688482403755\n",
      "          vf_explained_var: 1.25233839298744e-07\n",
      "          vf_loss: 6.152259288683126e-07\n",
      "    num_agent_steps_sampled: 1004000\n",
      "    num_agent_steps_trained: 1004000\n",
      "    num_steps_sampled: 1004000\n",
      "    num_steps_trained: 1004000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.63979238754325\n",
      "    ram_util_percent: 95.90553633217992\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10652791081364249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9712708675202666\n",
      "    mean_inference_ms: 2.1603651085830817\n",
      "    mean_raw_obs_processing_ms: 0.12980475711257336\n",
      "  time_since_restore: 54573.42084312439\n",
      "  time_this_iter_s: 218.01412510871887\n",
      "  time_total_s: 54573.42084312439\n",
      "  timers:\n",
      "    learn_throughput: 19.053\n",
      "    learn_time_ms: 209937.35\n",
      "    load_throughput: 3019113.91\n",
      "    load_time_ms: 1.325\n",
      "    sample_throughput: 18.215\n",
      "    sample_time_ms: 219603.008\n",
      "    update_time_ms: 13.455\n",
      "  timestamp: 1650272930\n",
      "  timesteps_since_restore: 1004000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1004000\n",
      "  training_iteration: 251\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1008000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-12-27\n",
      "  done: false\n",
      "  episode_len_mean: 555.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.55\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2931\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7720248699188232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013142017647624016\n",
      "          model: {}\n",
      "          policy_loss: -0.0504121296107769\n",
      "          total_loss: 0.006155069451779127\n",
      "          vf_explained_var: 0.791210412979126\n",
      "          vf_loss: 0.05656720697879791\n",
      "    num_agent_steps_sampled: 1008000\n",
      "    num_agent_steps_trained: 1008000\n",
      "    num_steps_sampled: 1008000\n",
      "    num_steps_trained: 1008000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.44172413793103\n",
      "    ram_util_percent: 95.86068965517241\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10425549091184454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9769347772309758\n",
      "    mean_inference_ms: 2.1033033307778397\n",
      "    mean_raw_obs_processing_ms: 0.19552961456994894\n",
      "  time_since_restore: 54797.75929737091\n",
      "  time_this_iter_s: 219.29188704490662\n",
      "  time_total_s: 54797.75929737091\n",
      "  timers:\n",
      "    learn_throughput: 19.078\n",
      "    learn_time_ms: 209660.66\n",
      "    load_throughput: 3675506.288\n",
      "    load_time_ms: 1.088\n",
      "    sample_throughput: 18.236\n",
      "    sample_time_ms: 219340.705\n",
      "    update_time_ms: 17.788\n",
      "  timestamp: 1650273147\n",
      "  timesteps_since_restore: 1008000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1008000\n",
      "  training_iteration: 252\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1008000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-12-29\n",
      "  done: false\n",
      "  episode_len_mean: 1722.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 525\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128188602626324\n",
      "          total_loss: -0.014128041453659534\n",
      "          vf_explained_var: 8.04983173452456e-08\n",
      "          vf_loss: 1.5556820187612175e-07\n",
      "    num_agent_steps_sampled: 1008000\n",
      "    num_agent_steps_trained: 1008000\n",
      "    num_steps_sampled: 1008000\n",
      "    num_steps_trained: 1008000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.75798611111111\n",
      "    ram_util_percent: 95.85208333333333\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10652791081364249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9712708675202666\n",
      "    mean_inference_ms: 2.1603651085830817\n",
      "    mean_raw_obs_processing_ms: 0.12980475711257336\n",
      "  time_since_restore: 54791.69817018509\n",
      "  time_this_iter_s: 218.27732706069946\n",
      "  time_total_s: 54791.69817018509\n",
      "  timers:\n",
      "    learn_throughput: 19.05\n",
      "    learn_time_ms: 209971.479\n",
      "    load_throughput: 3176420.16\n",
      "    load_time_ms: 1.259\n",
      "    sample_throughput: 18.227\n",
      "    sample_time_ms: 219458.053\n",
      "    update_time_ms: 12.789\n",
      "  timestamp: 1650273149\n",
      "  timesteps_since_restore: 1008000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1008000\n",
      "  training_iteration: 252\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 10:15:46 (running for 15:17:06.36)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.55 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1012000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-16-10\n",
      "  done: false\n",
      "  episode_len_mean: 559.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.66\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2938\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7036807537078857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014924349263310432\n",
      "          model: {}\n",
      "          policy_loss: -0.047133274376392365\n",
      "          total_loss: 0.04626917839050293\n",
      "          vf_explained_var: 0.699821949005127\n",
      "          vf_loss: 0.0934024453163147\n",
      "    num_agent_steps_sampled: 1012000\n",
      "    num_agent_steps_trained: 1012000\n",
      "    num_steps_sampled: 1012000\n",
      "    num_steps_trained: 1012000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.92891156462585\n",
      "    ram_util_percent: 95.70544217687075\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10416648165282441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9762821037923406\n",
      "    mean_inference_ms: 2.100508936206994\n",
      "    mean_raw_obs_processing_ms: 0.19529883750697166\n",
      "  time_since_restore: 55019.87591648102\n",
      "  time_this_iter_s: 222.11661911010742\n",
      "  time_total_s: 55019.87591648102\n",
      "  timers:\n",
      "    learn_throughput: 19.052\n",
      "    learn_time_ms: 209950.009\n",
      "    load_throughput: 3683657.042\n",
      "    load_time_ms: 1.086\n",
      "    sample_throughput: 18.224\n",
      "    sample_time_ms: 219495.052\n",
      "    update_time_ms: 22.128\n",
      "  timestamp: 1650273370\n",
      "  timesteps_since_restore: 1012000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1012000\n",
      "  training_iteration: 253\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1012000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-16-10\n",
      "  done: false\n",
      "  episode_len_mean: 1816.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 530\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.960103727736027e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.147307647033528e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.00011622074816841632\n",
      "          total_loss: -9.334473725175485e-05\n",
      "          vf_explained_var: -0.10518808662891388\n",
      "          vf_loss: 2.287627648911439e-05\n",
      "    num_agent_steps_sampled: 1012000\n",
      "    num_agent_steps_trained: 1012000\n",
      "    num_steps_sampled: 1012000\n",
      "    num_steps_trained: 1012000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.9486301369863\n",
      "    ram_util_percent: 95.67431506849316\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10627715899970548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9694466005759369\n",
      "    mean_inference_ms: 2.1520722398487586\n",
      "    mean_raw_obs_processing_ms: 0.12948454853931401\n",
      "  time_since_restore: 55013.127217292786\n",
      "  time_this_iter_s: 221.42904710769653\n",
      "  time_total_s: 55013.127217292786\n",
      "  timers:\n",
      "    learn_throughput: 19.03\n",
      "    learn_time_ms: 210198.392\n",
      "    load_throughput: 4000957.718\n",
      "    load_time_ms: 1.0\n",
      "    sample_throughput: 18.221\n",
      "    sample_time_ms: 219525.223\n",
      "    update_time_ms: 13.54\n",
      "  timestamp: 1650273370\n",
      "  timesteps_since_restore: 1012000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1012000\n",
      "  training_iteration: 253\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1016000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-19-51\n",
      "  done: false\n",
      "  episode_len_mean: 560.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.7\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2947\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7640518546104431\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013009070418775082\n",
      "          model: {}\n",
      "          policy_loss: -0.046993743628263474\n",
      "          total_loss: 0.02739620767533779\n",
      "          vf_explained_var: 0.7427437901496887\n",
      "          vf_loss: 0.07438995689153671\n",
      "    num_agent_steps_sampled: 1016000\n",
      "    num_agent_steps_trained: 1016000\n",
      "    num_steps_sampled: 1016000\n",
      "    num_steps_trained: 1016000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.82832764505119\n",
      "    ram_util_percent: 95.9283276450512\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1040498835605226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.975429857203787\n",
      "    mean_inference_ms: 2.0968558294038404\n",
      "    mean_raw_obs_processing_ms: 0.19500100730373268\n",
      "  time_since_restore: 55241.384811639786\n",
      "  time_this_iter_s: 221.5088951587677\n",
      "  time_total_s: 55241.384811639786\n",
      "  timers:\n",
      "    learn_throughput: 19.053\n",
      "    learn_time_ms: 209941.67\n",
      "    load_throughput: 3690057.625\n",
      "    load_time_ms: 1.084\n",
      "    sample_throughput: 18.201\n",
      "    sample_time_ms: 219762.769\n",
      "    update_time_ms: 28.978\n",
      "  timestamp: 1650273591\n",
      "  timesteps_since_restore: 1016000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1016000\n",
      "  training_iteration: 254\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1016000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-19-52\n",
      "  done: false\n",
      "  episode_len_mean: 1816.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 530\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0018432379001751542\n",
      "          total_loss: -0.0018432376673445106\n",
      "          vf_explained_var: 3.80701276299078e-05\n",
      "          vf_loss: 6.982729949811528e-10\n",
      "    num_agent_steps_sampled: 1016000\n",
      "    num_agent_steps_trained: 1016000\n",
      "    num_steps_sampled: 1016000\n",
      "    num_steps_trained: 1016000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.85476190476192\n",
      "    ram_util_percent: 95.92517006802721\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10627715899970548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9694466005759369\n",
      "    mean_inference_ms: 2.1520722398487586\n",
      "    mean_raw_obs_processing_ms: 0.12948454853931401\n",
      "  time_since_restore: 55234.657125234604\n",
      "  time_this_iter_s: 221.52990794181824\n",
      "  time_total_s: 55234.657125234604\n",
      "  timers:\n",
      "    learn_throughput: 19.025\n",
      "    learn_time_ms: 210250.65\n",
      "    load_throughput: 3969247.658\n",
      "    load_time_ms: 1.008\n",
      "    sample_throughput: 18.205\n",
      "    sample_time_ms: 219715.637\n",
      "    update_time_ms: 13.046\n",
      "  timestamp: 1650273592\n",
      "  timesteps_since_restore: 1016000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1016000\n",
      "  training_iteration: 254\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1020000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 555.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.59\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2955\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6943894624710083\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014568163082003593\n",
      "          model: {}\n",
      "          policy_loss: -0.040060654282569885\n",
      "          total_loss: 0.03796280547976494\n",
      "          vf_explained_var: 0.6905121207237244\n",
      "          vf_loss: 0.07802346348762512\n",
      "    num_agent_steps_sampled: 1020000\n",
      "    num_agent_steps_trained: 1020000\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.30308219178082\n",
      "    ram_util_percent: 95.79794520547945\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10394812834673262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9746873356222713\n",
      "    mean_inference_ms: 2.0936641575918062\n",
      "    mean_raw_obs_processing_ms: 0.194743344903835\n",
      "  time_since_restore: 55460.91383385658\n",
      "  time_this_iter_s: 219.52902221679688\n",
      "  time_total_s: 55460.91383385658\n",
      "  timers:\n",
      "    learn_throughput: 19.073\n",
      "    learn_time_ms: 209720.848\n",
      "    load_throughput: 3681232.255\n",
      "    load_time_ms: 1.087\n",
      "    sample_throughput: 18.203\n",
      "    sample_time_ms: 219745.376\n",
      "    update_time_ms: 28.211\n",
      "  timestamp: 1650273811\n",
      "  timesteps_since_restore: 1020000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 255\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1020000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 1912.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 531\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1140611954001666e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.655815188199015e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.013796621933579445\n",
      "          total_loss: 0.014138231985270977\n",
      "          vf_explained_var: 0.03186032921075821\n",
      "          vf_loss: 0.00034161636722274125\n",
      "    num_agent_steps_sampled: 1020000\n",
      "    num_agent_steps_trained: 1020000\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.00993150684933\n",
      "    ram_util_percent: 95.7678082191781\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1062250790569756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9690678282241736\n",
      "    mean_inference_ms: 2.150349756639186\n",
      "    mean_raw_obs_processing_ms: 0.1294175763069935\n",
      "  time_since_restore: 55454.15504145622\n",
      "  time_this_iter_s: 219.49791622161865\n",
      "  time_total_s: 55454.15504145622\n",
      "  timers:\n",
      "    learn_throughput: 19.036\n",
      "    learn_time_ms: 210132.055\n",
      "    load_throughput: 4003726.613\n",
      "    load_time_ms: 0.999\n",
      "    sample_throughput: 18.202\n",
      "    sample_time_ms: 219758.548\n",
      "    update_time_ms: 11.643\n",
      "  timestamp: 1650273811\n",
      "  timesteps_since_restore: 1020000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 255\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1024000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-27-11\n",
      "  done: false\n",
      "  episode_len_mean: 1913.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 534\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1936431764236503e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.4677482254260986e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0010720170103013515\n",
      "          total_loss: 0.0011162792798131704\n",
      "          vf_explained_var: 0.19331225752830505\n",
      "          vf_loss: 4.4264110329095274e-05\n",
      "    num_agent_steps_sampled: 1024000\n",
      "    num_agent_steps_trained: 1024000\n",
      "    num_steps_sampled: 1024000\n",
      "    num_steps_trained: 1024000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.53917525773196\n",
      "    ram_util_percent: 95.79140893470792\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10606612893190029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9679109948867811\n",
      "    mean_inference_ms: 2.1450868722692844\n",
      "    mean_raw_obs_processing_ms: 0.12921409503932238\n",
      "  time_since_restore: 55673.32978844643\n",
      "  time_this_iter_s: 219.17474699020386\n",
      "  time_total_s: 55673.32978844643\n",
      "  timers:\n",
      "    learn_throughput: 19.031\n",
      "    learn_time_ms: 210186.774\n",
      "    load_throughput: 3944888.429\n",
      "    load_time_ms: 1.014\n",
      "    sample_throughput: 18.214\n",
      "    sample_time_ms: 219608.092\n",
      "    update_time_ms: 8.267\n",
      "  timestamp: 1650274031\n",
      "  timesteps_since_restore: 1024000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1024000\n",
      "  training_iteration: 256\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1024000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-27-11\n",
      "  done: false\n",
      "  episode_len_mean: 549.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.48\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2963\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7162425518035889\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013980723917484283\n",
      "          model: {}\n",
      "          policy_loss: -0.04471730813384056\n",
      "          total_loss: 0.03826399892568588\n",
      "          vf_explained_var: 0.7504829168319702\n",
      "          vf_loss: 0.08298131823539734\n",
      "    num_agent_steps_sampled: 1024000\n",
      "    num_agent_steps_trained: 1024000\n",
      "    num_steps_sampled: 1024000\n",
      "    num_steps_trained: 1024000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.72260273972603\n",
      "    ram_util_percent: 95.78869863013699\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10384643060378687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9739460448928157\n",
      "    mean_inference_ms: 2.0904740431784785\n",
      "    mean_raw_obs_processing_ms: 0.19448734297464548\n",
      "  time_since_restore: 55680.637151002884\n",
      "  time_this_iter_s: 219.72331714630127\n",
      "  time_total_s: 55680.637151002884\n",
      "  timers:\n",
      "    learn_throughput: 19.058\n",
      "    learn_time_ms: 209889.463\n",
      "    load_throughput: 3654609.537\n",
      "    load_time_ms: 1.095\n",
      "    sample_throughput: 18.225\n",
      "    sample_time_ms: 219476.277\n",
      "    update_time_ms: 28.09\n",
      "  timestamp: 1650274031\n",
      "  timesteps_since_restore: 1024000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1024000\n",
      "  training_iteration: 256\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1028000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-30-49\n",
      "  done: false\n",
      "  episode_len_mean: 1913.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 534\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412827055901289\n",
      "          total_loss: 0.014128339476883411\n",
      "          vf_explained_var: -6.420638101189979e-07\n",
      "          vf_loss: 7.791326339656734e-08\n",
      "    num_agent_steps_sampled: 1028000\n",
      "    num_agent_steps_trained: 1028000\n",
      "    num_steps_sampled: 1028000\n",
      "    num_steps_trained: 1028000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.22048611111111\n",
      "    ram_util_percent: 95.81770833333333\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10606612893190029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9679109948867811\n",
      "    mean_inference_ms: 2.1450868722692844\n",
      "    mean_raw_obs_processing_ms: 0.12921409503932238\n",
      "  time_since_restore: 55891.51643037796\n",
      "  time_this_iter_s: 218.1866419315338\n",
      "  time_total_s: 55891.51643037796\n",
      "  timers:\n",
      "    learn_throughput: 19.034\n",
      "    learn_time_ms: 210146.607\n",
      "    load_throughput: 4994408.192\n",
      "    load_time_ms: 0.801\n",
      "    sample_throughput: 18.214\n",
      "    sample_time_ms: 219607.515\n",
      "    update_time_ms: 7.813\n",
      "  timestamp: 1650274249\n",
      "  timesteps_since_restore: 1028000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1028000\n",
      "  training_iteration: 257\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1028000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-30-50\n",
      "  done: false\n",
      "  episode_len_mean: 551.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.52\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2971\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6997091174125671\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016122734174132347\n",
      "          model: {}\n",
      "          policy_loss: -0.03745212405920029\n",
      "          total_loss: 0.037828851491212845\n",
      "          vf_explained_var: 0.7393869757652283\n",
      "          vf_loss: 0.07528097182512283\n",
      "    num_agent_steps_sampled: 1028000\n",
      "    num_agent_steps_trained: 1028000\n",
      "    num_steps_sampled: 1028000\n",
      "    num_steps_trained: 1028000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.74221453287197\n",
      "    ram_util_percent: 95.84152249134947\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10374799118042093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9732248966243165\n",
      "    mean_inference_ms: 2.0873508513516734\n",
      "    mean_raw_obs_processing_ms: 0.1942389030719002\n",
      "  time_since_restore: 55899.54043388367\n",
      "  time_this_iter_s: 218.90328288078308\n",
      "  time_total_s: 55899.54043388367\n",
      "  timers:\n",
      "    learn_throughput: 19.063\n",
      "    learn_time_ms: 209829.259\n",
      "    load_throughput: 3597404.636\n",
      "    load_time_ms: 1.112\n",
      "    sample_throughput: 18.209\n",
      "    sample_time_ms: 219669.527\n",
      "    update_time_ms: 28.073\n",
      "  timestamp: 1650274250\n",
      "  timesteps_since_restore: 1028000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1028000\n",
      "  training_iteration: 257\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 10:32:27 (running for 15:33:47.09)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.52 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1032000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-34-26\n",
      "  done: false\n",
      "  episode_len_mean: 550.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.52\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2978\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7166225910186768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01583259180188179\n",
      "          model: {}\n",
      "          policy_loss: -0.04235881567001343\n",
      "          total_loss: 0.038484666496515274\n",
      "          vf_explained_var: 0.7256597280502319\n",
      "          vf_loss: 0.080843485891819\n",
      "    num_agent_steps_sampled: 1032000\n",
      "    num_agent_steps_trained: 1032000\n",
      "    num_steps_sampled: 1032000\n",
      "    num_steps_trained: 1032000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.85993031358885\n",
      "    ram_util_percent: 95.85296167247385\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10366298175683702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9726015687870486\n",
      "    mean_inference_ms: 2.084658622028865\n",
      "    mean_raw_obs_processing_ms: 0.19402360763965693\n",
      "  time_since_restore: 56115.908488988876\n",
      "  time_this_iter_s: 216.36805510520935\n",
      "  time_total_s: 56115.908488988876\n",
      "  timers:\n",
      "    learn_throughput: 19.087\n",
      "    learn_time_ms: 209566.951\n",
      "    load_throughput: 3557886.969\n",
      "    load_time_ms: 1.124\n",
      "    sample_throughput: 18.211\n",
      "    sample_time_ms: 219644.212\n",
      "    update_time_ms: 28.45\n",
      "  timestamp: 1650274466\n",
      "  timesteps_since_restore: 1032000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1032000\n",
      "  training_iteration: 258\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1032000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-34-27\n",
      "  done: false\n",
      "  episode_len_mean: 1911.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 537\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.919366370419376e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.364814959057485e-28\n",
      "          model: {}\n",
      "          policy_loss: -1.5473749954253435e-05\n",
      "          total_loss: 0.00014769454719498754\n",
      "          vf_explained_var: -0.02430446445941925\n",
      "          vf_loss: 0.00016317148401867598\n",
      "    num_agent_steps_sampled: 1032000\n",
      "    num_agent_steps_trained: 1032000\n",
      "    num_steps_sampled: 1032000\n",
      "    num_steps_trained: 1032000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.99446366782007\n",
      "    ram_util_percent: 95.86574394463666\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10591155736951421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9667866934719723\n",
      "    mean_inference_ms: 2.1399677850957755\n",
      "    mean_raw_obs_processing_ms: 0.1290156529156526\n",
      "  time_since_restore: 56109.251687288284\n",
      "  time_this_iter_s: 217.7352569103241\n",
      "  time_total_s: 56109.251687288284\n",
      "  timers:\n",
      "    learn_throughput: 19.051\n",
      "    learn_time_ms: 209958.527\n",
      "    load_throughput: 4953561.074\n",
      "    load_time_ms: 0.807\n",
      "    sample_throughput: 18.217\n",
      "    sample_time_ms: 219579.966\n",
      "    update_time_ms: 8.284\n",
      "  timestamp: 1650274467\n",
      "  timesteps_since_restore: 1032000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1032000\n",
      "  training_iteration: 258\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1036000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-38-05\n",
      "  done: false\n",
      "  episode_len_mean: 1911.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 537\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014127838425338268\n",
      "          total_loss: -0.014111301861703396\n",
      "          vf_explained_var: 9.165015413259425e-09\n",
      "          vf_loss: 1.653316394367721e-05\n",
      "    num_agent_steps_sampled: 1036000\n",
      "    num_agent_steps_trained: 1036000\n",
      "    num_steps_sampled: 1036000\n",
      "    num_steps_trained: 1036000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.02395833333334\n",
      "    ram_util_percent: 95.91944444444444\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10591155736951421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9667866934719723\n",
      "    mean_inference_ms: 2.1399677850957755\n",
      "    mean_raw_obs_processing_ms: 0.1290156529156526\n",
      "  time_since_restore: 56327.24138426781\n",
      "  time_this_iter_s: 217.9896969795227\n",
      "  time_total_s: 56327.24138426781\n",
      "  timers:\n",
      "    learn_throughput: 19.075\n",
      "    learn_time_ms: 209702.57\n",
      "    load_throughput: 6318864.073\n",
      "    load_time_ms: 0.633\n",
      "    sample_throughput: 18.232\n",
      "    sample_time_ms: 219394.28\n",
      "    update_time_ms: 8.8\n",
      "  timestamp: 1650274685\n",
      "  timesteps_since_restore: 1036000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1036000\n",
      "  training_iteration: 259\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1036000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-38-05\n",
      "  done: false\n",
      "  episode_len_mean: 548.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.47\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2987\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7189907431602478\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012286133132874966\n",
      "          model: {}\n",
      "          policy_loss: -0.04424981400370598\n",
      "          total_loss: 0.051874395459890366\n",
      "          vf_explained_var: 0.6934902667999268\n",
      "          vf_loss: 0.09612420946359634\n",
      "    num_agent_steps_sampled: 1036000\n",
      "    num_agent_steps_trained: 1036000\n",
      "    num_steps_sampled: 1036000\n",
      "    num_steps_trained: 1036000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.26\n",
      "    ram_util_percent: 95.92413793103448\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10355444807444153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9718054792710697\n",
      "    mean_inference_ms: 2.081201693212496\n",
      "    mean_raw_obs_processing_ms: 0.19374642919017782\n",
      "  time_since_restore: 56334.50157499313\n",
      "  time_this_iter_s: 218.5930860042572\n",
      "  time_total_s: 56334.50157499313\n",
      "  timers:\n",
      "    learn_throughput: 19.1\n",
      "    learn_time_ms: 209420.946\n",
      "    load_throughput: 6695085.997\n",
      "    load_time_ms: 0.597\n",
      "    sample_throughput: 18.232\n",
      "    sample_time_ms: 219399.444\n",
      "    update_time_ms: 27.108\n",
      "  timestamp: 1650274685\n",
      "  timesteps_since_restore: 1036000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1036000\n",
      "  training_iteration: 259\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1040000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 1911.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 537\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0141282444819808\n",
      "          total_loss: -0.014125405810773373\n",
      "          vf_explained_var: 3.3711874181108215e-08\n",
      "          vf_loss: 2.845836434062221e-06\n",
      "    num_agent_steps_sampled: 1040000\n",
      "    num_agent_steps_trained: 1040000\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.50412371134021\n",
      "    ram_util_percent: 95.97525773195875\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10591155736951421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9667866934719723\n",
      "    mean_inference_ms: 2.1399677850957755\n",
      "    mean_raw_obs_processing_ms: 0.1290156529156526\n",
      "  time_since_restore: 56547.78986620903\n",
      "  time_this_iter_s: 220.54848194122314\n",
      "  time_total_s: 56547.78986620903\n",
      "  timers:\n",
      "    learn_throughput: 19.06\n",
      "    learn_time_ms: 209862.373\n",
      "    load_throughput: 6380139.945\n",
      "    load_time_ms: 0.627\n",
      "    sample_throughput: 18.257\n",
      "    sample_time_ms: 219094.123\n",
      "    update_time_ms: 9.645\n",
      "  timestamp: 1650274905\n",
      "  timesteps_since_restore: 1040000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 260\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1040000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-41-46\n",
      "  done: false\n",
      "  episode_len_mean: 550.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.47\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2995\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7092039585113525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015494253486394882\n",
      "          model: {}\n",
      "          policy_loss: -0.04475371539592743\n",
      "          total_loss: 0.03516377508640289\n",
      "          vf_explained_var: 0.7225450873374939\n",
      "          vf_loss: 0.07991749793291092\n",
      "    num_agent_steps_sampled: 1040000\n",
      "    num_agent_steps_trained: 1040000\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.43883161512028\n",
      "    ram_util_percent: 95.9573883161512\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345850445103895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9711046220098841\n",
      "    mean_inference_ms: 2.078179228498141\n",
      "    mean_raw_obs_processing_ms: 0.19350372437645244\n",
      "  time_since_restore: 56555.22320437431\n",
      "  time_this_iter_s: 220.7216293811798\n",
      "  time_total_s: 56555.22320437431\n",
      "  timers:\n",
      "    learn_throughput: 19.078\n",
      "    learn_time_ms: 209664.628\n",
      "    load_throughput: 6662913.423\n",
      "    load_time_ms: 0.6\n",
      "    sample_throughput: 18.247\n",
      "    sample_time_ms: 219219.635\n",
      "    update_time_ms: 26.588\n",
      "  timestamp: 1650274906\n",
      "  timesteps_since_restore: 1040000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 260\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1044000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-45-24\n",
      "  done: false\n",
      "  episode_len_mean: 1916.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 542\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1010071221113435e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8518814817366443e-26\n",
      "          model: {}\n",
      "          policy_loss: -9.065328049473464e-05\n",
      "          total_loss: 0.000436037196777761\n",
      "          vf_explained_var: 0.1916312426328659\n",
      "          vf_loss: 0.0005266779335215688\n",
      "    num_agent_steps_sampled: 1044000\n",
      "    num_agent_steps_trained: 1044000\n",
      "    num_steps_sampled: 1044000\n",
      "    num_steps_trained: 1044000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.67793103448275\n",
      "    ram_util_percent: 95.69724137931034\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10565232577589372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9648997735048367\n",
      "    mean_inference_ms: 2.1313815930487476\n",
      "    mean_raw_obs_processing_ms: 0.12868267387999657\n",
      "  time_since_restore: 56766.80477523804\n",
      "  time_this_iter_s: 219.01490902900696\n",
      "  time_total_s: 56766.80477523804\n",
      "  timers:\n",
      "    learn_throughput: 19.045\n",
      "    learn_time_ms: 210034.006\n",
      "    load_throughput: 6497004.996\n",
      "    load_time_ms: 0.616\n",
      "    sample_throughput: 18.25\n",
      "    sample_time_ms: 219178.844\n",
      "    update_time_ms: 11.348\n",
      "  timestamp: 1650275124\n",
      "  timesteps_since_restore: 1044000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1044000\n",
      "  training_iteration: 261\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1044000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-45-25\n",
      "  done: false\n",
      "  episode_len_mean: 547.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.37\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3001\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6871047616004944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01667638123035431\n",
      "          model: {}\n",
      "          policy_loss: -0.03380592167377472\n",
      "          total_loss: 0.045307692140340805\n",
      "          vf_explained_var: 0.7783796191215515\n",
      "          vf_loss: 0.07911361008882523\n",
      "    num_agent_steps_sampled: 1044000\n",
      "    num_agent_steps_trained: 1044000\n",
      "    num_steps_sampled: 1044000\n",
      "    num_steps_trained: 1044000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.6123711340206\n",
      "    ram_util_percent: 95.67697594501718\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10338701331906217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9705815465308573\n",
      "    mean_inference_ms: 2.0759278189255577\n",
      "    mean_raw_obs_processing_ms: 0.19332213252187522\n",
      "  time_since_restore: 56774.44794130325\n",
      "  time_this_iter_s: 219.22473692893982\n",
      "  time_total_s: 56774.44794130325\n",
      "  timers:\n",
      "    learn_throughput: 19.052\n",
      "    learn_time_ms: 209948.065\n",
      "    load_throughput: 6594299.19\n",
      "    load_time_ms: 0.607\n",
      "    sample_throughput: 18.234\n",
      "    sample_time_ms: 219371.664\n",
      "    update_time_ms: 25.7\n",
      "  timestamp: 1650275125\n",
      "  timesteps_since_restore: 1044000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1044000\n",
      "  training_iteration: 261\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1048000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-49-04\n",
      "  done: false\n",
      "  episode_len_mean: 1916.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 542\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128024689853191\n",
      "          total_loss: -0.01411474123597145\n",
      "          vf_explained_var: -7.440966953708994e-08\n",
      "          vf_loss: 1.3276129720907193e-05\n",
      "    num_agent_steps_sampled: 1048000\n",
      "    num_agent_steps_trained: 1048000\n",
      "    num_steps_sampled: 1048000\n",
      "    num_steps_trained: 1048000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.94344827586207\n",
      "    ram_util_percent: 95.83379310344826\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10565232577589372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9648997735048367\n",
      "    mean_inference_ms: 2.1313815930487476\n",
      "    mean_raw_obs_processing_ms: 0.12868267387999657\n",
      "  time_since_restore: 56986.30576610565\n",
      "  time_this_iter_s: 219.50099086761475\n",
      "  time_total_s: 56986.30576610565\n",
      "  timers:\n",
      "    learn_throughput: 19.023\n",
      "    learn_time_ms: 210267.225\n",
      "    load_throughput: 6618231.164\n",
      "    load_time_ms: 0.604\n",
      "    sample_throughput: 18.245\n",
      "    sample_time_ms: 219242.225\n",
      "    update_time_ms: 12.034\n",
      "  timestamp: 1650275344\n",
      "  timesteps_since_restore: 1048000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1048000\n",
      "  training_iteration: 262\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1048000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-49-05\n",
      "  done: false\n",
      "  episode_len_mean: 545.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.35\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3009\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6531836986541748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014086399227380753\n",
      "          model: {}\n",
      "          policy_loss: -0.033748332411050797\n",
      "          total_loss: 0.07432663440704346\n",
      "          vf_explained_var: 0.7146756052970886\n",
      "          vf_loss: 0.10807497054338455\n",
      "    num_agent_steps_sampled: 1048000\n",
      "    num_agent_steps_trained: 1048000\n",
      "    num_steps_sampled: 1048000\n",
      "    num_steps_trained: 1048000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.89931271477664\n",
      "    ram_util_percent: 95.82749140893469\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10329140767966617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9698829494436362\n",
      "    mean_inference_ms: 2.072932629018687\n",
      "    mean_raw_obs_processing_ms: 0.1930808373533717\n",
      "  time_since_restore: 56994.12129354477\n",
      "  time_this_iter_s: 219.6733522415161\n",
      "  time_total_s: 56994.12129354477\n",
      "  timers:\n",
      "    learn_throughput: 19.042\n",
      "    learn_time_ms: 210066.721\n",
      "    load_throughput: 6544141.67\n",
      "    load_time_ms: 0.611\n",
      "    sample_throughput: 18.219\n",
      "    sample_time_ms: 219554.74\n",
      "    update_time_ms: 26.101\n",
      "  timestamp: 1650275345\n",
      "  timesteps_since_restore: 1048000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1048000\n",
      "  training_iteration: 262\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 10:49:08 (running for 15:50:27.77)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.35 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1052000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-52-43\n",
      "  done: false\n",
      "  episode_len_mean: 1916.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 542\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.02278215065598488\n",
      "          total_loss: -0.02277936227619648\n",
      "          vf_explained_var: -0.032257989048957825\n",
      "          vf_loss: 2.7917901661567157e-06\n",
      "    num_agent_steps_sampled: 1052000\n",
      "    num_agent_steps_trained: 1052000\n",
      "    num_steps_sampled: 1052000\n",
      "    num_steps_trained: 1052000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.75413793103449\n",
      "    ram_util_percent: 95.92758620689654\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10565232577589372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9648997735048367\n",
      "    mean_inference_ms: 2.1313815930487476\n",
      "    mean_raw_obs_processing_ms: 0.12868267387999657\n",
      "  time_since_restore: 57205.52649331093\n",
      "  time_this_iter_s: 219.2207272052765\n",
      "  time_total_s: 57205.52649331093\n",
      "  timers:\n",
      "    learn_throughput: 19.039\n",
      "    learn_time_ms: 210089.581\n",
      "    load_throughput: 6763642.814\n",
      "    load_time_ms: 0.591\n",
      "    sample_throughput: 18.228\n",
      "    sample_time_ms: 219436.967\n",
      "    update_time_ms: 12.924\n",
      "  timestamp: 1650275563\n",
      "  timesteps_since_restore: 1052000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1052000\n",
      "  training_iteration: 263\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1052000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-52-44\n",
      "  done: false\n",
      "  episode_len_mean: 542.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.31\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3017\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7567578554153442\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013798925094306469\n",
      "          model: {}\n",
      "          policy_loss: -0.04919742792844772\n",
      "          total_loss: 0.07714090496301651\n",
      "          vf_explained_var: 0.6236181259155273\n",
      "          vf_loss: 0.12633833289146423\n",
      "    num_agent_steps_sampled: 1052000\n",
      "    num_agent_steps_trained: 1052000\n",
      "    num_steps_sampled: 1052000\n",
      "    num_steps_trained: 1052000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.45827586206896\n",
      "    ram_util_percent: 95.93206896551725\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10319915788067956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9692044493828809\n",
      "    mean_inference_ms: 2.0700186629323962\n",
      "    mean_raw_obs_processing_ms: 0.19285057986678253\n",
      "  time_since_restore: 57213.53727555275\n",
      "  time_this_iter_s: 219.41598200798035\n",
      "  time_total_s: 57213.53727555275\n",
      "  timers:\n",
      "    learn_throughput: 19.065\n",
      "    learn_time_ms: 209811.339\n",
      "    load_throughput: 6268575.699\n",
      "    load_time_ms: 0.638\n",
      "    sample_throughput: 18.21\n",
      "    sample_time_ms: 219662.107\n",
      "    update_time_ms: 20.764\n",
      "  timestamp: 1650275564\n",
      "  timesteps_since_restore: 1052000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1052000\n",
      "  training_iteration: 263\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1056000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 1921.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 549\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.533226218370004e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.301774787848416e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.001489406917244196\n",
      "          total_loss: -0.0011981700081378222\n",
      "          vf_explained_var: 0.21333184838294983\n",
      "          vf_loss: 0.00029123519198037684\n",
      "    num_agent_steps_sampled: 1056000\n",
      "    num_agent_steps_trained: 1056000\n",
      "    num_steps_sampled: 1056000\n",
      "    num_steps_trained: 1056000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.8233676975945\n",
      "    ram_util_percent: 95.96666666666665\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10530416426048614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.962360628406999\n",
      "    mean_inference_ms: 2.11981800914454\n",
      "    mean_raw_obs_processing_ms: 0.12823825001036762\n",
      "  time_since_restore: 57424.802141189575\n",
      "  time_this_iter_s: 219.27564787864685\n",
      "  time_total_s: 57424.802141189575\n",
      "  timers:\n",
      "    learn_throughput: 19.061\n",
      "    learn_time_ms: 209848.51\n",
      "    load_throughput: 6916728.232\n",
      "    load_time_ms: 0.578\n",
      "    sample_throughput: 18.238\n",
      "    sample_time_ms: 219324.683\n",
      "    update_time_ms: 13.425\n",
      "  timestamp: 1650275783\n",
      "  timesteps_since_restore: 1056000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1056000\n",
      "  training_iteration: 264\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1056000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_10-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 535.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.2\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 3022\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.46574077010154724\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010631636716425419\n",
      "          model: {}\n",
      "          policy_loss: -0.039345379918813705\n",
      "          total_loss: 0.003825099440291524\n",
      "          vf_explained_var: 0.3996434509754181\n",
      "          vf_loss: 0.04317047819495201\n",
      "    num_agent_steps_sampled: 1056000\n",
      "    num_agent_steps_trained: 1056000\n",
      "    num_steps_sampled: 1056000\n",
      "    num_steps_trained: 1056000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.08310344827586\n",
      "    ram_util_percent: 95.93862068965517\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10314069998147435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9687720470310822\n",
      "    mean_inference_ms: 2.0681682628838316\n",
      "    mean_raw_obs_processing_ms: 0.19270106885447788\n",
      "  time_since_restore: 57432.49150967598\n",
      "  time_this_iter_s: 218.95423412322998\n",
      "  time_total_s: 57432.49150967598\n",
      "  timers:\n",
      "    learn_throughput: 19.083\n",
      "    learn_time_ms: 209606.56\n",
      "    load_throughput: 6229472.746\n",
      "    load_time_ms: 0.642\n",
      "    sample_throughput: 18.232\n",
      "    sample_time_ms: 219398.28\n",
      "    update_time_ms: 9.797\n",
      "  timestamp: 1650275783\n",
      "  timesteps_since_restore: 1056000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1056000\n",
      "  training_iteration: 264\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1060000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-00-01\n",
      "  done: false\n",
      "  episode_len_mean: 535.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.2\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 3022\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.003992803860455751\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0002642733452375978\n",
      "          model: {}\n",
      "          policy_loss: -0.014184117317199707\n",
      "          total_loss: -0.014092201367020607\n",
      "          vf_explained_var: -1.647139136196074e-08\n",
      "          vf_loss: 9.191728167934343e-05\n",
      "    num_agent_steps_sampled: 1060000\n",
      "    num_agent_steps_trained: 1060000\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.59723183391003\n",
      "    ram_util_percent: 95.92871972318338\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10314069998147435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9687720470310822\n",
      "    mean_inference_ms: 2.0681682628838316\n",
      "    mean_raw_obs_processing_ms: 0.19270106885447788\n",
      "  time_since_restore: 57650.934831380844\n",
      "  time_this_iter_s: 218.4433217048645\n",
      "  time_total_s: 57650.934831380844\n",
      "  timers:\n",
      "    learn_throughput: 19.088\n",
      "    learn_time_ms: 209553.219\n",
      "    load_throughput: 5552244.101\n",
      "    load_time_ms: 0.72\n",
      "    sample_throughput: 18.256\n",
      "    sample_time_ms: 219103.885\n",
      "    update_time_ms: 10.185\n",
      "  timestamp: 1650276001\n",
      "  timesteps_since_restore: 1060000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 265\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1060000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 1921.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 549\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128231443464756\n",
      "          total_loss: -0.014125037007033825\n",
      "          vf_explained_var: -8.966333098214818e-08\n",
      "          vf_loss: 3.184256001986796e-06\n",
      "    num_agent_steps_sampled: 1060000\n",
      "    num_agent_steps_trained: 1060000\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.6955172413793\n",
      "    ram_util_percent: 95.93896551724137\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10530416426048614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.962360628406999\n",
      "    mean_inference_ms: 2.11981800914454\n",
      "    mean_raw_obs_processing_ms: 0.12823825001036762\n",
      "  time_since_restore: 57644.27979803085\n",
      "  time_this_iter_s: 219.47765684127808\n",
      "  time_total_s: 57644.27979803085\n",
      "  timers:\n",
      "    learn_throughput: 19.06\n",
      "    learn_time_ms: 209864.302\n",
      "    load_throughput: 6943349.75\n",
      "    load_time_ms: 0.576\n",
      "    sample_throughput: 18.259\n",
      "    sample_time_ms: 219072.149\n",
      "    update_time_ms: 13.991\n",
      "  timestamp: 1650276002\n",
      "  timesteps_since_restore: 1060000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 265\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1064000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 629.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.24\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3023\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.05395481735467911\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006791976280510426\n",
      "          model: {}\n",
      "          policy_loss: 0.006986001972109079\n",
      "          total_loss: 0.012006049044430256\n",
      "          vf_explained_var: 0.07845468819141388\n",
      "          vf_loss: 0.005020048003643751\n",
      "    num_agent_steps_sampled: 1064000\n",
      "    num_agent_steps_trained: 1064000\n",
      "    num_steps_sampled: 1064000\n",
      "    num_steps_trained: 1064000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.4617747440273\n",
      "    ram_util_percent: 95.91194539249146\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10312724008115913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9686719189210632\n",
      "    mean_inference_ms: 2.0677397193819633\n",
      "    mean_raw_obs_processing_ms: 0.1926634418234197\n",
      "  time_since_restore: 57870.92881035805\n",
      "  time_this_iter_s: 219.99397897720337\n",
      "  time_total_s: 57870.92881035805\n",
      "  timers:\n",
      "    learn_throughput: 19.087\n",
      "    learn_time_ms: 209562.72\n",
      "    load_throughput: 5609983.281\n",
      "    load_time_ms: 0.713\n",
      "    sample_throughput: 18.259\n",
      "    sample_time_ms: 219066.989\n",
      "    update_time_ms: 11.592\n",
      "  timestamp: 1650276222\n",
      "  timesteps_since_restore: 1064000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1064000\n",
      "  training_iteration: 266\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1064000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 1920.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 552\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.303732097735033e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.52103827999521e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.008697405457496643\n",
      "          total_loss: -0.008675099350512028\n",
      "          vf_explained_var: -4.545084811979905e-05\n",
      "          vf_loss: 2.230424252047669e-05\n",
      "    num_agent_steps_sampled: 1064000\n",
      "    num_agent_steps_trained: 1064000\n",
      "    num_steps_sampled: 1064000\n",
      "    num_steps_trained: 1064000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.51335616438357\n",
      "    ram_util_percent: 95.9154109589041\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10515744887922224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9612929222669273\n",
      "    mean_inference_ms: 2.114951284828562\n",
      "    mean_raw_obs_processing_ms: 0.12804981668545334\n",
      "  time_since_restore: 57864.32778215408\n",
      "  time_this_iter_s: 220.04798412322998\n",
      "  time_total_s: 57864.32778215408\n",
      "  timers:\n",
      "    learn_throughput: 19.056\n",
      "    learn_time_ms: 209909.356\n",
      "    load_throughput: 6985849.434\n",
      "    load_time_ms: 0.573\n",
      "    sample_throughput: 18.254\n",
      "    sample_time_ms: 219129.652\n",
      "    update_time_ms: 14.041\n",
      "  timestamp: 1650276222\n",
      "  timesteps_since_restore: 1064000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1064000\n",
      "  training_iteration: 266\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 11:05:48 (running for 16:07:08.64)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.24 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1068000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-07-12\n",
      "  done: false\n",
      "  episode_len_mean: 626.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.15\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3032\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.685598611831665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016321875154972076\n",
      "          model: {}\n",
      "          policy_loss: -0.04120072349905968\n",
      "          total_loss: 0.033058613538742065\n",
      "          vf_explained_var: 0.7380923628807068\n",
      "          vf_loss: 0.07425934076309204\n",
      "    num_agent_steps_sampled: 1068000\n",
      "    num_agent_steps_trained: 1068000\n",
      "    num_steps_sampled: 1068000\n",
      "    num_steps_trained: 1068000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.3491103202847\n",
      "    ram_util_percent: 95.6914590747331\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10300385770102868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9677678866554676\n",
      "    mean_inference_ms: 2.0638619098911506\n",
      "    mean_raw_obs_processing_ms: 0.19232605404069647\n",
      "  time_since_restore: 58081.14978647232\n",
      "  time_this_iter_s: 210.22097611427307\n",
      "  time_total_s: 58081.14978647232\n",
      "  timers:\n",
      "    learn_throughput: 19.166\n",
      "    learn_time_ms: 208701.57\n",
      "    load_throughput: 5756859.623\n",
      "    load_time_ms: 0.695\n",
      "    sample_throughput: 18.258\n",
      "    sample_time_ms: 219078.306\n",
      "    update_time_ms: 12.095\n",
      "  timestamp: 1650276432\n",
      "  timesteps_since_restore: 1068000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1068000\n",
      "  training_iteration: 267\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1068000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-07-14\n",
      "  done: false\n",
      "  episode_len_mean: 1528.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 568\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.392891118306784e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.6622868905911394e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0005831858143210411\n",
      "          total_loss: 0.0016024904325604439\n",
      "          vf_explained_var: 0.47152382135391235\n",
      "          vf_loss: 0.0010193095076829195\n",
      "    num_agent_steps_sampled: 1068000\n",
      "    num_agent_steps_trained: 1068000\n",
      "    num_steps_sampled: 1068000\n",
      "    num_steps_trained: 1068000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.03604240282685\n",
      "    ram_util_percent: 95.71908127208481\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10447127578334539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9562996858036138\n",
      "    mean_inference_ms: 2.092152541580609\n",
      "    mean_raw_obs_processing_ms: 0.12722951087345163\n",
      "  time_since_restore: 58075.51056408882\n",
      "  time_this_iter_s: 211.18278193473816\n",
      "  time_total_s: 58075.51056408882\n",
      "  timers:\n",
      "    learn_throughput: 19.127\n",
      "    learn_time_ms: 209130.729\n",
      "    load_throughput: 7133170.068\n",
      "    load_time_ms: 0.561\n",
      "    sample_throughput: 18.244\n",
      "    sample_time_ms: 219250.801\n",
      "    update_time_ms: 14.203\n",
      "  timestamp: 1650276434\n",
      "  timesteps_since_restore: 1068000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1068000\n",
      "  training_iteration: 267\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1072000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-10-45\n",
      "  done: false\n",
      "  episode_len_mean: 631.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.23\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3038\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5836979746818542\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016888948157429695\n",
      "          model: {}\n",
      "          policy_loss: -0.04930496960878372\n",
      "          total_loss: 0.02972039207816124\n",
      "          vf_explained_var: 0.7244975566864014\n",
      "          vf_loss: 0.07902536541223526\n",
      "    num_agent_steps_sampled: 1072000\n",
      "    num_agent_steps_trained: 1072000\n",
      "    num_steps_sampled: 1072000\n",
      "    num_steps_trained: 1072000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.55950704225353\n",
      "    ram_util_percent: 95.71478873239437\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1029214107164896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9671673163236867\n",
      "    mean_inference_ms: 2.0612678802616218\n",
      "    mean_raw_obs_processing_ms: 0.19209905538789102\n",
      "  time_since_restore: 58294.49457740784\n",
      "  time_this_iter_s: 213.34479093551636\n",
      "  time_total_s: 58294.49457740784\n",
      "  timers:\n",
      "    learn_throughput: 19.194\n",
      "    learn_time_ms: 208403.255\n",
      "    load_throughput: 5810693.728\n",
      "    load_time_ms: 0.688\n",
      "    sample_throughput: 18.331\n",
      "    sample_time_ms: 218215.169\n",
      "    update_time_ms: 11.601\n",
      "  timestamp: 1650276645\n",
      "  timesteps_since_restore: 1072000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1072000\n",
      "  training_iteration: 268\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1072000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-10-46\n",
      "  done: false\n",
      "  episode_len_mean: 1528.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 568\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128145761787891\n",
      "          total_loss: -0.014125547371804714\n",
      "          vf_explained_var: -9.158605962511501e-08\n",
      "          vf_loss: 2.6058460207423195e-06\n",
      "    num_agent_steps_sampled: 1072000\n",
      "    num_agent_steps_trained: 1072000\n",
      "    num_steps_sampled: 1072000\n",
      "    num_steps_trained: 1072000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.94310954063604\n",
      "    ram_util_percent: 95.70918727915196\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10447127578334539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9562996858036138\n",
      "    mean_inference_ms: 2.092152541580609\n",
      "    mean_raw_obs_processing_ms: 0.12722951087345163\n",
      "  time_since_restore: 58288.38095712662\n",
      "  time_this_iter_s: 212.87039303779602\n",
      "  time_total_s: 58288.38095712662\n",
      "  timers:\n",
      "    learn_throughput: 19.17\n",
      "    learn_time_ms: 208658.944\n",
      "    load_throughput: 7126504.12\n",
      "    load_time_ms: 0.561\n",
      "    sample_throughput: 18.311\n",
      "    sample_time_ms: 218453.375\n",
      "    update_time_ms: 14.175\n",
      "  timestamp: 1650276646\n",
      "  timesteps_since_restore: 1072000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1072000\n",
      "  training_iteration: 268\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1076000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-14-19\n",
      "  done: false\n",
      "  episode_len_mean: 636.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.35\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3046\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6109945774078369\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015588922426104546\n",
      "          model: {}\n",
      "          policy_loss: -0.04582538455724716\n",
      "          total_loss: 0.03012372925877571\n",
      "          vf_explained_var: 0.7735936045646667\n",
      "          vf_loss: 0.07594911009073257\n",
      "    num_agent_steps_sampled: 1076000\n",
      "    num_agent_steps_trained: 1076000\n",
      "    num_steps_sampled: 1076000\n",
      "    num_steps_trained: 1076000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.08098591549296\n",
      "    ram_util_percent: 95.7169014084507\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10281144539503792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.966370144854464\n",
      "    mean_inference_ms: 2.0578430385517743\n",
      "    mean_raw_obs_processing_ms: 0.19179478471883407\n",
      "  time_since_restore: 58508.114817619324\n",
      "  time_this_iter_s: 213.62024021148682\n",
      "  time_total_s: 58508.114817619324\n",
      "  timers:\n",
      "    learn_throughput: 19.241\n",
      "    learn_time_ms: 207887.906\n",
      "    load_throughput: 6060695.036\n",
      "    load_time_ms: 0.66\n",
      "    sample_throughput: 18.354\n",
      "    sample_time_ms: 217937.24\n",
      "    update_time_ms: 15.046\n",
      "  timestamp: 1650276859\n",
      "  timesteps_since_restore: 1076000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1076000\n",
      "  training_iteration: 269\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1076000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-14-20\n",
      "  done: false\n",
      "  episode_len_mean: 1528.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 568\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128075912594795\n",
      "          total_loss: -0.01412744540721178\n",
      "          vf_explained_var: 3.082777766394429e-08\n",
      "          vf_loss: 6.313412086456083e-07\n",
      "    num_agent_steps_sampled: 1076000\n",
      "    num_agent_steps_trained: 1076000\n",
      "    num_steps_sampled: 1076000\n",
      "    num_steps_trained: 1076000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.04558303886927\n",
      "    ram_util_percent: 95.70070671378092\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10447127578334539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9562996858036138\n",
      "    mean_inference_ms: 2.092152541580609\n",
      "    mean_raw_obs_processing_ms: 0.12722951087345163\n",
      "  time_since_restore: 58501.3692510128\n",
      "  time_this_iter_s: 212.9882938861847\n",
      "  time_total_s: 58501.3692510128\n",
      "  timers:\n",
      "    learn_throughput: 19.215\n",
      "    learn_time_ms: 208166.383\n",
      "    load_throughput: 7239359.655\n",
      "    load_time_ms: 0.553\n",
      "    sample_throughput: 18.35\n",
      "    sample_time_ms: 217985.427\n",
      "    update_time_ms: 13.646\n",
      "  timestamp: 1650276860\n",
      "  timesteps_since_restore: 1076000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1076000\n",
      "  training_iteration: 269\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 636.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.38\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3054\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6692379117012024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014874055981636047\n",
      "          model: {}\n",
      "          policy_loss: -0.043358102440834045\n",
      "          total_loss: 0.0655023530125618\n",
      "          vf_explained_var: 0.6793375611305237\n",
      "          vf_loss: 0.10886044800281525\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_agent_steps_trained: 1080000\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.27907801418439\n",
      "    ram_util_percent: 95.71702127659576\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1027001431586801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9655725978773029\n",
      "    mean_inference_ms: 2.0544214512066743\n",
      "    mean_raw_obs_processing_ms: 0.19148957371501865\n",
      "  time_since_restore: 58720.34453654289\n",
      "  time_this_iter_s: 212.22971892356873\n",
      "  time_total_s: 58720.34453654289\n",
      "  timers:\n",
      "    learn_throughput: 19.322\n",
      "    learn_time_ms: 207022.699\n",
      "    load_throughput: 6048459.154\n",
      "    load_time_ms: 0.661\n",
      "    sample_throughput: 18.396\n",
      "    sample_time_ms: 217444.069\n",
      "    update_time_ms: 19.735\n",
      "  timestamp: 1650277071\n",
      "  timesteps_since_restore: 1080000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 270\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-17-52\n",
      "  done: false\n",
      "  episode_len_mean: 1529.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 573\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4618668615361039e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.1355240854100378e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.00044754109694622457\n",
      "          total_loss: 0.0005444409907795489\n",
      "          vf_explained_var: -0.12386412918567657\n",
      "          vf_loss: 9.689520811662078e-05\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_agent_steps_trained: 1080000\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.33723404255319\n",
      "    ram_util_percent: 95.69893617021276\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10428729277097966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9549712531411529\n",
      "    mean_inference_ms: 2.086054127612848\n",
      "    mean_raw_obs_processing_ms: 0.12702171148861532\n",
      "  time_since_restore: 58713.55033826828\n",
      "  time_this_iter_s: 212.1810872554779\n",
      "  time_total_s: 58713.55033826828\n",
      "  timers:\n",
      "    learn_throughput: 19.296\n",
      "    learn_time_ms: 207299.444\n",
      "    load_throughput: 7191262.752\n",
      "    load_time_ms: 0.556\n",
      "    sample_throughput: 18.389\n",
      "    sample_time_ms: 217518.741\n",
      "    update_time_ms: 13.7\n",
      "  timestamp: 1650277072\n",
      "  timesteps_since_restore: 1080000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 270\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1084000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-21-23\n",
      "  done: false\n",
      "  episode_len_mean: 636.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.31\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3062\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6079123616218567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016811717301607132\n",
      "          model: {}\n",
      "          policy_loss: -0.038799215108156204\n",
      "          total_loss: 0.05635788291692734\n",
      "          vf_explained_var: 0.635780930519104\n",
      "          vf_loss: 0.09515708684921265\n",
      "    num_agent_steps_sampled: 1084000\n",
      "    num_agent_steps_trained: 1084000\n",
      "    num_steps_sampled: 1084000\n",
      "    num_steps_trained: 1084000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.84607142857142\n",
      "    ram_util_percent: 95.89285714285714\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10258893975636642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.964780906619676\n",
      "    mean_inference_ms: 2.0509975999090333\n",
      "    mean_raw_obs_processing_ms: 0.19118875257269324\n",
      "  time_since_restore: 58931.54777264595\n",
      "  time_this_iter_s: 211.20323610305786\n",
      "  time_total_s: 58931.54777264595\n",
      "  timers:\n",
      "    learn_throughput: 19.395\n",
      "    learn_time_ms: 206240.662\n",
      "    load_throughput: 5442377.137\n",
      "    load_time_ms: 0.735\n",
      "    sample_throughput: 18.47\n",
      "    sample_time_ms: 216571.127\n",
      "    update_time_ms: 20.311\n",
      "  timestamp: 1650277283\n",
      "  timesteps_since_restore: 1084000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1084000\n",
      "  training_iteration: 271\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1084000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-21-23\n",
      "  done: false\n",
      "  episode_len_mean: 1529.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 573\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128154143691063\n",
      "          total_loss: 0.014128320850431919\n",
      "          vf_explained_var: -1.0055880039772092e-07\n",
      "          vf_loss: 1.6698565730166592e-07\n",
      "    num_agent_steps_sampled: 1084000\n",
      "    num_agent_steps_trained: 1084000\n",
      "    num_steps_sampled: 1084000\n",
      "    num_steps_trained: 1084000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.64874551971326\n",
      "    ram_util_percent: 95.88064516129032\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10428729277097966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9549712531411529\n",
      "    mean_inference_ms: 2.086054127612848\n",
      "    mean_raw_obs_processing_ms: 0.12702171148861532\n",
      "  time_since_restore: 58924.588392972946\n",
      "  time_this_iter_s: 211.03805470466614\n",
      "  time_total_s: 58924.588392972946\n",
      "  timers:\n",
      "    learn_throughput: 19.367\n",
      "    learn_time_ms: 206539.255\n",
      "    load_throughput: 7084374.631\n",
      "    load_time_ms: 0.565\n",
      "    sample_throughput: 18.466\n",
      "    sample_time_ms: 216612.852\n",
      "    update_time_ms: 11.962\n",
      "  timestamp: 1650277283\n",
      "  timesteps_since_restore: 1084000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1084000\n",
      "  training_iteration: 271\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 11:22:29 (running for 16:23:48.86)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.31 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1088000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-24-52\n",
      "  done: false\n",
      "  episode_len_mean: 634.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.34\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3070\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6760424375534058\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015765661373734474\n",
      "          model: {}\n",
      "          policy_loss: -0.04513508826494217\n",
      "          total_loss: 0.035276930779218674\n",
      "          vf_explained_var: 0.7152586579322815\n",
      "          vf_loss: 0.08041202276945114\n",
      "    num_agent_steps_sampled: 1088000\n",
      "    num_agent_steps_trained: 1088000\n",
      "    num_steps_sampled: 1088000\n",
      "    num_steps_trained: 1088000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.30649819494586\n",
      "    ram_util_percent: 95.91732851985559\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1024761579715803\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9639895608654575\n",
      "    mean_inference_ms: 2.0475790211349185\n",
      "    mean_raw_obs_processing_ms: 0.19088874570558814\n",
      "  time_since_restore: 59141.183106422424\n",
      "  time_this_iter_s: 209.635333776474\n",
      "  time_total_s: 59141.183106422424\n",
      "  timers:\n",
      "    learn_throughput: 19.484\n",
      "    learn_time_ms: 205292.629\n",
      "    load_throughput: 5290494.45\n",
      "    load_time_ms: 0.756\n",
      "    sample_throughput: 18.541\n",
      "    sample_time_ms: 215741.216\n",
      "    update_time_ms: 19.773\n",
      "  timestamp: 1650277492\n",
      "  timesteps_since_restore: 1088000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1088000\n",
      "  training_iteration: 272\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1088000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-24-54\n",
      "  done: false\n",
      "  episode_len_mean: 1529.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 573\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128055423498154\n",
      "          total_loss: 0.014128067530691624\n",
      "          vf_explained_var: -4.240902455876494e-07\n",
      "          vf_loss: 2.3878081734096668e-08\n",
      "    num_agent_steps_sampled: 1088000\n",
      "    num_agent_steps_trained: 1088000\n",
      "    num_steps_sampled: 1088000\n",
      "    num_steps_trained: 1088000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.32401433691756\n",
      "    ram_util_percent: 95.93297491039426\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10428729277097966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9549712531411529\n",
      "    mean_inference_ms: 2.086054127612848\n",
      "    mean_raw_obs_processing_ms: 0.12702171148861532\n",
      "  time_since_restore: 59135.52359080315\n",
      "  time_this_iter_s: 210.9351978302002\n",
      "  time_total_s: 59135.52359080315\n",
      "  timers:\n",
      "    learn_throughput: 19.443\n",
      "    learn_time_ms: 205728.741\n",
      "    load_throughput: 7111099.055\n",
      "    load_time_ms: 0.563\n",
      "    sample_throughput: 18.535\n",
      "    sample_time_ms: 215804.633\n",
      "    update_time_ms: 11.274\n",
      "  timestamp: 1650277494\n",
      "  timesteps_since_restore: 1088000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1088000\n",
      "  training_iteration: 272\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1092000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 638.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.47\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3077\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5877355933189392\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01568487472832203\n",
      "          model: {}\n",
      "          policy_loss: -0.04220800846815109\n",
      "          total_loss: 0.05394202098250389\n",
      "          vf_explained_var: 0.7520977258682251\n",
      "          vf_loss: 0.09615002572536469\n",
      "    num_agent_steps_sampled: 1092000\n",
      "    num_agent_steps_trained: 1092000\n",
      "    num_steps_sampled: 1092000\n",
      "    num_steps_trained: 1092000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.2784722222222\n",
      "    ram_util_percent: 95.97569444444444\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10237668145769352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9632993076128363\n",
      "    mean_inference_ms: 2.0445925278900763\n",
      "    mean_raw_obs_processing_ms: 0.1906264192986348\n",
      "  time_since_restore: 59358.15631866455\n",
      "  time_this_iter_s: 216.97321224212646\n",
      "  time_total_s: 59358.15631866455\n",
      "  timers:\n",
      "    learn_throughput: 19.505\n",
      "    learn_time_ms: 205072.807\n",
      "    load_throughput: 5525910.214\n",
      "    load_time_ms: 0.724\n",
      "    sample_throughput: 18.626\n",
      "    sample_time_ms: 214755.667\n",
      "    update_time_ms: 20.668\n",
      "  timestamp: 1650277709\n",
      "  timesteps_since_restore: 1092000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1092000\n",
      "  training_iteration: 273\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1092000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-28-33\n",
      "  done: false\n",
      "  episode_len_mean: 1529.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 582\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0292406736723014e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3589511910131998e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.00046725771971978247\n",
      "          total_loss: 0.0006171721033751965\n",
      "          vf_explained_var: 0.27680736780166626\n",
      "          vf_loss: 0.00014990851923357695\n",
      "    num_agent_steps_sampled: 1092000\n",
      "    num_agent_steps_trained: 1092000\n",
      "    num_steps_sampled: 1092000\n",
      "    num_steps_trained: 1092000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.93413793103448\n",
      "    ram_util_percent: 96.00206896551724\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10396983151847497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9527056208328054\n",
      "    mean_inference_ms: 2.075632406126897\n",
      "    mean_raw_obs_processing_ms: 0.12667181525366994\n",
      "  time_since_restore: 59354.5501806736\n",
      "  time_this_iter_s: 219.02658987045288\n",
      "  time_total_s: 59354.5501806736\n",
      "  timers:\n",
      "    learn_throughput: 19.448\n",
      "    learn_time_ms: 205679.424\n",
      "    load_throughput: 7117434.244\n",
      "    load_time_ms: 0.562\n",
      "    sample_throughput: 18.603\n",
      "    sample_time_ms: 215019.805\n",
      "    update_time_ms: 9.483\n",
      "  timestamp: 1650277713\n",
      "  timesteps_since_restore: 1092000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1092000\n",
      "  training_iteration: 273\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1096000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-32-10\n",
      "  done: false\n",
      "  episode_len_mean: 638.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.42\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3085\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6569031476974487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013484552502632141\n",
      "          model: {}\n",
      "          policy_loss: -0.04290487989783287\n",
      "          total_loss: 0.030559992417693138\n",
      "          vf_explained_var: 0.7538016438484192\n",
      "          vf_loss: 0.07346488535404205\n",
      "    num_agent_steps_sampled: 1096000\n",
      "    num_agent_steps_trained: 1096000\n",
      "    num_steps_sampled: 1096000\n",
      "    num_steps_trained: 1096000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.53174061433445\n",
      "    ram_util_percent: 95.71501706484642\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10227339270134746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9625535121429587\n",
      "    mean_inference_ms: 2.041353214542463\n",
      "    mean_raw_obs_processing_ms: 0.19034056915254957\n",
      "  time_since_restore: 59578.12750291824\n",
      "  time_this_iter_s: 219.97118425369263\n",
      "  time_total_s: 59578.12750291824\n",
      "  timers:\n",
      "    learn_throughput: 19.524\n",
      "    learn_time_ms: 204876.414\n",
      "    load_throughput: 5542705.739\n",
      "    load_time_ms: 0.722\n",
      "    sample_throughput: 18.617\n",
      "    sample_time_ms: 214858.377\n",
      "    update_time_ms: 20.996\n",
      "  timestamp: 1650277930\n",
      "  timesteps_since_restore: 1096000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1096000\n",
      "  training_iteration: 274\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1096000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-32-12\n",
      "  done: false\n",
      "  episode_len_mean: 1529.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 582\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 5.738105380370897e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.428093780231864e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014127951115369797\n",
      "          total_loss: -0.014127831906080246\n",
      "          vf_explained_var: 4.210779636082407e-08\n",
      "          vf_loss: 1.262226021481183e-07\n",
      "    num_agent_steps_sampled: 1096000\n",
      "    num_agent_steps_trained: 1096000\n",
      "    num_steps_sampled: 1096000\n",
      "    num_steps_trained: 1096000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.54879725085911\n",
      "    ram_util_percent: 95.70309278350516\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10396983151847497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9527056208328054\n",
      "    mean_inference_ms: 2.075632406126897\n",
      "    mean_raw_obs_processing_ms: 0.12667181525366994\n",
      "  time_since_restore: 59573.298011779785\n",
      "  time_this_iter_s: 218.7478311061859\n",
      "  time_total_s: 59573.298011779785\n",
      "  timers:\n",
      "    learn_throughput: 19.465\n",
      "    learn_time_ms: 205492.138\n",
      "    load_throughput: 6825833.435\n",
      "    load_time_ms: 0.586\n",
      "    sample_throughput: 18.596\n",
      "    sample_time_ms: 215094.856\n",
      "    update_time_ms: 9.3\n",
      "  timestamp: 1650277932\n",
      "  timesteps_since_restore: 1096000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1096000\n",
      "  training_iteration: 274\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-35-47\n",
      "  done: false\n",
      "  episode_len_mean: 647.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.6\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3092\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6193677186965942\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013640680350363255\n",
      "          model: {}\n",
      "          policy_loss: -0.048136863857507706\n",
      "          total_loss: 0.028399070724844933\n",
      "          vf_explained_var: 0.743440568447113\n",
      "          vf_loss: 0.07653594017028809\n",
      "    num_agent_steps_sampled: 1100000\n",
      "    num_agent_steps_trained: 1100000\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.64083044982699\n",
      "    ram_util_percent: 95.60034602076125\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1021832621872033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9619089336662632\n",
      "    mean_inference_ms: 2.038544315144277\n",
      "    mean_raw_obs_processing_ms: 0.19009020367098312\n",
      "  time_since_restore: 59795.28008890152\n",
      "  time_this_iter_s: 217.15258598327637\n",
      "  time_total_s: 59795.28008890152\n",
      "  timers:\n",
      "    learn_throughput: 19.549\n",
      "    learn_time_ms: 204611.023\n",
      "    load_throughput: 6223002.967\n",
      "    load_time_ms: 0.643\n",
      "    sample_throughput: 18.622\n",
      "    sample_time_ms: 214799.367\n",
      "    update_time_ms: 21.197\n",
      "  timestamp: 1650278147\n",
      "  timesteps_since_restore: 1100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 275\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-35-49\n",
      "  done: false\n",
      "  episode_len_mean: 1529.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 582\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 5.738105380370897e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.428093780231864e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.010781511664390564\n",
      "          total_loss: -0.010781507939100266\n",
      "          vf_explained_var: 1.390775050680304e-08\n",
      "          vf_loss: 9.257355770841968e-09\n",
      "    num_agent_steps_sampled: 1100000\n",
      "    num_agent_steps_trained: 1100000\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.8003448275862\n",
      "    ram_util_percent: 95.58482758620688\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10396983151847497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9527056208328054\n",
      "    mean_inference_ms: 2.075632406126897\n",
      "    mean_raw_obs_processing_ms: 0.12667181525366994\n",
      "  time_since_restore: 59790.24622154236\n",
      "  time_this_iter_s: 216.94820976257324\n",
      "  time_total_s: 59790.24622154236\n",
      "  timers:\n",
      "    learn_throughput: 19.495\n",
      "    learn_time_ms: 205182.525\n",
      "    load_throughput: 6780864.926\n",
      "    load_time_ms: 0.59\n",
      "    sample_throughput: 18.608\n",
      "    sample_time_ms: 214958.584\n",
      "    update_time_ms: 8.954\n",
      "  timestamp: 1650278149\n",
      "  timesteps_since_restore: 1100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 275\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 11:39:10 (running for 16:40:29.69)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.6 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-39-22\n",
      "  done: false\n",
      "  episode_len_mean: 648.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.64\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3098\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5365813970565796\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01755460910499096\n",
      "          model: {}\n",
      "          policy_loss: -0.04354388639330864\n",
      "          total_loss: 0.05196850001811981\n",
      "          vf_explained_var: 0.6896324157714844\n",
      "          vf_loss: 0.09551239013671875\n",
      "    num_agent_steps_sampled: 1104000\n",
      "    num_agent_steps_trained: 1104000\n",
      "    num_steps_sampled: 1104000\n",
      "    num_steps_trained: 1104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.86864111498258\n",
      "    ram_util_percent: 95.67735191637631\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10210603927371946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9613598555258936\n",
      "    mean_inference_ms: 2.0361328272164174\n",
      "    mean_raw_obs_processing_ms: 0.18987460041711046\n",
      "  time_since_restore: 60010.6659219265\n",
      "  time_this_iter_s: 215.38583302497864\n",
      "  time_total_s: 60010.6659219265\n",
      "  timers:\n",
      "    learn_throughput: 19.597\n",
      "    learn_time_ms: 204115.855\n",
      "    load_throughput: 6254787.309\n",
      "    load_time_ms: 0.64\n",
      "    sample_throughput: 18.642\n",
      "    sample_time_ms: 214570.65\n",
      "    update_time_ms: 23.117\n",
      "  timestamp: 1650278362\n",
      "  timesteps_since_restore: 1104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1104000\n",
      "  training_iteration: 276\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-39-25\n",
      "  done: false\n",
      "  episode_len_mean: 1439.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.51\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 594\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.971058732708004e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.1605372241044654e-28\n",
      "          model: {}\n",
      "          policy_loss: 0.004119148477911949\n",
      "          total_loss: 0.005462003406137228\n",
      "          vf_explained_var: 0.816053032875061\n",
      "          vf_loss: 0.001342854811809957\n",
      "    num_agent_steps_sampled: 1104000\n",
      "    num_agent_steps_trained: 1104000\n",
      "    num_steps_sampled: 1104000\n",
      "    num_steps_trained: 1104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.27839721254355\n",
      "    ram_util_percent: 95.6665505226481\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10356235261639454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9498064017523571\n",
      "    mean_inference_ms: 2.062218825806377\n",
      "    mean_raw_obs_processing_ms: 0.12623152579540758\n",
      "  time_since_restore: 60006.392523527145\n",
      "  time_this_iter_s: 216.146301984787\n",
      "  time_total_s: 60006.392523527145\n",
      "  timers:\n",
      "    learn_throughput: 19.538\n",
      "    learn_time_ms: 204725.319\n",
      "    load_throughput: 6627119.608\n",
      "    load_time_ms: 0.604\n",
      "    sample_throughput: 18.629\n",
      "    sample_time_ms: 214715.605\n",
      "    update_time_ms: 9.093\n",
      "  timestamp: 1650278365\n",
      "  timesteps_since_restore: 1104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1104000\n",
      "  training_iteration: 276\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-43-01\n",
      "  done: false\n",
      "  episode_len_mean: 648.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.77\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3106\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6764313578605652\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016939332708716393\n",
      "          model: {}\n",
      "          policy_loss: -0.043308962136507034\n",
      "          total_loss: 0.05002786964178085\n",
      "          vf_explained_var: 0.707159698009491\n",
      "          vf_loss: 0.09333682805299759\n",
      "    num_agent_steps_sampled: 1108000\n",
      "    num_agent_steps_trained: 1108000\n",
      "    num_steps_sampled: 1108000\n",
      "    num_steps_trained: 1108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.02172413793105\n",
      "    ram_util_percent: 95.77\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10200414048862143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9606529869350078\n",
      "    mean_inference_ms: 2.032995691138761\n",
      "    mean_raw_obs_processing_ms: 0.18959188631702203\n",
      "  time_since_restore: 60229.13133597374\n",
      "  time_this_iter_s: 218.4654140472412\n",
      "  time_total_s: 60229.13133597374\n",
      "  timers:\n",
      "    learn_throughput: 19.525\n",
      "    learn_time_ms: 204866.757\n",
      "    load_throughput: 6241059.445\n",
      "    load_time_ms: 0.641\n",
      "    sample_throughput: 18.679\n",
      "    sample_time_ms: 214145.15\n",
      "    update_time_ms: 25.812\n",
      "  timestamp: 1650278581\n",
      "  timesteps_since_restore: 1108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1108000\n",
      "  training_iteration: 277\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 1243.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.51\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 610\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0169850235450479e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -6.055909043088293e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.00031410198425874114\n",
      "          total_loss: 0.000168564758496359\n",
      "          vf_explained_var: 0.45102229714393616\n",
      "          vf_loss: 0.0004826619115192443\n",
      "    num_agent_steps_sampled: 1108000\n",
      "    num_agent_steps_trained: 1108000\n",
      "    num_steps_sampled: 1108000\n",
      "    num_steps_trained: 1108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.99041095890412\n",
      "    ram_util_percent: 95.74417808219178\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10308922086425323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9464554784339344\n",
      "    mean_inference_ms: 2.046646699112922\n",
      "    mean_raw_obs_processing_ms: 0.125766168384725\n",
      "  time_since_restore: 60225.800686359406\n",
      "  time_this_iter_s: 219.40816283226013\n",
      "  time_total_s: 60225.800686359406\n",
      "  timers:\n",
      "    learn_throughput: 19.468\n",
      "    learn_time_ms: 205460.312\n",
      "    load_throughput: 6611450.189\n",
      "    load_time_ms: 0.605\n",
      "    sample_throughput: 18.661\n",
      "    sample_time_ms: 214349.06\n",
      "    update_time_ms: 9.948\n",
      "  timestamp: 1650278584\n",
      "  timesteps_since_restore: 1108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1108000\n",
      "  training_iteration: 277\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-46-43\n",
      "  done: false\n",
      "  episode_len_mean: 648.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.84\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3114\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6953081488609314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012816308997571468\n",
      "          model: {}\n",
      "          policy_loss: -0.041440632194280624\n",
      "          total_loss: 0.04701238498091698\n",
      "          vf_explained_var: 0.6747155785560608\n",
      "          vf_loss: 0.0884530171751976\n",
      "    num_agent_steps_sampled: 1112000\n",
      "    num_agent_steps_trained: 1112000\n",
      "    num_steps_sampled: 1112000\n",
      "    num_steps_trained: 1112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.19127516778524\n",
      "    ram_util_percent: 95.75838926174497\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10190345631471898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9599660883661626\n",
      "    mean_inference_ms: 2.029944907210452\n",
      "    mean_raw_obs_processing_ms: 0.18931323009026066\n",
      "  time_since_restore: 60451.34858131409\n",
      "  time_this_iter_s: 222.2172453403473\n",
      "  time_total_s: 60451.34858131409\n",
      "  timers:\n",
      "    learn_throughput: 19.453\n",
      "    learn_time_ms: 205624.612\n",
      "    load_throughput: 5929392.472\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 18.603\n",
      "    sample_time_ms: 215014.531\n",
      "    update_time_ms: 29.403\n",
      "  timestamp: 1650278803\n",
      "  timesteps_since_restore: 1112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1112000\n",
      "  training_iteration: 278\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-46-46\n",
      "  done: false\n",
      "  episode_len_mean: 1243.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.51\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 610\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.329142083911374e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2430768727823652e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.002619785023853183\n",
      "          total_loss: 0.00261978548951447\n",
      "          vf_explained_var: 2.454429522913415e-05\n",
      "          vf_loss: 5.405342284880987e-10\n",
      "    num_agent_steps_sampled: 1112000\n",
      "    num_agent_steps_trained: 1112000\n",
      "    num_steps_sampled: 1112000\n",
      "    num_steps_trained: 1112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.48918918918919\n",
      "    ram_util_percent: 95.77804054054054\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10308922086425323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9464554784339344\n",
      "    mean_inference_ms: 2.046646699112922\n",
      "    mean_raw_obs_processing_ms: 0.125766168384725\n",
      "  time_since_restore: 60447.43800544739\n",
      "  time_this_iter_s: 221.63731908798218\n",
      "  time_total_s: 60447.43800544739\n",
      "  timers:\n",
      "    learn_throughput: 19.397\n",
      "    learn_time_ms: 206212.973\n",
      "    load_throughput: 6481944.133\n",
      "    load_time_ms: 0.617\n",
      "    sample_throughput: 18.587\n",
      "    sample_time_ms: 215199.339\n",
      "    update_time_ms: 10.003\n",
      "  timestamp: 1650278806\n",
      "  timesteps_since_restore: 1112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1112000\n",
      "  training_iteration: 278\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-50-31\n",
      "  done: false\n",
      "  episode_len_mean: 650.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.9\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3121\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6463832855224609\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01500083040446043\n",
      "          model: {}\n",
      "          policy_loss: -0.036809638142585754\n",
      "          total_loss: 0.05155416205525398\n",
      "          vf_explained_var: 0.737865686416626\n",
      "          vf_loss: 0.08836380392313004\n",
      "    num_agent_steps_sampled: 1116000\n",
      "    num_agent_steps_trained: 1116000\n",
      "    num_steps_sampled: 1116000\n",
      "    num_steps_trained: 1116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.93729372937294\n",
      "    ram_util_percent: 95.91914191419141\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10181759211882763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9593832429705982\n",
      "    mean_inference_ms: 2.027354521573473\n",
      "    mean_raw_obs_processing_ms: 0.1890756462823924\n",
      "  time_since_restore: 60679.11108016968\n",
      "  time_this_iter_s: 227.76249885559082\n",
      "  time_total_s: 60679.11108016968\n",
      "  timers:\n",
      "    learn_throughput: 19.333\n",
      "    learn_time_ms: 206897.222\n",
      "    load_throughput: 5932747.268\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 18.525\n",
      "    sample_time_ms: 215919.278\n",
      "    update_time_ms: 27.642\n",
      "  timestamp: 1650279031\n",
      "  timesteps_since_restore: 1116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1116000\n",
      "  training_iteration: 279\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 1243.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.51\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 610\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.329142083911374e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2430768727823652e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0013428243109956384\n",
      "          total_loss: 0.0013428243109956384\n",
      "          vf_explained_var: -6.14150267210789e-05\n",
      "          vf_loss: 1.3735726445140983e-10\n",
      "    num_agent_steps_sampled: 1116000\n",
      "    num_agent_steps_trained: 1116000\n",
      "    num_steps_sampled: 1116000\n",
      "    num_steps_trained: 1116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.54455445544555\n",
      "    ram_util_percent: 95.91551155115512\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10308922086425323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9464554784339344\n",
      "    mean_inference_ms: 2.046646699112922\n",
      "    mean_raw_obs_processing_ms: 0.125766168384725\n",
      "  time_since_restore: 60675.495542526245\n",
      "  time_this_iter_s: 228.05753707885742\n",
      "  time_total_s: 60675.495542526245\n",
      "  timers:\n",
      "    learn_throughput: 19.272\n",
      "    learn_time_ms: 207559.362\n",
      "    load_throughput: 6417233.782\n",
      "    load_time_ms: 0.623\n",
      "    sample_throughput: 18.51\n",
      "    sample_time_ms: 216104.109\n",
      "    update_time_ms: 10.189\n",
      "  timestamp: 1650279034\n",
      "  timesteps_since_restore: 1116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1116000\n",
      "  training_iteration: 279\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-54-11\n",
      "  done: false\n",
      "  episode_len_mean: 558.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.95\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3130\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7433709502220154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012845125049352646\n",
      "          model: {}\n",
      "          policy_loss: -0.041780024766922\n",
      "          total_loss: 0.03841639682650566\n",
      "          vf_explained_var: 0.7424854636192322\n",
      "          vf_loss: 0.08019642531871796\n",
      "    num_agent_steps_sampled: 1120000\n",
      "    num_agent_steps_trained: 1120000\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.30547945205481\n",
      "    ram_util_percent: 95.78664383561645\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10172757126987059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9587817274667328\n",
      "    mean_inference_ms: 2.0246474786166986\n",
      "    mean_raw_obs_processing_ms: 0.1888436556782101\n",
      "  time_since_restore: 60898.9905731678\n",
      "  time_this_iter_s: 219.87949299812317\n",
      "  time_total_s: 60898.9905731678\n",
      "  timers:\n",
      "    learn_throughput: 19.272\n",
      "    learn_time_ms: 207553.707\n",
      "    load_throughput: 5857352.931\n",
      "    load_time_ms: 0.683\n",
      "    sample_throughput: 18.408\n",
      "    sample_time_ms: 217298.891\n",
      "    update_time_ms: 23.657\n",
      "  timestamp: 1650279251\n",
      "  timesteps_since_restore: 1120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 280\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-54-13\n",
      "  done: false\n",
      "  episode_len_mean: 1340.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.5\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 621\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.931679765671651e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.3029409656749437e-28\n",
      "          model: {}\n",
      "          policy_loss: 0.0015488683711737394\n",
      "          total_loss: 0.0017541947308927774\n",
      "          vf_explained_var: 0.19312068819999695\n",
      "          vf_loss: 0.00020532803318928927\n",
      "    num_agent_steps_sampled: 1120000\n",
      "    num_agent_steps_trained: 1120000\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.86632302405498\n",
      "    ram_util_percent: 95.77285223367699\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10279430158015296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.944354511232466\n",
      "    mean_inference_ms: 2.0368710875957534\n",
      "    mean_raw_obs_processing_ms: 0.12548412865530695\n",
      "  time_since_restore: 60894.3440284729\n",
      "  time_this_iter_s: 218.84848594665527\n",
      "  time_total_s: 60894.3440284729\n",
      "  timers:\n",
      "    learn_throughput: 19.226\n",
      "    learn_time_ms: 208051.003\n",
      "    load_throughput: 6253854.699\n",
      "    load_time_ms: 0.64\n",
      "    sample_throughput: 18.38\n",
      "    sample_time_ms: 217626.509\n",
      "    update_time_ms: 9.206\n",
      "  timestamp: 1650279253\n",
      "  timesteps_since_restore: 1120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 280\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 11:55:50 (running for 16:57:10.22)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.95 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-58-03\n",
      "  done: false\n",
      "  episode_len_mean: 556.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.89\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3137\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7095562815666199\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01449011079967022\n",
      "          model: {}\n",
      "          policy_loss: -0.05072220787405968\n",
      "          total_loss: 0.05551674962043762\n",
      "          vf_explained_var: 0.6571369171142578\n",
      "          vf_loss: 0.1062389686703682\n",
      "    num_agent_steps_sampled: 1124000\n",
      "    num_agent_steps_trained: 1124000\n",
      "    num_steps_sampled: 1124000\n",
      "    num_steps_trained: 1124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.77605177993527\n",
      "    ram_util_percent: 96.01682847896438\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10165982339505203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9583305081521933\n",
      "    mean_inference_ms: 2.0226172058269123\n",
      "    mean_raw_obs_processing_ms: 0.18867305682176805\n",
      "  time_since_restore: 61131.43263030052\n",
      "  time_this_iter_s: 232.44205713272095\n",
      "  time_total_s: 61131.43263030052\n",
      "  timers:\n",
      "    learn_throughput: 19.088\n",
      "    learn_time_ms: 209560.563\n",
      "    load_throughput: 6576463.486\n",
      "    load_time_ms: 0.608\n",
      "    sample_throughput: 18.344\n",
      "    sample_time_ms: 218056.02\n",
      "    update_time_ms: 26.533\n",
      "  timestamp: 1650279483\n",
      "  timesteps_since_restore: 1124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1124000\n",
      "  training_iteration: 281\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_11-58-05\n",
      "  done: false\n",
      "  episode_len_mean: 1152.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.65\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 627\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0964410851300507e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.437098879994623e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0008186226477846503\n",
      "          total_loss: -8.910714677767828e-05\n",
      "          vf_explained_var: 0.286770224571228\n",
      "          vf_loss: 0.0007295148097909987\n",
      "    num_agent_steps_sampled: 1124000\n",
      "    num_agent_steps_trained: 1124000\n",
      "    num_steps_sampled: 1124000\n",
      "    num_steps_trained: 1124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.00906148867314\n",
      "    ram_util_percent: 96.03883495145631\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1026485720410857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9433200394871887\n",
      "    mean_inference_ms: 2.032039307781805\n",
      "    mean_raw_obs_processing_ms: 0.12535518495634376\n",
      "  time_since_restore: 61126.60442829132\n",
      "  time_this_iter_s: 232.2603998184204\n",
      "  time_total_s: 61126.60442829132\n",
      "  timers:\n",
      "    learn_throughput: 19.048\n",
      "    learn_time_ms: 209993.611\n",
      "    load_throughput: 6189940.968\n",
      "    load_time_ms: 0.646\n",
      "    sample_throughput: 18.324\n",
      "    sample_time_ms: 218296.175\n",
      "    update_time_ms: 9.829\n",
      "  timestamp: 1650279485\n",
      "  timesteps_since_restore: 1124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1124000\n",
      "  training_iteration: 281\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-01-46\n",
      "  done: false\n",
      "  episode_len_mean: 550.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.77\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3145\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6588556170463562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014422440901398659\n",
      "          model: {}\n",
      "          policy_loss: -0.042489226907491684\n",
      "          total_loss: 0.03316160663962364\n",
      "          vf_explained_var: 0.748646080493927\n",
      "          vf_loss: 0.07565082609653473\n",
      "    num_agent_steps_sampled: 1128000\n",
      "    num_agent_steps_trained: 1128000\n",
      "    num_steps_sampled: 1128000\n",
      "    num_steps_trained: 1128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.61898305084745\n",
      "    ram_util_percent: 96.00203389830509\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10158515304422328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9578360475400851\n",
      "    mean_inference_ms: 2.0203444926722356\n",
      "    mean_raw_obs_processing_ms: 0.18848576188730098\n",
      "  time_since_restore: 61353.75891828537\n",
      "  time_this_iter_s: 222.32628798484802\n",
      "  time_total_s: 61353.75891828537\n",
      "  timers:\n",
      "    learn_throughput: 18.981\n",
      "    learn_time_ms: 210734.968\n",
      "    load_throughput: 6829167.583\n",
      "    load_time_ms: 0.586\n",
      "    sample_throughput: 18.168\n",
      "    sample_time_ms: 220162.963\n",
      "    update_time_ms: 29.283\n",
      "  timestamp: 1650279706\n",
      "  timesteps_since_restore: 1128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1128000\n",
      "  training_iteration: 282\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-01-47\n",
      "  done: false\n",
      "  episode_len_mean: 1152.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.65\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 627\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014127708971500397\n",
      "          total_loss: -0.01412675529718399\n",
      "          vf_explained_var: -2.4771176754256885e-07\n",
      "          vf_loss: 9.596884638085612e-07\n",
      "    num_agent_steps_sampled: 1128000\n",
      "    num_agent_steps_trained: 1128000\n",
      "    num_steps_sampled: 1128000\n",
      "    num_steps_trained: 1128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.58469387755103\n",
      "    ram_util_percent: 95.97755102040816\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1026485720410857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9433200394871887\n",
      "    mean_inference_ms: 2.032039307781805\n",
      "    mean_raw_obs_processing_ms: 0.12535518495634376\n",
      "  time_since_restore: 61348.531873226166\n",
      "  time_this_iter_s: 221.92744493484497\n",
      "  time_total_s: 61348.531873226166\n",
      "  timers:\n",
      "    learn_throughput: 18.955\n",
      "    learn_time_ms: 211028.923\n",
      "    load_throughput: 6020460.042\n",
      "    load_time_ms: 0.664\n",
      "    sample_throughput: 18.157\n",
      "    sample_time_ms: 220299.402\n",
      "    update_time_ms: 10.615\n",
      "  timestamp: 1650279707\n",
      "  timesteps_since_restore: 1128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1128000\n",
      "  training_iteration: 282\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-05-29\n",
      "  done: false\n",
      "  episode_len_mean: 559.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.91\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3152\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5846867561340332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016391871497035027\n",
      "          model: {}\n",
      "          policy_loss: -0.036509692668914795\n",
      "          total_loss: 0.01870310865342617\n",
      "          vf_explained_var: 0.7115122675895691\n",
      "          vf_loss: 0.055212799459695816\n",
      "    num_agent_steps_sampled: 1132000\n",
      "    num_agent_steps_trained: 1132000\n",
      "    num_steps_sampled: 1132000\n",
      "    num_steps_trained: 1132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.67181208053691\n",
      "    ram_util_percent: 95.91174496644295\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10152047981207665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9574105193605669\n",
      "    mean_inference_ms: 2.0183797524538525\n",
      "    mean_raw_obs_processing_ms: 0.18831978039898123\n",
      "  time_since_restore: 61576.909989356995\n",
      "  time_this_iter_s: 223.15107107162476\n",
      "  time_total_s: 61576.909989356995\n",
      "  timers:\n",
      "    learn_throughput: 18.928\n",
      "    learn_time_ms: 211330.797\n",
      "    load_throughput: 6620842.936\n",
      "    load_time_ms: 0.604\n",
      "    sample_throughput: 18.069\n",
      "    sample_time_ms: 221372.864\n",
      "    update_time_ms: 31.211\n",
      "  timestamp: 1650279929\n",
      "  timesteps_since_restore: 1132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1132000\n",
      "  training_iteration: 283\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-05-31\n",
      "  done: false\n",
      "  episode_len_mean: 1251.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.67\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 629\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.2196511883334728e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.4729569109465956e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.03399679437279701\n",
      "          total_loss: -0.03396197035908699\n",
      "          vf_explained_var: -0.032320547848939896\n",
      "          vf_loss: 3.48225403286051e-05\n",
      "    num_agent_steps_sampled: 1132000\n",
      "    num_agent_steps_trained: 1132000\n",
      "    num_steps_sampled: 1132000\n",
      "    num_steps_trained: 1132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.53569023569023\n",
      "    ram_util_percent: 95.92558922558923\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10260018891363895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9429817465349921\n",
      "    mean_inference_ms: 2.030457544047447\n",
      "    mean_raw_obs_processing_ms: 0.1253124559454699\n",
      "  time_since_restore: 61571.95395421982\n",
      "  time_this_iter_s: 223.42208099365234\n",
      "  time_total_s: 61571.95395421982\n",
      "  timers:\n",
      "    learn_throughput: 18.915\n",
      "    learn_time_ms: 211470.058\n",
      "    load_throughput: 5934845.944\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 18.072\n",
      "    sample_time_ms: 221335.441\n",
      "    update_time_ms: 10.309\n",
      "  timestamp: 1650279931\n",
      "  timesteps_since_restore: 1132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1132000\n",
      "  training_iteration: 283\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-09-08\n",
      "  done: false\n",
      "  episode_len_mean: 560.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.93\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3160\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6534202694892883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01650003343820572\n",
      "          model: {}\n",
      "          policy_loss: -0.04067079350352287\n",
      "          total_loss: 0.06556951254606247\n",
      "          vf_explained_var: 0.6826898455619812\n",
      "          vf_loss: 0.10624030232429504\n",
      "    num_agent_steps_sampled: 1136000\n",
      "    num_agent_steps_trained: 1136000\n",
      "    num_steps_sampled: 1136000\n",
      "    num_steps_trained: 1136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.1873287671233\n",
      "    ram_util_percent: 95.72534246575343\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.101447978699567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.956933240180968\n",
      "    mean_inference_ms: 2.016178364610456\n",
      "    mean_raw_obs_processing_ms: 0.18813073680228026\n",
      "  time_since_restore: 61795.374271154404\n",
      "  time_this_iter_s: 218.46428179740906\n",
      "  time_total_s: 61795.374271154404\n",
      "  timers:\n",
      "    learn_throughput: 18.92\n",
      "    learn_time_ms: 211414.455\n",
      "    load_throughput: 6598189.326\n",
      "    load_time_ms: 0.606\n",
      "    sample_throughput: 18.041\n",
      "    sample_time_ms: 221717.607\n",
      "    update_time_ms: 30.797\n",
      "  timestamp: 1650280148\n",
      "  timesteps_since_restore: 1136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1136000\n",
      "  training_iteration: 284\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-09-10\n",
      "  done: false\n",
      "  episode_len_mean: 1058.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.7\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 636\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.2843420113609187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.2798976873869348e-26\n",
      "          model: {}\n",
      "          policy_loss: -7.507720874855295e-05\n",
      "          total_loss: 5.9547252021729946e-05\n",
      "          vf_explained_var: 0.3527834117412567\n",
      "          vf_loss: 0.0001346252829534933\n",
      "    num_agent_steps_sampled: 1136000\n",
      "    num_agent_steps_trained: 1136000\n",
      "    num_steps_sampled: 1136000\n",
      "    num_steps_trained: 1136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.99419795221841\n",
      "    ram_util_percent: 95.72389078498293\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10244542534122428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.941909434862487\n",
      "    mean_inference_ms: 2.025451934232775\n",
      "    mean_raw_obs_processing_ms: 0.12519202855166864\n",
      "  time_since_restore: 61790.84968781471\n",
      "  time_this_iter_s: 218.8957335948944\n",
      "  time_total_s: 61790.84968781471\n",
      "  timers:\n",
      "    learn_throughput: 18.909\n",
      "    learn_time_ms: 211543.602\n",
      "    load_throughput: 6037141.418\n",
      "    load_time_ms: 0.663\n",
      "    sample_throughput: 18.041\n",
      "    sample_time_ms: 221716.16\n",
      "    update_time_ms: 9.97\n",
      "  timestamp: 1650280150\n",
      "  timesteps_since_restore: 1136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1136000\n",
      "  training_iteration: 284\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 12:12:30 (running for 17:13:50.42)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.93 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-12-54\n",
      "  done: false\n",
      "  episode_len_mean: 564.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 17.0\n",
      "  episode_reward_mean: 7.97\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3167\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7092596292495728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01297777146100998\n",
      "          model: {}\n",
      "          policy_loss: -0.04802032560110092\n",
      "          total_loss: 0.03072158806025982\n",
      "          vf_explained_var: 0.7459908723831177\n",
      "          vf_loss: 0.07874191552400589\n",
      "    num_agent_steps_sampled: 1140000\n",
      "    num_agent_steps_trained: 1140000\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.03708609271523\n",
      "    ram_util_percent: 95.84569536423841\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10138685402078342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9565292388632457\n",
      "    mean_inference_ms: 2.0143048325223147\n",
      "    mean_raw_obs_processing_ms: 0.18796623895755815\n",
      "  time_since_restore: 62021.92274594307\n",
      "  time_this_iter_s: 226.54847478866577\n",
      "  time_total_s: 62021.92274594307\n",
      "  timers:\n",
      "    learn_throughput: 18.834\n",
      "    learn_time_ms: 212383.202\n",
      "    load_throughput: 6531403.434\n",
      "    load_time_ms: 0.612\n",
      "    sample_throughput: 18.037\n",
      "    sample_time_ms: 221771.436\n",
      "    update_time_ms: 32.575\n",
      "  timestamp: 1650280374\n",
      "  timesteps_since_restore: 1140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 285\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-12-56\n",
      "  done: false\n",
      "  episode_len_mean: 1058.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.7\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 636\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.01348649337887764\n",
      "          total_loss: 0.013486495241522789\n",
      "          vf_explained_var: -9.425865528100985e-07\n",
      "          vf_loss: 1.1631244412058095e-08\n",
      "    num_agent_steps_sampled: 1140000\n",
      "    num_agent_steps_trained: 1140000\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.29370860927153\n",
      "    ram_util_percent: 95.8682119205298\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10244542534122428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.941909434862487\n",
      "    mean_inference_ms: 2.025451934232775\n",
      "    mean_raw_obs_processing_ms: 0.12519202855166864\n",
      "  time_since_restore: 62017.39197874069\n",
      "  time_this_iter_s: 226.54229092597961\n",
      "  time_total_s: 62017.39197874069\n",
      "  timers:\n",
      "    learn_throughput: 18.824\n",
      "    learn_time_ms: 212496.797\n",
      "    load_throughput: 4350712.1\n",
      "    load_time_ms: 0.919\n",
      "    sample_throughput: 18.035\n",
      "    sample_time_ms: 221790.687\n",
      "    update_time_ms: 10.214\n",
      "  timestamp: 1650280376\n",
      "  timesteps_since_restore: 1140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 285\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 557.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 7.76\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3175\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7520828247070312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014048732817173004\n",
      "          model: {}\n",
      "          policy_loss: -0.04051036387681961\n",
      "          total_loss: 0.04263371229171753\n",
      "          vf_explained_var: 0.7337687015533447\n",
      "          vf_loss: 0.08314407616853714\n",
      "    num_agent_steps_sampled: 1144000\n",
      "    num_agent_steps_trained: 1144000\n",
      "    num_steps_sampled: 1144000\n",
      "    num_steps_trained: 1144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.5257627118644\n",
      "    ram_util_percent: 95.80881355932203\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10131966002881189\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9560797008484616\n",
      "    mean_inference_ms: 2.0122159583276362\n",
      "    mean_raw_obs_processing_ms: 0.1877820456963399\n",
      "  time_since_restore: 62243.40572977066\n",
      "  time_this_iter_s: 221.48298382759094\n",
      "  time_total_s: 62243.40572977066\n",
      "  timers:\n",
      "    learn_throughput: 18.786\n",
      "    learn_time_ms: 212927.975\n",
      "    load_throughput: 6453271.79\n",
      "    load_time_ms: 0.62\n",
      "    sample_throughput: 17.954\n",
      "    sample_time_ms: 222794.622\n",
      "    update_time_ms: 31.412\n",
      "  timestamp: 1650280596\n",
      "  timesteps_since_restore: 1144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1144000\n",
      "  training_iteration: 286\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-16-37\n",
      "  done: false\n",
      "  episode_len_mean: 1059.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.72\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 638\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6083954447013372e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.125997065207342e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0019045553635805845\n",
      "          total_loss: 0.0019784155301749706\n",
      "          vf_explained_var: -0.01754322461783886\n",
      "          vf_loss: 7.386223296634853e-05\n",
      "    num_agent_steps_sampled: 1144000\n",
      "    num_agent_steps_trained: 1144000\n",
      "    num_steps_sampled: 1144000\n",
      "    num_steps_trained: 1144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.95510204081631\n",
      "    ram_util_percent: 95.828231292517\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1024047525904004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9416311920701256\n",
      "    mean_inference_ms: 2.024152958246108\n",
      "    mean_raw_obs_processing_ms: 0.12516215613704884\n",
      "  time_since_restore: 62238.10632753372\n",
      "  time_this_iter_s: 220.71434879302979\n",
      "  time_total_s: 62238.10632753372\n",
      "  timers:\n",
      "    learn_throughput: 18.782\n",
      "    learn_time_ms: 212975.349\n",
      "    load_throughput: 4380245.418\n",
      "    load_time_ms: 0.913\n",
      "    sample_throughput: 17.96\n",
      "    sample_time_ms: 222719.087\n",
      "    update_time_ms: 10.053\n",
      "  timestamp: 1650280597\n",
      "  timesteps_since_restore: 1144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1144000\n",
      "  training_iteration: 286\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-20-22\n",
      "  done: false\n",
      "  episode_len_mean: 559.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 7.85\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3182\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5872830152511597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015723953023552895\n",
      "          model: {}\n",
      "          policy_loss: -0.04644528031349182\n",
      "          total_loss: 0.019184863194823265\n",
      "          vf_explained_var: 0.7536159157752991\n",
      "          vf_loss: 0.06563013792037964\n",
      "    num_agent_steps_sampled: 1148000\n",
      "    num_agent_steps_trained: 1148000\n",
      "    num_steps_sampled: 1148000\n",
      "    num_steps_trained: 1148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.7189368770764\n",
      "    ram_util_percent: 95.89302325581396\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10125629368723767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9556721498199662\n",
      "    mean_inference_ms: 2.0103333837780433\n",
      "    mean_raw_obs_processing_ms: 0.18761517954326426\n",
      "  time_since_restore: 62469.3347864151\n",
      "  time_this_iter_s: 225.9290566444397\n",
      "  time_total_s: 62469.3347864151\n",
      "  timers:\n",
      "    learn_throughput: 18.716\n",
      "    learn_time_ms: 213715.896\n",
      "    load_throughput: 6435942.919\n",
      "    load_time_ms: 0.622\n",
      "    sample_throughput: 17.913\n",
      "    sample_time_ms: 223302.913\n",
      "    update_time_ms: 29.988\n",
      "  timestamp: 1650280822\n",
      "  timesteps_since_restore: 1148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1148000\n",
      "  training_iteration: 287\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-20-22\n",
      "  done: false\n",
      "  episode_len_mean: 1059.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.72\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 638\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128195121884346\n",
      "          total_loss: -0.014128011651337147\n",
      "          vf_explained_var: 6.65264735744131e-07\n",
      "          vf_loss: 1.8068433860207733e-07\n",
      "    num_agent_steps_sampled: 1148000\n",
      "    num_agent_steps_trained: 1148000\n",
      "    num_steps_sampled: 1148000\n",
      "    num_steps_trained: 1148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.81404682274248\n",
      "    ram_util_percent: 95.88026755852843\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1024047525904004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9416311920701256\n",
      "    mean_inference_ms: 2.024152958246108\n",
      "    mean_raw_obs_processing_ms: 0.12516215613704884\n",
      "  time_since_restore: 62462.94530558586\n",
      "  time_this_iter_s: 224.83897805213928\n",
      "  time_total_s: 62462.94530558586\n",
      "  timers:\n",
      "    learn_throughput: 18.723\n",
      "    learn_time_ms: 213638.655\n",
      "    load_throughput: 4194723.472\n",
      "    load_time_ms: 0.954\n",
      "    sample_throughput: 17.932\n",
      "    sample_time_ms: 223068.509\n",
      "    update_time_ms: 10.47\n",
      "  timestamp: 1650280822\n",
      "  timesteps_since_restore: 1148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1148000\n",
      "  training_iteration: 287\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-24-08\n",
      "  done: false\n",
      "  episode_len_mean: 1059.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.72\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 638\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128157868981361\n",
      "          total_loss: -0.014128110371530056\n",
      "          vf_explained_var: 9.063751349458471e-07\n",
      "          vf_loss: 5.507481404265491e-08\n",
      "    num_agent_steps_sampled: 1152000\n",
      "    num_agent_steps_trained: 1152000\n",
      "    num_steps_sampled: 1152000\n",
      "    num_steps_trained: 1152000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.09102990033223\n",
      "    ram_util_percent: 95.8607973421927\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1024047525904004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9416311920701256\n",
      "    mean_inference_ms: 2.024152958246108\n",
      "    mean_raw_obs_processing_ms: 0.12516215613704884\n",
      "  time_since_restore: 62688.671243429184\n",
      "  time_this_iter_s: 225.72593784332275\n",
      "  time_total_s: 62688.671243429184\n",
      "  timers:\n",
      "    learn_throughput: 18.676\n",
      "    learn_time_ms: 214179.162\n",
      "    load_throughput: 4247288.929\n",
      "    load_time_ms: 0.942\n",
      "    sample_throughput: 17.888\n",
      "    sample_time_ms: 223616.02\n",
      "    update_time_ms: 11.697\n",
      "  timestamp: 1650281048\n",
      "  timesteps_since_restore: 1152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1152000\n",
      "  training_iteration: 288\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-24-09\n",
      "  done: false\n",
      "  episode_len_mean: 551.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 7.66\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3192\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8249155879020691\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012424221262335777\n",
      "          model: {}\n",
      "          policy_loss: -0.05063270777463913\n",
      "          total_loss: 0.039440933614969254\n",
      "          vf_explained_var: 0.7635219097137451\n",
      "          vf_loss: 0.09007363766431808\n",
      "    num_agent_steps_sampled: 1152000\n",
      "    num_agent_steps_trained: 1152000\n",
      "    num_steps_sampled: 1152000\n",
      "    num_steps_trained: 1152000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.13003300330033\n",
      "    ram_util_percent: 95.85544554455446\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1011624338718547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9550741899763655\n",
      "    mean_inference_ms: 2.007592318701656\n",
      "    mean_raw_obs_processing_ms: 0.18738026139945305\n",
      "  time_since_restore: 62695.94011259079\n",
      "  time_this_iter_s: 226.6053261756897\n",
      "  time_total_s: 62695.94011259079\n",
      "  timers:\n",
      "    learn_throughput: 18.668\n",
      "    learn_time_ms: 214274.87\n",
      "    load_throughput: 6788822.077\n",
      "    load_time_ms: 0.589\n",
      "    sample_throughput: 17.858\n",
      "    sample_time_ms: 223987.827\n",
      "    update_time_ms: 25.811\n",
      "  timestamp: 1650281049\n",
      "  timesteps_since_restore: 1152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1152000\n",
      "  training_iteration: 288\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-27-54\n",
      "  done: false\n",
      "  episode_len_mean: 1153.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.66\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 642\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.687899945629471e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.224671984913119e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0001698532432783395\n",
      "          total_loss: 0.0012147726956754923\n",
      "          vf_explained_var: 0.03168296441435814\n",
      "          vf_loss: 0.0010449200635775924\n",
      "    num_agent_steps_sampled: 1156000\n",
      "    num_agent_steps_trained: 1156000\n",
      "    num_steps_sampled: 1156000\n",
      "    num_steps_trained: 1156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.48151815181518\n",
      "    ram_util_percent: 95.9082508250825\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10231967231839832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9410544628964077\n",
      "    mean_inference_ms: 2.0214605077651595\n",
      "    mean_raw_obs_processing_ms: 0.12509820302803912\n",
      "  time_since_restore: 62914.90398359299\n",
      "  time_this_iter_s: 226.2327401638031\n",
      "  time_total_s: 62914.90398359299\n",
      "  timers:\n",
      "    learn_throughput: 18.684\n",
      "    learn_time_ms: 214088.628\n",
      "    load_throughput: 4278701.385\n",
      "    load_time_ms: 0.935\n",
      "    sample_throughput: 17.852\n",
      "    sample_time_ms: 224059.381\n",
      "    update_time_ms: 17.665\n",
      "  timestamp: 1650281274\n",
      "  timesteps_since_restore: 1156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1156000\n",
      "  training_iteration: 289\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-27-54\n",
      "  done: false\n",
      "  episode_len_mean: 551.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16.0\n",
      "  episode_reward_mean: 7.68\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3198\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6661890149116516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015472637489438057\n",
      "          model: {}\n",
      "          policy_loss: -0.04899662733078003\n",
      "          total_loss: 0.02236730046570301\n",
      "          vf_explained_var: 0.7614272236824036\n",
      "          vf_loss: 0.0713639184832573\n",
      "    num_agent_steps_sampled: 1156000\n",
      "    num_agent_steps_trained: 1156000\n",
      "    num_steps_sampled: 1156000\n",
      "    num_steps_trained: 1156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.98543046357615\n",
      "    ram_util_percent: 95.88807947019868\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10110779033112277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9547221640608524\n",
      "    mean_inference_ms: 2.005994319116647\n",
      "    mean_raw_obs_processing_ms: 0.18724682889861385\n",
      "  time_since_restore: 62921.8260846138\n",
      "  time_this_iter_s: 225.88597202301025\n",
      "  time_total_s: 62921.8260846138\n",
      "  timers:\n",
      "    learn_throughput: 18.673\n",
      "    learn_time_ms: 214209.175\n",
      "    load_throughput: 6763642.814\n",
      "    load_time_ms: 0.591\n",
      "    sample_throughput: 17.825\n",
      "    sample_time_ms: 224407.141\n",
      "    update_time_ms: 24.999\n",
      "  timestamp: 1650281274\n",
      "  timesteps_since_restore: 1156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1156000\n",
      "  training_iteration: 289\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 12:29:10 (running for 17:30:30.45)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.68 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 1153.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.66\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 642\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.646962228391861e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -8.38203153359015e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128300361335278\n",
      "          total_loss: 0.01448541134595871\n",
      "          vf_explained_var: -6.665465512867286e-09\n",
      "          vf_loss: 0.0003571131092030555\n",
      "    num_agent_steps_sampled: 1160000\n",
      "    num_agent_steps_trained: 1160000\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.4866220735786\n",
      "    ram_util_percent: 95.80501672240803\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10231967231839832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9410544628964077\n",
      "    mean_inference_ms: 2.0214605077651595\n",
      "    mean_raw_obs_processing_ms: 0.12509820302803912\n",
      "  time_since_restore: 63139.2012386322\n",
      "  time_this_iter_s: 224.2972550392151\n",
      "  time_total_s: 63139.2012386322\n",
      "  timers:\n",
      "    learn_throughput: 18.619\n",
      "    learn_time_ms: 214839.253\n",
      "    load_throughput: 4407865.062\n",
      "    load_time_ms: 0.907\n",
      "    sample_throughput: 17.875\n",
      "    sample_time_ms: 223771.951\n",
      "    update_time_ms: 19.477\n",
      "  timestamp: 1650281498\n",
      "  timesteps_since_restore: 1160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 290\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-31-39\n",
      "  done: false\n",
      "  episode_len_mean: 550.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.0\n",
      "  episode_reward_mean: 7.55\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3205\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6900356411933899\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014795077964663506\n",
      "          model: {}\n",
      "          policy_loss: -0.03939013555645943\n",
      "          total_loss: 0.05453983694314957\n",
      "          vf_explained_var: 0.7598469853401184\n",
      "          vf_loss: 0.0939299687743187\n",
      "    num_agent_steps_sampled: 1160000\n",
      "    num_agent_steps_trained: 1160000\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.18896321070234\n",
      "    ram_util_percent: 95.80066889632107\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10104440538999515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9543064321394935\n",
      "    mean_inference_ms: 2.0041051652345145\n",
      "    mean_raw_obs_processing_ms: 0.18709082957789092\n",
      "  time_since_restore: 63146.7768843174\n",
      "  time_this_iter_s: 224.95079970359802\n",
      "  time_total_s: 63146.7768843174\n",
      "  timers:\n",
      "    learn_throughput: 18.617\n",
      "    learn_time_ms: 214858.309\n",
      "    load_throughput: 6772926.406\n",
      "    load_time_ms: 0.591\n",
      "    sample_throughput: 17.842\n",
      "    sample_time_ms: 224196.021\n",
      "    update_time_ms: 24.736\n",
      "  timestamp: 1650281499\n",
      "  timesteps_since_restore: 1160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 290\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-35-23\n",
      "  done: false\n",
      "  episode_len_mean: 1153.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.66\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 642\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.646962228391861e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -8.38203153359015e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014127974398434162\n",
      "          total_loss: 0.014215853065252304\n",
      "          vf_explained_var: 1.2369565816072736e-08\n",
      "          vf_loss: 8.789011189946905e-05\n",
      "    num_agent_steps_sampled: 1164000\n",
      "    num_agent_steps_trained: 1164000\n",
      "    num_steps_sampled: 1164000\n",
      "    num_steps_trained: 1164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.2294314381271\n",
      "    ram_util_percent: 95.85551839464883\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10231967231839832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9410544628964077\n",
      "    mean_inference_ms: 2.0214605077651595\n",
      "    mean_raw_obs_processing_ms: 0.12509820302803912\n",
      "  time_since_restore: 63363.31625556946\n",
      "  time_this_iter_s: 224.11501693725586\n",
      "  time_total_s: 63363.31625556946\n",
      "  timers:\n",
      "    learn_throughput: 18.683\n",
      "    learn_time_ms: 214102.756\n",
      "    load_throughput: 4446180.103\n",
      "    load_time_ms: 0.9\n",
      "    sample_throughput: 17.82\n",
      "    sample_time_ms: 224466.106\n",
      "    update_time_ms: 20.441\n",
      "  timestamp: 1650281723\n",
      "  timesteps_since_restore: 1164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1164000\n",
      "  training_iteration: 291\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-35-23\n",
      "  done: false\n",
      "  episode_len_mean: 553.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.65\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3213\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7196192741394043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013086597435176373\n",
      "          model: {}\n",
      "          policy_loss: -0.034455832093954086\n",
      "          total_loss: 0.04537658393383026\n",
      "          vf_explained_var: 0.7561315894126892\n",
      "          vf_loss: 0.07983241230249405\n",
      "    num_agent_steps_sampled: 1164000\n",
      "    num_agent_steps_trained: 1164000\n",
      "    num_steps_sampled: 1164000\n",
      "    num_steps_trained: 1164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.11800000000001\n",
      "    ram_util_percent: 95.84933333333333\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10097141072236857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9538216544367786\n",
      "    mean_inference_ms: 2.00188375310459\n",
      "    mean_raw_obs_processing_ms: 0.18691090018055803\n",
      "  time_since_restore: 63370.72680735588\n",
      "  time_this_iter_s: 223.94992303848267\n",
      "  time_total_s: 63370.72680735588\n",
      "  timers:\n",
      "    learn_throughput: 18.687\n",
      "    learn_time_ms: 214053.838\n",
      "    load_throughput: 5225407.544\n",
      "    load_time_ms: 0.765\n",
      "    sample_throughput: 17.792\n",
      "    sample_time_ms: 224813.954\n",
      "    update_time_ms: 20.981\n",
      "  timestamp: 1650281723\n",
      "  timesteps_since_restore: 1164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1164000\n",
      "  training_iteration: 291\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-39-09\n",
      "  done: false\n",
      "  episode_len_mean: 1151.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.64\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 646\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0253225200207075e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -5.0082286719134e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.000920606660656631\n",
      "          total_loss: 0.0010003498755395412\n",
      "          vf_explained_var: 0.024805007502436638\n",
      "          vf_loss: 7.97360553406179e-05\n",
      "    num_agent_steps_sampled: 1168000\n",
      "    num_agent_steps_trained: 1168000\n",
      "    num_steps_sampled: 1168000\n",
      "    num_steps_trained: 1168000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.18245033112582\n",
      "    ram_util_percent: 95.77715231788079\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10223525916145478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9404933510241261\n",
      "    mean_inference_ms: 2.0188370160358096\n",
      "    mean_raw_obs_processing_ms: 0.12503514306743205\n",
      "  time_since_restore: 63589.460255622864\n",
      "  time_this_iter_s: 226.14400005340576\n",
      "  time_total_s: 63589.460255622864\n",
      "  timers:\n",
      "    learn_throughput: 18.648\n",
      "    learn_time_ms: 214497.481\n",
      "    load_throughput: 4501533.673\n",
      "    load_time_ms: 0.889\n",
      "    sample_throughput: 17.876\n",
      "    sample_time_ms: 223761.479\n",
      "    update_time_ms: 24.346\n",
      "  timestamp: 1650281949\n",
      "  timesteps_since_restore: 1168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1168000\n",
      "  training_iteration: 292\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-39-09\n",
      "  done: false\n",
      "  episode_len_mean: 555.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.65\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3220\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6319618225097656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014335171319544315\n",
      "          model: {}\n",
      "          policy_loss: -0.03894134610891342\n",
      "          total_loss: 0.03184591978788376\n",
      "          vf_explained_var: 0.7386859059333801\n",
      "          vf_loss: 0.07078725844621658\n",
      "    num_agent_steps_sampled: 1168000\n",
      "    num_agent_steps_trained: 1168000\n",
      "    num_steps_sampled: 1168000\n",
      "    num_steps_trained: 1168000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.06866666666666\n",
      "    ram_util_percent: 95.78833333333333\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1009060469892382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9533868167897339\n",
      "    mean_inference_ms: 1.999888428591626\n",
      "    mean_raw_obs_processing_ms: 0.1867505375536918\n",
      "  time_since_restore: 63596.39463543892\n",
      "  time_this_iter_s: 225.66782808303833\n",
      "  time_total_s: 63596.39463543892\n",
      "  timers:\n",
      "    learn_throughput: 18.655\n",
      "    learn_time_ms: 214415.238\n",
      "    load_throughput: 4966612.197\n",
      "    load_time_ms: 0.805\n",
      "    sample_throughput: 17.859\n",
      "    sample_time_ms: 223972.876\n",
      "    update_time_ms: 18.287\n",
      "  timestamp: 1650281949\n",
      "  timesteps_since_restore: 1168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1168000\n",
      "  training_iteration: 292\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-42-55\n",
      "  done: false\n",
      "  episode_len_mean: 1151.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.64\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 646\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.4558103167540897e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -5.562752821523073e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014127966947853565\n",
      "          total_loss: -0.014127524569630623\n",
      "          vf_explained_var: 9.761061647850511e-08\n",
      "          vf_loss: 4.4633799234361504e-07\n",
      "    num_agent_steps_sampled: 1172000\n",
      "    num_agent_steps_trained: 1172000\n",
      "    num_steps_sampled: 1172000\n",
      "    num_steps_trained: 1172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.74\n",
      "    ram_util_percent: 95.72366666666666\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10223525916145478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9404933510241261\n",
      "    mean_inference_ms: 2.0188370160358096\n",
      "    mean_raw_obs_processing_ms: 0.12503514306743205\n",
      "  time_since_restore: 63815.26985549927\n",
      "  time_this_iter_s: 225.8095998764038\n",
      "  time_total_s: 63815.26985549927\n",
      "  timers:\n",
      "    learn_throughput: 18.623\n",
      "    learn_time_ms: 214791.162\n",
      "    load_throughput: 4591340.138\n",
      "    load_time_ms: 0.871\n",
      "    sample_throughput: 17.849\n",
      "    sample_time_ms: 224106.866\n",
      "    update_time_ms: 29.131\n",
      "  timestamp: 1650282175\n",
      "  timesteps_since_restore: 1172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1172000\n",
      "  training_iteration: 293\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-42-55\n",
      "  done: false\n",
      "  episode_len_mean: 562.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.79\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3226\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6016092300415039\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01833619549870491\n",
      "          model: {}\n",
      "          policy_loss: -0.0607030875980854\n",
      "          total_loss: 0.025282390415668488\n",
      "          vf_explained_var: 0.7042336463928223\n",
      "          vf_loss: 0.08598547428846359\n",
      "    num_agent_steps_sampled: 1172000\n",
      "    num_agent_steps_trained: 1172000\n",
      "    num_steps_sampled: 1172000\n",
      "    num_steps_trained: 1172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.85894039735099\n",
      "    ram_util_percent: 95.73046357615894\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10084833836942454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.953004750253283\n",
      "    mean_inference_ms: 1.9981409294978092\n",
      "    mean_raw_obs_processing_ms: 0.18660993116161306\n",
      "  time_since_restore: 63822.61721253395\n",
      "  time_this_iter_s: 226.22257709503174\n",
      "  time_total_s: 63822.61721253395\n",
      "  timers:\n",
      "    learn_throughput: 18.626\n",
      "    learn_time_ms: 214753.292\n",
      "    load_throughput: 5060694.981\n",
      "    load_time_ms: 0.79\n",
      "    sample_throughput: 17.833\n",
      "    sample_time_ms: 224298.498\n",
      "    update_time_ms: 18.911\n",
      "  timestamp: 1650282175\n",
      "  timesteps_since_restore: 1172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1172000\n",
      "  training_iteration: 293\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 12:45:51 (running for 17:47:11.36)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.79 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-46-41\n",
      "  done: false\n",
      "  episode_len_mean: 562.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.82\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3235\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.750863790512085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013253980316221714\n",
      "          model: {}\n",
      "          policy_loss: -0.044258054345846176\n",
      "          total_loss: 0.028597092255949974\n",
      "          vf_explained_var: 0.7441774010658264\n",
      "          vf_loss: 0.0728551372885704\n",
      "    num_agent_steps_sampled: 1176000\n",
      "    num_agent_steps_trained: 1176000\n",
      "    num_steps_sampled: 1176000\n",
      "    num_steps_trained: 1176000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.27333333333333\n",
      "    ram_util_percent: 95.77533333333332\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1007608149620155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9524160155445975\n",
      "    mean_inference_ms: 1.9954499729938526\n",
      "    mean_raw_obs_processing_ms: 0.18639584392067804\n",
      "  time_since_restore: 64047.91630935669\n",
      "  time_this_iter_s: 225.29909682273865\n",
      "  time_total_s: 64047.91630935669\n",
      "  timers:\n",
      "    learn_throughput: 18.565\n",
      "    learn_time_ms: 215457.047\n",
      "    load_throughput: 5068185.965\n",
      "    load_time_ms: 0.789\n",
      "    sample_throughput: 17.809\n",
      "    sample_time_ms: 224611.703\n",
      "    update_time_ms: 20.842\n",
      "  timestamp: 1650282401\n",
      "  timesteps_since_restore: 1176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1176000\n",
      "  training_iteration: 294\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-46-42\n",
      "  done: false\n",
      "  episode_len_mean: 1148.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.59\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 652\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.198008622106399e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.9629743256381556e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0074232821352779865\n",
      "          total_loss: 0.007477450650185347\n",
      "          vf_explained_var: -0.1137537807226181\n",
      "          vf_loss: 5.416556086856872e-05\n",
      "    num_agent_steps_sampled: 1176000\n",
      "    num_agent_steps_trained: 1176000\n",
      "    num_steps_sampled: 1176000\n",
      "    num_steps_trained: 1176000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.25445544554455\n",
      "    ram_util_percent: 95.77755775577558\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10210469653704307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9396296393995547\n",
      "    mean_inference_ms: 2.0147933578491726\n",
      "    mean_raw_obs_processing_ms: 0.12493893695032224\n",
      "  time_since_restore: 64042.05357837677\n",
      "  time_this_iter_s: 226.78372287750244\n",
      "  time_total_s: 64042.05357837677\n",
      "  timers:\n",
      "    learn_throughput: 18.55\n",
      "    learn_time_ms: 215629.513\n",
      "    load_throughput: 4660726.172\n",
      "    load_time_ms: 0.858\n",
      "    sample_throughput: 17.828\n",
      "    sample_time_ms: 224363.283\n",
      "    update_time_ms: 31.591\n",
      "  timestamp: 1650282402\n",
      "  timesteps_since_restore: 1176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1176000\n",
      "  training_iteration: 294\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-50-22\n",
      "  done: false\n",
      "  episode_len_mean: 563.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.9\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3243\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.722420334815979\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013788137584924698\n",
      "          model: {}\n",
      "          policy_loss: -0.05157981812953949\n",
      "          total_loss: 0.02203899435698986\n",
      "          vf_explained_var: 0.7822320461273193\n",
      "          vf_loss: 0.0736188143491745\n",
      "    num_agent_steps_sampled: 1180000\n",
      "    num_agent_steps_trained: 1180000\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.6233108108108\n",
      "    ram_util_percent: 95.74324324324324\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10068380218412404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9518969689979697\n",
      "    mean_inference_ms: 1.993093190302439\n",
      "    mean_raw_obs_processing_ms: 0.186208153231742\n",
      "  time_since_restore: 64269.37807941437\n",
      "  time_this_iter_s: 221.46177005767822\n",
      "  time_total_s: 64269.37807941437\n",
      "  timers:\n",
      "    learn_throughput: 18.607\n",
      "    learn_time_ms: 214973.368\n",
      "    load_throughput: 5101318.414\n",
      "    load_time_ms: 0.784\n",
      "    sample_throughput: 17.754\n",
      "    sample_time_ms: 225297.236\n",
      "    update_time_ms: 18.927\n",
      "  timestamp: 1650282622\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 295\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-50-23\n",
      "  done: false\n",
      "  episode_len_mean: 1147.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.57\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 664\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1282143849279197e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.0434867164988883e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0002568693598732352\n",
      "          total_loss: -7.781200838508084e-05\n",
      "          vf_explained_var: 0.1701008528470993\n",
      "          vf_loss: 0.00017905517597682774\n",
      "    num_agent_steps_sampled: 1180000\n",
      "    num_agent_steps_trained: 1180000\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.0122033898305\n",
      "    ram_util_percent: 95.72915254237287\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10185822050228685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9379952757205312\n",
      "    mean_inference_ms: 2.007115397487599\n",
      "    mean_raw_obs_processing_ms: 0.124756414222166\n",
      "  time_since_restore: 64263.758882284164\n",
      "  time_this_iter_s: 221.7053039073944\n",
      "  time_total_s: 64263.758882284164\n",
      "  timers:\n",
      "    learn_throughput: 18.595\n",
      "    learn_time_ms: 215115.547\n",
      "    load_throughput: 6726761.557\n",
      "    load_time_ms: 0.595\n",
      "    sample_throughput: 17.759\n",
      "    sample_time_ms: 225238.602\n",
      "    update_time_ms: 31.708\n",
      "  timestamp: 1650282623\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 295\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-54-00\n",
      "  done: false\n",
      "  episode_len_mean: 555.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.72\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3251\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7480476498603821\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013387537561357021\n",
      "          model: {}\n",
      "          policy_loss: -0.05449466407299042\n",
      "          total_loss: 0.013960273936390877\n",
      "          vf_explained_var: 0.7056549191474915\n",
      "          vf_loss: 0.06845494359731674\n",
      "    num_agent_steps_sampled: 1184000\n",
      "    num_agent_steps_trained: 1184000\n",
      "    num_steps_sampled: 1184000\n",
      "    num_steps_trained: 1184000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.9506896551724\n",
      "    ram_util_percent: 95.84482758620689\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10060705246375047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.951375407905309\n",
      "    mean_inference_ms: 1.990754683728037\n",
      "    mean_raw_obs_processing_ms: 0.1860259227793634\n",
      "  time_since_restore: 64486.752521276474\n",
      "  time_this_iter_s: 217.37444186210632\n",
      "  time_total_s: 64486.752521276474\n",
      "  timers:\n",
      "    learn_throughput: 18.636\n",
      "    learn_time_ms: 214635.896\n",
      "    load_throughput: 5092492.336\n",
      "    load_time_ms: 0.785\n",
      "    sample_throughput: 17.798\n",
      "    sample_time_ms: 224744.614\n",
      "    update_time_ms: 19.857\n",
      "  timestamp: 1650282840\n",
      "  timesteps_since_restore: 1184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1184000\n",
      "  training_iteration: 296\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-54-01\n",
      "  done: false\n",
      "  episode_len_mean: 1147.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.57\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 664\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.457150898068786e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.3574807594724636e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014127993024885654\n",
      "          total_loss: -0.014127504080533981\n",
      "          vf_explained_var: -7.876786156657545e-08\n",
      "          vf_loss: 4.874274281974067e-07\n",
      "    num_agent_steps_sampled: 1184000\n",
      "    num_agent_steps_trained: 1184000\n",
      "    num_steps_sampled: 1184000\n",
      "    num_steps_trained: 1184000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.15517241379311\n",
      "    ram_util_percent: 95.85137931034484\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10185822050228685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9379952757205312\n",
      "    mean_inference_ms: 2.007115397487599\n",
      "    mean_raw_obs_processing_ms: 0.124756414222166\n",
      "  time_since_restore: 64481.047610998154\n",
      "  time_this_iter_s: 217.28872871398926\n",
      "  time_total_s: 64481.047610998154\n",
      "  timers:\n",
      "    learn_throughput: 18.617\n",
      "    learn_time_ms: 214855.432\n",
      "    load_throughput: 6799828.152\n",
      "    load_time_ms: 0.588\n",
      "    sample_throughput: 17.806\n",
      "    sample_time_ms: 224647.089\n",
      "    update_time_ms: 32.497\n",
      "  timestamp: 1650282841\n",
      "  timesteps_since_restore: 1184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1184000\n",
      "  training_iteration: 296\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 558.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.77\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3258\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6136174201965332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017344314604997635\n",
      "          model: {}\n",
      "          policy_loss: -0.04347639158368111\n",
      "          total_loss: 0.03276655450463295\n",
      "          vf_explained_var: 0.7505922317504883\n",
      "          vf_loss: 0.07624295353889465\n",
      "    num_agent_steps_sampled: 1188000\n",
      "    num_agent_steps_trained: 1188000\n",
      "    num_steps_sampled: 1188000\n",
      "    num_steps_trained: 1188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.9156146179402\n",
      "    ram_util_percent: 95.73255813953489\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10054005552397832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9509209351719011\n",
      "    mean_inference_ms: 1.9887086044385653\n",
      "    mean_raw_obs_processing_ms: 0.18586644499949745\n",
      "  time_since_restore: 64712.531729221344\n",
      "  time_this_iter_s: 225.77920794487\n",
      "  time_total_s: 64712.531729221344\n",
      "  timers:\n",
      "    learn_throughput: 18.635\n",
      "    learn_time_ms: 214644.276\n",
      "    load_throughput: 5108619.104\n",
      "    load_time_ms: 0.783\n",
      "    sample_throughput: 17.827\n",
      "    sample_time_ms: 224381.039\n",
      "    update_time_ms: 20.161\n",
      "  timestamp: 1650283066\n",
      "  timesteps_since_restore: 1188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1188000\n",
      "  training_iteration: 297\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_12-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 1147.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.57\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 664\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.457150898068786e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.3574807594724636e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.009635581634938717\n",
      "          total_loss: -0.009635577909648418\n",
      "          vf_explained_var: -9.882834461905077e-08\n",
      "          vf_loss: 1.0461641331005467e-08\n",
      "    num_agent_steps_sampled: 1188000\n",
      "    num_agent_steps_trained: 1188000\n",
      "    num_steps_sampled: 1188000\n",
      "    num_steps_trained: 1188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.82166666666667\n",
      "    ram_util_percent: 95.74400000000001\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10185822050228685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9379952757205312\n",
      "    mean_inference_ms: 2.007115397487599\n",
      "    mean_raw_obs_processing_ms: 0.124756414222166\n",
      "  time_since_restore: 64706.026654958725\n",
      "  time_this_iter_s: 224.9790439605713\n",
      "  time_total_s: 64706.026654958725\n",
      "  timers:\n",
      "    learn_throughput: 18.614\n",
      "    learn_time_ms: 214893.998\n",
      "    load_throughput: 7127109.601\n",
      "    load_time_ms: 0.561\n",
      "    sample_throughput: 17.828\n",
      "    sample_time_ms: 224367.902\n",
      "    update_time_ms: 32.378\n",
      "  timestamp: 1650283066\n",
      "  timesteps_since_restore: 1188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1188000\n",
      "  training_iteration: 297\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 561.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3265\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.564842700958252\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019106954336166382\n",
      "          model: {}\n",
      "          policy_loss: -0.038224536925554276\n",
      "          total_loss: 0.05479767546057701\n",
      "          vf_explained_var: 0.6894170045852661\n",
      "          vf_loss: 0.09302221983671188\n",
      "    num_agent_steps_sampled: 1192000\n",
      "    num_agent_steps_trained: 1192000\n",
      "    num_steps_sampled: 1192000\n",
      "    num_steps_trained: 1192000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.56033898305085\n",
      "    ram_util_percent: 95.71661016949153\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10047171262954663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9504548773585867\n",
      "    mean_inference_ms: 1.986625848733342\n",
      "    mean_raw_obs_processing_ms: 0.18570305669405915\n",
      "  time_since_restore: 64934.48904824257\n",
      "  time_this_iter_s: 221.95731902122498\n",
      "  time_total_s: 64934.48904824257\n",
      "  timers:\n",
      "    learn_throughput: 18.672\n",
      "    learn_time_ms: 214226.189\n",
      "    load_throughput: 4900030.959\n",
      "    load_time_ms: 0.816\n",
      "    sample_throughput: 17.83\n",
      "    sample_time_ms: 224344.685\n",
      "    update_time_ms: 20.61\n",
      "  timestamp: 1650283288\n",
      "  timesteps_since_restore: 1192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1192000\n",
      "  training_iteration: 298\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 1245.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.55\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 667\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.5451201344971226e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5199865794445764e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.002629590453580022\n",
      "          total_loss: 0.002812368096783757\n",
      "          vf_explained_var: 0.09776310622692108\n",
      "          vf_loss: 0.00018277882190886885\n",
      "    num_agent_steps_sampled: 1192000\n",
      "    num_agent_steps_trained: 1192000\n",
      "    num_steps_sampled: 1192000\n",
      "    num_steps_trained: 1192000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.78440677966103\n",
      "    ram_util_percent: 95.70949152542373\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10178959201671482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9375401451422014\n",
      "    mean_inference_ms: 2.0049796825629813\n",
      "    mean_raw_obs_processing_ms: 0.12470057464838426\n",
      "  time_since_restore: 64928.00761604309\n",
      "  time_this_iter_s: 221.98096108436584\n",
      "  time_total_s: 64928.00761604309\n",
      "  timers:\n",
      "    learn_throughput: 18.646\n",
      "    learn_time_ms: 214527.139\n",
      "    load_throughput: 7321818.975\n",
      "    load_time_ms: 0.546\n",
      "    sample_throughput: 17.826\n",
      "    sample_time_ms: 224395.17\n",
      "    update_time_ms: 31.266\n",
      "  timestamp: 1650283288\n",
      "  timesteps_since_restore: 1192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1192000\n",
      "  training_iteration: 298\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 13:02:32 (running for 18:03:51.87)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.9 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-05-08\n",
      "  done: false\n",
      "  episode_len_mean: 1245.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.55\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 667\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6875004068904024e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.668840318176706e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128077775239944\n",
      "          total_loss: 0.014135828241705894\n",
      "          vf_explained_var: 3.8775063160301215e-08\n",
      "          vf_loss: 7.752575584163424e-06\n",
      "    num_agent_steps_sampled: 1196000\n",
      "    num_agent_steps_trained: 1196000\n",
      "    num_steps_sampled: 1196000\n",
      "    num_steps_trained: 1196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.579180887372\n",
      "    ram_util_percent: 95.79692832764503\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10178959201671482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9375401451422014\n",
      "    mean_inference_ms: 2.0049796825629813\n",
      "    mean_raw_obs_processing_ms: 0.12470057464838426\n",
      "  time_since_restore: 65148.57682514191\n",
      "  time_this_iter_s: 220.56920909881592\n",
      "  time_total_s: 65148.57682514191\n",
      "  timers:\n",
      "    learn_throughput: 18.689\n",
      "    learn_time_ms: 214025.506\n",
      "    load_throughput: 7344576.457\n",
      "    load_time_ms: 0.545\n",
      "    sample_throughput: 17.86\n",
      "    sample_time_ms: 223968.161\n",
      "    update_time_ms: 25.682\n",
      "  timestamp: 1650283508\n",
      "  timesteps_since_restore: 1196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1196000\n",
      "  training_iteration: 299\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-05-09\n",
      "  done: false\n",
      "  episode_len_mean: 566.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 8.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3272\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6632274389266968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013580869883298874\n",
      "          model: {}\n",
      "          policy_loss: -0.047481440007686615\n",
      "          total_loss: 0.029538597911596298\n",
      "          vf_explained_var: 0.7366984486579895\n",
      "          vf_loss: 0.07702003419399261\n",
      "    num_agent_steps_sampled: 1196000\n",
      "    num_agent_steps_trained: 1196000\n",
      "    num_steps_sampled: 1196000\n",
      "    num_steps_trained: 1196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.85034013605443\n",
      "    ram_util_percent: 95.81394557823128\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.100402597941516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9499804779933032\n",
      "    mean_inference_ms: 1.9845145657455732\n",
      "    mean_raw_obs_processing_ms: 0.18553762662531273\n",
      "  time_since_restore: 65155.726813554764\n",
      "  time_this_iter_s: 221.23776531219482\n",
      "  time_total_s: 65155.726813554764\n",
      "  timers:\n",
      "    learn_throughput: 18.708\n",
      "    learn_time_ms: 213812.95\n",
      "    load_throughput: 4930125.184\n",
      "    load_time_ms: 0.811\n",
      "    sample_throughput: 17.866\n",
      "    sample_time_ms: 223884.595\n",
      "    update_time_ms: 19.865\n",
      "  timestamp: 1650283509\n",
      "  timesteps_since_restore: 1196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1196000\n",
      "  training_iteration: 299\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-08-49\n",
      "  done: false\n",
      "  episode_len_mean: 568.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 8.04\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3278\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.675541877746582\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014971628785133362\n",
      "          model: {}\n",
      "          policy_loss: -0.042731937021017075\n",
      "          total_loss: 0.02912481501698494\n",
      "          vf_explained_var: 0.7659034132957458\n",
      "          vf_loss: 0.07185675203800201\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_agent_steps_trained: 1200000\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.62636986301371\n",
      "    ram_util_percent: 95.88287671232877\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10034251400432936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9495672986539208\n",
      "    mean_inference_ms: 1.9826786522346191\n",
      "    mean_raw_obs_processing_ms: 0.18539419600220378\n",
      "  time_since_restore: 65375.78713679314\n",
      "  time_this_iter_s: 220.0603232383728\n",
      "  time_total_s: 65375.78713679314\n",
      "  timers:\n",
      "    learn_throughput: 18.754\n",
      "    learn_time_ms: 213287.546\n",
      "    load_throughput: 5006032.106\n",
      "    load_time_ms: 0.799\n",
      "    sample_throughput: 17.897\n",
      "    sample_time_ms: 223501.302\n",
      "    update_time_ms: 20.254\n",
      "  timestamp: 1650283729\n",
      "  timesteps_since_restore: 1200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 300\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-08-50\n",
      "  done: false\n",
      "  episode_len_mean: 1245.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.6\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 672\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.547931126863125e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.0487463428664143e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.020374752581119537\n",
      "          total_loss: -0.0203149002045393\n",
      "          vf_explained_var: 0.08243940025568008\n",
      "          vf_loss: 5.985713141853921e-05\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_agent_steps_trained: 1200000\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.59624573378838\n",
      "    ram_util_percent: 95.86075085324231\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.101679022773774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9368019973354589\n",
      "    mean_inference_ms: 2.001536661990418\n",
      "    mean_raw_obs_processing_ms: 0.1246118686348882\n",
      "  time_since_restore: 65369.67256236076\n",
      "  time_this_iter_s: 221.0957372188568\n",
      "  time_total_s: 65369.67256236076\n",
      "  timers:\n",
      "    learn_throughput: 18.721\n",
      "    learn_time_ms: 213666.259\n",
      "    load_throughput: 7324695.918\n",
      "    load_time_ms: 0.546\n",
      "    sample_throughput: 17.897\n",
      "    sample_time_ms: 223495.163\n",
      "    update_time_ms: 23.806\n",
      "  timestamp: 1650283730\n",
      "  timesteps_since_restore: 1200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 300\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-12-31\n",
      "  done: false\n",
      "  episode_len_mean: 572.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 8.17\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3286\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7351970672607422\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014002487994730473\n",
      "          model: {}\n",
      "          policy_loss: -0.04656257480382919\n",
      "          total_loss: 0.016248216852545738\n",
      "          vf_explained_var: 0.7919773459434509\n",
      "          vf_loss: 0.06281078606843948\n",
      "    num_agent_steps_sampled: 1204000\n",
      "    num_agent_steps_trained: 1204000\n",
      "    num_steps_sampled: 1204000\n",
      "    num_steps_trained: 1204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.20171232876713\n",
      "    ram_util_percent: 95.88253424657533\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10026262944438033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9490169714520021\n",
      "    mean_inference_ms: 1.9802249499519378\n",
      "    mean_raw_obs_processing_ms: 0.18520020250582328\n",
      "  time_since_restore: 65597.62767076492\n",
      "  time_this_iter_s: 221.8405339717865\n",
      "  time_total_s: 65597.62767076492\n",
      "  timers:\n",
      "    learn_throughput: 18.767\n",
      "    learn_time_ms: 213140.222\n",
      "    load_throughput: 6438906.97\n",
      "    load_time_ms: 0.621\n",
      "    sample_throughput: 17.945\n",
      "    sample_time_ms: 222907.337\n",
      "    update_time_ms: 20.467\n",
      "  timestamp: 1650283951\n",
      "  timesteps_since_restore: 1204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1204000\n",
      "  training_iteration: 301\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-12-32\n",
      "  done: false\n",
      "  episode_len_mean: 1244.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.57\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 673\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0099481076011798e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.157276444799931e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.007054928690195084\n",
      "          total_loss: -0.007006075233221054\n",
      "          vf_explained_var: 0.06421265006065369\n",
      "          vf_loss: 4.885431189904921e-05\n",
      "    num_agent_steps_sampled: 1204000\n",
      "    num_agent_steps_trained: 1204000\n",
      "    num_steps_sampled: 1204000\n",
      "    num_steps_trained: 1204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.89351535836177\n",
      "    ram_util_percent: 95.89658703071672\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10165663492306777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9366525692489054\n",
      "    mean_inference_ms: 2.000841586119771\n",
      "    mean_raw_obs_processing_ms: 0.12459367711473596\n",
      "  time_since_restore: 65591.96166944504\n",
      "  time_this_iter_s: 222.2891070842743\n",
      "  time_total_s: 65591.96166944504\n",
      "  timers:\n",
      "    learn_throughput: 18.73\n",
      "    learn_time_ms: 213559.456\n",
      "    load_throughput: 7341683.879\n",
      "    load_time_ms: 0.545\n",
      "    sample_throughput: 17.934\n",
      "    sample_time_ms: 223040.5\n",
      "    update_time_ms: 22.433\n",
      "  timestamp: 1650283952\n",
      "  timesteps_since_restore: 1204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1204000\n",
      "  training_iteration: 301\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 581.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 8.33\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3293\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6963712573051453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015455334447324276\n",
      "          model: {}\n",
      "          policy_loss: -0.049371328204870224\n",
      "          total_loss: 0.007209741044789553\n",
      "          vf_explained_var: 0.7997763156890869\n",
      "          vf_loss: 0.05658106878399849\n",
      "    num_agent_steps_sampled: 1208000\n",
      "    num_agent_steps_trained: 1208000\n",
      "    num_steps_sampled: 1208000\n",
      "    num_steps_trained: 1208000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.86847457627118\n",
      "    ram_util_percent: 95.65118644067798\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10019252443124983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9485258905882173\n",
      "    mean_inference_ms: 1.978029758182052\n",
      "    mean_raw_obs_processing_ms: 0.18502413340572993\n",
      "  time_since_restore: 65818.85478782654\n",
      "  time_this_iter_s: 221.227117061615\n",
      "  time_total_s: 65818.85478782654\n",
      "  timers:\n",
      "    learn_throughput: 18.806\n",
      "    learn_time_ms: 212700.642\n",
      "    load_throughput: 6923864.471\n",
      "    load_time_ms: 0.578\n",
      "    sample_throughput: 17.957\n",
      "    sample_time_ms: 222758.248\n",
      "    update_time_ms: 20.131\n",
      "  timestamp: 1650284172\n",
      "  timesteps_since_restore: 1208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1208000\n",
      "  training_iteration: 302\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-16-13\n",
      "  done: false\n",
      "  episode_len_mean: 1244.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.57\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 673\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0223031486489103e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.735102420806745e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128195121884346\n",
      "          total_loss: -0.014121676795184612\n",
      "          vf_explained_var: -2.9930504297226435e-08\n",
      "          vf_loss: 6.522340299852658e-06\n",
      "    num_agent_steps_sampled: 1208000\n",
      "    num_agent_steps_trained: 1208000\n",
      "    num_steps_sampled: 1208000\n",
      "    num_steps_trained: 1208000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.784693877551\n",
      "    ram_util_percent: 95.65340136054424\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10165663492306777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9366525692489054\n",
      "    mean_inference_ms: 2.000841586119771\n",
      "    mean_raw_obs_processing_ms: 0.12459367711473596\n",
      "  time_since_restore: 65813.43495154381\n",
      "  time_this_iter_s: 221.47328209877014\n",
      "  time_total_s: 65813.43495154381\n",
      "  timers:\n",
      "    learn_throughput: 18.768\n",
      "    learn_time_ms: 213129.742\n",
      "    load_throughput: 7363595.506\n",
      "    load_time_ms: 0.543\n",
      "    sample_throughput: 17.946\n",
      "    sample_time_ms: 222896.381\n",
      "    update_time_ms: 17.876\n",
      "  timestamp: 1650284173\n",
      "  timesteps_since_restore: 1208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1208000\n",
      "  training_iteration: 302\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 13:19:12 (running for 18:20:31.99)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=8.33 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-19-53\n",
      "  done: false\n",
      "  episode_len_mean: 582.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 8.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3300\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6078398823738098\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020372308790683746\n",
      "          model: {}\n",
      "          policy_loss: -0.0381113663315773\n",
      "          total_loss: 0.0833699181675911\n",
      "          vf_explained_var: 0.648784875869751\n",
      "          vf_loss: 0.1214812844991684\n",
      "    num_agent_steps_sampled: 1212000\n",
      "    num_agent_steps_trained: 1212000\n",
      "    num_steps_sampled: 1212000\n",
      "    num_steps_trained: 1212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.95979381443298\n",
      "    ram_util_percent: 95.77835051546391\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10012363915362975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9480425906720993\n",
      "    mean_inference_ms: 1.975878482019923\n",
      "    mean_raw_obs_processing_ms: 0.1848481214812839\n",
      "  time_since_restore: 66039.28474497795\n",
      "  time_this_iter_s: 220.42995715141296\n",
      "  time_total_s: 66039.28474497795\n",
      "  timers:\n",
      "    learn_throughput: 18.858\n",
      "    learn_time_ms: 212108.718\n",
      "    load_throughput: 6830835.878\n",
      "    load_time_ms: 0.586\n",
      "    sample_throughput: 17.99\n",
      "    sample_time_ms: 222343.469\n",
      "    update_time_ms: 17.739\n",
      "  timestamp: 1650284393\n",
      "  timesteps_since_restore: 1212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1212000\n",
      "  training_iteration: 303\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-19-55\n",
      "  done: false\n",
      "  episode_len_mean: 1243.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.58\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 682\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0942954561692706e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.648299328326647e-27\n",
      "          model: {}\n",
      "          policy_loss: 1.866162165242713e-05\n",
      "          total_loss: 0.0002536718384362757\n",
      "          vf_explained_var: 0.22141414880752563\n",
      "          vf_loss: 0.0002350139111513272\n",
      "    num_agent_steps_sampled: 1212000\n",
      "    num_agent_steps_trained: 1212000\n",
      "    num_steps_sampled: 1212000\n",
      "    num_steps_trained: 1212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.53116438356165\n",
      "    ram_util_percent: 95.78527397260272\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10147088219037918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9353913927367791\n",
      "    mean_inference_ms: 1.9949992794029376\n",
      "    mean_raw_obs_processing_ms: 0.12444839500542251\n",
      "  time_since_restore: 66034.52138066292\n",
      "  time_this_iter_s: 221.0864291191101\n",
      "  time_total_s: 66034.52138066292\n",
      "  timers:\n",
      "    learn_throughput: 18.817\n",
      "    learn_time_ms: 212573.958\n",
      "    load_throughput: 7303014.844\n",
      "    load_time_ms: 0.548\n",
      "    sample_throughput: 17.974\n",
      "    sample_time_ms: 222541.964\n",
      "    update_time_ms: 13.485\n",
      "  timestamp: 1650284395\n",
      "  timesteps_since_restore: 1212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1212000\n",
      "  training_iteration: 303\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-23-33\n",
      "  done: false\n",
      "  episode_len_mean: 578.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 8.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3306\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6393179297447205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015282799489796162\n",
      "          model: {}\n",
      "          policy_loss: -0.03261035680770874\n",
      "          total_loss: 0.06213106960058212\n",
      "          vf_explained_var: 0.7304357886314392\n",
      "          vf_loss: 0.09474141895771027\n",
      "    num_agent_steps_sampled: 1216000\n",
      "    num_agent_steps_trained: 1216000\n",
      "    num_steps_sampled: 1216000\n",
      "    num_steps_trained: 1216000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.75938566552901\n",
      "    ram_util_percent: 95.7853242320819\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10006536951680443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9476312667846508\n",
      "    mean_inference_ms: 1.9740589857116873\n",
      "    mean_raw_obs_processing_ms: 0.18469699514920088\n",
      "  time_since_restore: 66259.37325382233\n",
      "  time_this_iter_s: 220.0885088443756\n",
      "  time_total_s: 66259.37325382233\n",
      "  timers:\n",
      "    learn_throughput: 18.906\n",
      "    learn_time_ms: 211568.176\n",
      "    load_throughput: 6897108.325\n",
      "    load_time_ms: 0.58\n",
      "    sample_throughput: 18.036\n",
      "    sample_time_ms: 221778.032\n",
      "    update_time_ms: 16.82\n",
      "  timestamp: 1650284613\n",
      "  timesteps_since_restore: 1216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1216000\n",
      "  training_iteration: 304\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-23-35\n",
      "  done: false\n",
      "  episode_len_mean: 1140.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.48\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 690\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.762608603844265e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.5371949481047691e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.00032421783544123173\n",
      "          total_loss: -0.00020650506485253572\n",
      "          vf_explained_var: 0.3092726171016693\n",
      "          vf_loss: 0.00011771237041102722\n",
      "    num_agent_steps_sampled: 1216000\n",
      "    num_agent_steps_trained: 1216000\n",
      "    num_steps_sampled: 1216000\n",
      "    num_steps_trained: 1216000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.7982993197279\n",
      "    ram_util_percent: 95.8047619047619\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10131617919970477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9343398258459525\n",
      "    mean_inference_ms: 1.9901572296720438\n",
      "    mean_raw_obs_processing_ms: 0.12433173170383646\n",
      "  time_since_restore: 66255.25235676765\n",
      "  time_this_iter_s: 220.73097610473633\n",
      "  time_total_s: 66255.25235676765\n",
      "  timers:\n",
      "    learn_throughput: 18.874\n",
      "    learn_time_ms: 211936.957\n",
      "    load_throughput: 7160264.607\n",
      "    load_time_ms: 0.559\n",
      "    sample_throughput: 18.018\n",
      "    sample_time_ms: 222005.759\n",
      "    update_time_ms: 11.349\n",
      "  timestamp: 1650284615\n",
      "  timesteps_since_restore: 1216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1216000\n",
      "  training_iteration: 304\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-27-13\n",
      "  done: false\n",
      "  episode_len_mean: 586.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.48\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3313\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6892790794372559\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015134409070014954\n",
      "          model: {}\n",
      "          policy_loss: -0.04731673002243042\n",
      "          total_loss: 0.02621876820921898\n",
      "          vf_explained_var: 0.7546675205230713\n",
      "          vf_loss: 0.0735354945063591\n",
      "    num_agent_steps_sampled: 1220000\n",
      "    num_agent_steps_trained: 1220000\n",
      "    num_steps_sampled: 1220000\n",
      "    num_steps_trained: 1220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.38075601374571\n",
      "    ram_util_percent: 95.8209621993127\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09999810804478099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9471538301189025\n",
      "    mean_inference_ms: 1.9719511292425052\n",
      "    mean_raw_obs_processing_ms: 0.18451962761109073\n",
      "  time_since_restore: 66479.24809098244\n",
      "  time_this_iter_s: 219.87483716011047\n",
      "  time_total_s: 66479.24809098244\n",
      "  timers:\n",
      "    learn_throughput: 18.926\n",
      "    learn_time_ms: 211346.285\n",
      "    load_throughput: 6931588.167\n",
      "    load_time_ms: 0.577\n",
      "    sample_throughput: 18.075\n",
      "    sample_time_ms: 221299.37\n",
      "    update_time_ms: 16.881\n",
      "  timestamp: 1650284833\n",
      "  timesteps_since_restore: 1220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1220000\n",
      "  training_iteration: 305\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-27-16\n",
      "  done: false\n",
      "  episode_len_mean: 1140.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.48\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 690\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128079637885094\n",
      "          total_loss: 0.014128266833722591\n",
      "          vf_explained_var: 4.678644316413738e-08\n",
      "          vf_loss: 1.8597590667468467e-07\n",
      "    num_agent_steps_sampled: 1220000\n",
      "    num_agent_steps_trained: 1220000\n",
      "    num_steps_sampled: 1220000\n",
      "    num_steps_trained: 1220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.77842465753425\n",
      "    ram_util_percent: 95.8376712328767\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10131617919970477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9343398258459525\n",
      "    mean_inference_ms: 1.9901572296720438\n",
      "    mean_raw_obs_processing_ms: 0.12433173170383646\n",
      "  time_since_restore: 66475.51850152016\n",
      "  time_this_iter_s: 220.26614475250244\n",
      "  time_total_s: 66475.51850152016\n",
      "  timers:\n",
      "    learn_throughput: 18.885\n",
      "    learn_time_ms: 211809.465\n",
      "    load_throughput: 7171895.866\n",
      "    load_time_ms: 0.558\n",
      "    sample_throughput: 18.071\n",
      "    sample_time_ms: 221350.845\n",
      "    update_time_ms: 10.735\n",
      "  timestamp: 1650284836\n",
      "  timesteps_since_restore: 1220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1220000\n",
      "  training_iteration: 305\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-30-55\n",
      "  done: false\n",
      "  episode_len_mean: 580.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.33\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3321\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7477630972862244\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015564154833555222\n",
      "          model: {}\n",
      "          policy_loss: -0.05251835659146309\n",
      "          total_loss: 0.02306227758526802\n",
      "          vf_explained_var: 0.7738580107688904\n",
      "          vf_loss: 0.07558062672615051\n",
      "    num_agent_steps_sampled: 1224000\n",
      "    num_agent_steps_trained: 1224000\n",
      "    num_steps_sampled: 1224000\n",
      "    num_steps_trained: 1224000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.98469387755102\n",
      "    ram_util_percent: 95.87653061224489\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09992467983334896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.946628008297749\n",
      "    mean_inference_ms: 1.9696268968402288\n",
      "    mean_raw_obs_processing_ms: 0.1843240371682998\n",
      "  time_since_restore: 66701.34906578064\n",
      "  time_this_iter_s: 222.10097479820251\n",
      "  time_total_s: 66701.34906578064\n",
      "  timers:\n",
      "    learn_throughput: 18.891\n",
      "    learn_time_ms: 211735.879\n",
      "    load_throughput: 6980036.612\n",
      "    load_time_ms: 0.573\n",
      "    sample_throughput: 18.086\n",
      "    sample_time_ms: 221162.662\n",
      "    update_time_ms: 15.733\n",
      "  timestamp: 1650285055\n",
      "  timesteps_since_restore: 1224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1224000\n",
      "  training_iteration: 306\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-30-57\n",
      "  done: false\n",
      "  episode_len_mean: 1236.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 691\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.698201638898979e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.7827609890437576e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0030596121214330196\n",
      "          total_loss: -0.002987532876431942\n",
      "          vf_explained_var: 0.03211909532546997\n",
      "          vf_loss: 7.207797898445278e-05\n",
      "    num_agent_steps_sampled: 1224000\n",
      "    num_agent_steps_trained: 1224000\n",
      "    num_steps_sampled: 1224000\n",
      "    num_steps_trained: 1224000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.3608843537415\n",
      "    ram_util_percent: 95.88877551020407\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10129581208096079\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9342009689458682\n",
      "    mean_inference_ms: 1.9895178606669373\n",
      "    mean_raw_obs_processing_ms: 0.12431525513840316\n",
      "  time_since_restore: 66696.99820446968\n",
      "  time_this_iter_s: 221.47970294952393\n",
      "  time_total_s: 66696.99820446968\n",
      "  timers:\n",
      "    learn_throughput: 18.856\n",
      "    learn_time_ms: 212134.496\n",
      "    load_throughput: 7190029.999\n",
      "    load_time_ms: 0.556\n",
      "    sample_throughput: 18.074\n",
      "    sample_time_ms: 221317.153\n",
      "    update_time_ms: 9.663\n",
      "  timestamp: 1650285057\n",
      "  timesteps_since_restore: 1224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1224000\n",
      "  training_iteration: 306\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-34-37\n",
      "  done: false\n",
      "  episode_len_mean: 577.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3328\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6797515749931335\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01873619668185711\n",
      "          model: {}\n",
      "          policy_loss: -0.050384633243083954\n",
      "          total_loss: 0.009549614042043686\n",
      "          vf_explained_var: 0.7942929863929749\n",
      "          vf_loss: 0.05993424355983734\n",
      "    num_agent_steps_sampled: 1228000\n",
      "    num_agent_steps_trained: 1228000\n",
      "    num_steps_sampled: 1228000\n",
      "    num_steps_trained: 1228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.81122448979592\n",
      "    ram_util_percent: 95.93333333333332\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09986322585699757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9461803358233354\n",
      "    mean_inference_ms: 1.967651516033867\n",
      "    mean_raw_obs_processing_ms: 0.18415545690720042\n",
      "  time_since_restore: 66923.1680316925\n",
      "  time_this_iter_s: 221.81896591186523\n",
      "  time_total_s: 66923.1680316925\n",
      "  timers:\n",
      "    learn_throughput: 18.932\n",
      "    learn_time_ms: 211278.935\n",
      "    load_throughput: 6921008.209\n",
      "    load_time_ms: 0.578\n",
      "    sample_throughput: 18.049\n",
      "    sample_time_ms: 221614.558\n",
      "    update_time_ms: 14.302\n",
      "  timestamp: 1650285277\n",
      "  timesteps_since_restore: 1228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1228000\n",
      "  training_iteration: 307\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-34-39\n",
      "  done: false\n",
      "  episode_len_mean: 1236.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 693\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.348524991305962e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.5533699670392025e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0016309681814163923\n",
      "          total_loss: 0.0017550953198224306\n",
      "          vf_explained_var: -0.06652045249938965\n",
      "          vf_loss: 0.0001241284335264936\n",
      "    num_agent_steps_sampled: 1228000\n",
      "    num_agent_steps_trained: 1228000\n",
      "    num_steps_sampled: 1228000\n",
      "    num_steps_trained: 1228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.1259385665529\n",
      "    ram_util_percent: 95.93720136518769\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10125395635580878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9339152053864044\n",
      "    mean_inference_ms: 1.98820317163561\n",
      "    mean_raw_obs_processing_ms: 0.12428140992959699\n",
      "  time_since_restore: 66919.0731754303\n",
      "  time_this_iter_s: 222.07497096061707\n",
      "  time_total_s: 66919.0731754303\n",
      "  timers:\n",
      "    learn_throughput: 18.888\n",
      "    learn_time_ms: 211772.267\n",
      "    load_throughput: 7254384.918\n",
      "    load_time_ms: 0.551\n",
      "    sample_throughput: 18.041\n",
      "    sample_time_ms: 221712.76\n",
      "    update_time_ms: 8.625\n",
      "  timestamp: 1650285279\n",
      "  timesteps_since_restore: 1228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1228000\n",
      "  training_iteration: 307\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 13:35:52 (running for 18:37:12.44)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=8.26 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-38-19\n",
      "  done: false\n",
      "  episode_len_mean: 589.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.6\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3335\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.579660177230835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017276788130402565\n",
      "          model: {}\n",
      "          policy_loss: -0.04543031379580498\n",
      "          total_loss: 0.05095675215125084\n",
      "          vf_explained_var: 0.7109979391098022\n",
      "          vf_loss: 0.09638706594705582\n",
      "    num_agent_steps_sampled: 1232000\n",
      "    num_agent_steps_trained: 1232000\n",
      "    num_steps_sampled: 1232000\n",
      "    num_steps_trained: 1232000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.79180887372014\n",
      "    ram_util_percent: 95.85767918088736\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09980112659521924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9457316130932983\n",
      "    mean_inference_ms: 1.965674141698046\n",
      "    mean_raw_obs_processing_ms: 0.1839811557286307\n",
      "  time_since_restore: 67144.35602664948\n",
      "  time_this_iter_s: 221.18799495697021\n",
      "  time_total_s: 67144.35602664948\n",
      "  timers:\n",
      "    learn_throughput: 18.948\n",
      "    learn_time_ms: 211107.633\n",
      "    load_throughput: 7313520.488\n",
      "    load_time_ms: 0.547\n",
      "    sample_throughput: 18.079\n",
      "    sample_time_ms: 221252.727\n",
      "    update_time_ms: 15.117\n",
      "  timestamp: 1650285499\n",
      "  timesteps_since_restore: 1232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1232000\n",
      "  training_iteration: 308\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-38-20\n",
      "  done: false\n",
      "  episode_len_mean: 1236.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 693\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128079637885094\n",
      "          total_loss: 0.014128287322819233\n",
      "          vf_explained_var: 1.7490438608547265e-07\n",
      "          vf_loss: 2.091965001227436e-07\n",
      "    num_agent_steps_sampled: 1232000\n",
      "    num_agent_steps_trained: 1232000\n",
      "    num_steps_sampled: 1232000\n",
      "    num_steps_trained: 1232000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.77986348122867\n",
      "    ram_util_percent: 95.8409556313993\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10125395635580878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9339152053864044\n",
      "    mean_inference_ms: 1.98820317163561\n",
      "    mean_raw_obs_processing_ms: 0.12428140992959699\n",
      "  time_since_restore: 67139.67580723763\n",
      "  time_this_iter_s: 220.60263180732727\n",
      "  time_total_s: 67139.67580723763\n",
      "  timers:\n",
      "    learn_throughput: 18.907\n",
      "    learn_time_ms: 211556.401\n",
      "    load_throughput: 7037127.637\n",
      "    load_time_ms: 0.568\n",
      "    sample_throughput: 18.065\n",
      "    sample_time_ms: 221428.759\n",
      "    update_time_ms: 8.075\n",
      "  timestamp: 1650285500\n",
      "  timesteps_since_restore: 1232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1232000\n",
      "  training_iteration: 308\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-42-00\n",
      "  done: false\n",
      "  episode_len_mean: 586.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.51\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3343\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8108416199684143\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012968692928552628\n",
      "          model: {}\n",
      "          policy_loss: -0.04785113409161568\n",
      "          total_loss: 0.023938164114952087\n",
      "          vf_explained_var: 0.730802059173584\n",
      "          vf_loss: 0.07178930193185806\n",
      "    num_agent_steps_sampled: 1236000\n",
      "    num_agent_steps_trained: 1236000\n",
      "    num_steps_sampled: 1236000\n",
      "    num_steps_trained: 1236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.98333333333333\n",
      "    ram_util_percent: 95.79557823129252\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09973163312407124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.945222131328855\n",
      "    mean_inference_ms: 1.9634557514676232\n",
      "    mean_raw_obs_processing_ms: 0.18378281632271673\n",
      "  time_since_restore: 67365.6940715313\n",
      "  time_this_iter_s: 221.33804488182068\n",
      "  time_total_s: 67365.6940715313\n",
      "  timers:\n",
      "    learn_throughput: 18.953\n",
      "    learn_time_ms: 211045.781\n",
      "    load_throughput: 7356814.734\n",
      "    load_time_ms: 0.544\n",
      "    sample_throughput: 18.087\n",
      "    sample_time_ms: 221154.817\n",
      "    update_time_ms: 17.639\n",
      "  timestamp: 1650285720\n",
      "  timesteps_since_restore: 1236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1236000\n",
      "  training_iteration: 309\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-42-02\n",
      "  done: false\n",
      "  episode_len_mean: 1235.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 694\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.109223449551961e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.864713105546921e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0005880970857106149\n",
      "          total_loss: -0.0005852071335539222\n",
      "          vf_explained_var: 0.003064296906813979\n",
      "          vf_loss: 2.8867316359537654e-06\n",
      "    num_agent_steps_sampled: 1236000\n",
      "    num_agent_steps_trained: 1236000\n",
      "    num_steps_sampled: 1236000\n",
      "    num_steps_trained: 1236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.3612244897959\n",
      "    ram_util_percent: 95.77721088435374\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10123209978811461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9337645515136838\n",
      "    mean_inference_ms: 1.9875101009844929\n",
      "    mean_raw_obs_processing_ms: 0.12426234372235574\n",
      "  time_since_restore: 67361.47068715096\n",
      "  time_this_iter_s: 221.79487991333008\n",
      "  time_total_s: 67361.47068715096\n",
      "  timers:\n",
      "    learn_throughput: 18.902\n",
      "    learn_time_ms: 211619.049\n",
      "    load_throughput: 6430762.39\n",
      "    load_time_ms: 0.622\n",
      "    sample_throughput: 18.077\n",
      "    sample_time_ms: 221271.183\n",
      "    update_time_ms: 7.68\n",
      "  timestamp: 1650285722\n",
      "  timesteps_since_restore: 1236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1236000\n",
      "  training_iteration: 309\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 586.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.59\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3352\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7524018883705139\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014462613500654697\n",
      "          model: {}\n",
      "          policy_loss: -0.05265410989522934\n",
      "          total_loss: 0.04237421602010727\n",
      "          vf_explained_var: 0.619143009185791\n",
      "          vf_loss: 0.09502832591533661\n",
      "    num_agent_steps_sampled: 1240000\n",
      "    num_agent_steps_trained: 1240000\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.20466666666665\n",
      "    ram_util_percent: 95.68\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09965635857167605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9446632783497318\n",
      "    mean_inference_ms: 1.9610227198738754\n",
      "    mean_raw_obs_processing_ms: 0.18356366985452724\n",
      "  time_since_restore: 67591.71676850319\n",
      "  time_this_iter_s: 226.0226969718933\n",
      "  time_total_s: 67591.71676850319\n",
      "  timers:\n",
      "    learn_throughput: 18.905\n",
      "    learn_time_ms: 211582.922\n",
      "    load_throughput: 7318625.022\n",
      "    load_time_ms: 0.547\n",
      "    sample_throughput: 18.086\n",
      "    sample_time_ms: 221160.491\n",
      "    update_time_ms: 22.307\n",
      "  timestamp: 1650285946\n",
      "  timesteps_since_restore: 1240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 310\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 1235.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 694\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128251932561398\n",
      "          total_loss: -0.014127831906080246\n",
      "          vf_explained_var: 6.184782819218526e-08\n",
      "          vf_loss: 4.1759810187613766e-07\n",
      "    num_agent_steps_sampled: 1240000\n",
      "    num_agent_steps_trained: 1240000\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.56086956521739\n",
      "    ram_util_percent: 95.66153846153846\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10123209978811461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9337645515136838\n",
      "    mean_inference_ms: 1.9875101009844929\n",
      "    mean_raw_obs_processing_ms: 0.12426234372235574\n",
      "  time_since_restore: 67587.11164212227\n",
      "  time_this_iter_s: 225.64095497131348\n",
      "  time_total_s: 67587.11164212227\n",
      "  timers:\n",
      "    learn_throughput: 18.862\n",
      "    learn_time_ms: 212066.765\n",
      "    load_throughput: 6410123.41\n",
      "    load_time_ms: 0.624\n",
      "    sample_throughput: 18.072\n",
      "    sample_time_ms: 221342.739\n",
      "    update_time_ms: 8.689\n",
      "  timestamp: 1650285947\n",
      "  timesteps_since_restore: 1240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 310\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-49-30\n",
      "  done: false\n",
      "  episode_len_mean: 574.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3360\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.802475094795227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014151561073958874\n",
      "          model: {}\n",
      "          policy_loss: -0.04652705043554306\n",
      "          total_loss: 0.026501700282096863\n",
      "          vf_explained_var: 0.7469400763511658\n",
      "          vf_loss: 0.07302875071763992\n",
      "    num_agent_steps_sampled: 1244000\n",
      "    num_agent_steps_trained: 1244000\n",
      "    num_steps_sampled: 1244000\n",
      "    num_steps_trained: 1244000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.81722972972973\n",
      "    ram_util_percent: 95.91351351351351\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09959167889437735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9441755739938378\n",
      "    mean_inference_ms: 1.9589405525137658\n",
      "    mean_raw_obs_processing_ms: 0.18337566584685397\n",
      "  time_since_restore: 67815.02614951134\n",
      "  time_this_iter_s: 223.3093810081482\n",
      "  time_total_s: 67815.02614951134\n",
      "  timers:\n",
      "    learn_throughput: 18.899\n",
      "    learn_time_ms: 211648.047\n",
      "    load_throughput: 7443638.138\n",
      "    load_time_ms: 0.537\n",
      "    sample_throughput: 18.035\n",
      "    sample_time_ms: 221787.658\n",
      "    update_time_ms: 24.367\n",
      "  timestamp: 1650286170\n",
      "  timesteps_since_restore: 1244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1244000\n",
      "  training_iteration: 311\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-49-30\n",
      "  done: false\n",
      "  episode_len_mean: 1235.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 694\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014128196984529495\n",
      "          total_loss: -0.014128018170595169\n",
      "          vf_explained_var: 1.5920208795705548e-07\n",
      "          vf_loss: 1.8352484687511605e-07\n",
      "    num_agent_steps_sampled: 1244000\n",
      "    num_agent_steps_trained: 1244000\n",
      "    num_steps_sampled: 1244000\n",
      "    num_steps_trained: 1244000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.64319727891156\n",
      "    ram_util_percent: 95.92619047619046\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10123209978811461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9337645515136838\n",
      "    mean_inference_ms: 1.9875101009844929\n",
      "    mean_raw_obs_processing_ms: 0.12426234372235574\n",
      "  time_since_restore: 67809.28620815277\n",
      "  time_this_iter_s: 222.17456603050232\n",
      "  time_total_s: 67809.28620815277\n",
      "  timers:\n",
      "    learn_throughput: 18.866\n",
      "    learn_time_ms: 212019.56\n",
      "    load_throughput: 6222772.152\n",
      "    load_time_ms: 0.643\n",
      "    sample_throughput: 18.032\n",
      "    sample_time_ms: 221826.131\n",
      "    update_time_ms: 8.634\n",
      "  timestamp: 1650286170\n",
      "  timesteps_since_restore: 1244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1244000\n",
      "  training_iteration: 311\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 13:52:33 (running for 18:53:53.18)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=8.35 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-53-11\n",
      "  done: false\n",
      "  episode_len_mean: 568.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.11\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3368\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7259002923965454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014917506836354733\n",
      "          model: {}\n",
      "          policy_loss: -0.06216692179441452\n",
      "          total_loss: 0.0033987516071647406\n",
      "          vf_explained_var: 0.7507884502410889\n",
      "          vf_loss: 0.06556568294763565\n",
      "    num_agent_steps_sampled: 1248000\n",
      "    num_agent_steps_trained: 1248000\n",
      "    num_steps_sampled: 1248000\n",
      "    num_steps_trained: 1248000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.53344709897611\n",
      "    ram_util_percent: 95.9310580204778\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09952884477129814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9436966886828856\n",
      "    mean_inference_ms: 1.9568923512033367\n",
      "    mean_raw_obs_processing_ms: 0.18319258859229343\n",
      "  time_since_restore: 68036.36074471474\n",
      "  time_this_iter_s: 221.33459520339966\n",
      "  time_total_s: 68036.36074471474\n",
      "  timers:\n",
      "    learn_throughput: 18.895\n",
      "    learn_time_ms: 211694.49\n",
      "    load_throughput: 7425189.644\n",
      "    load_time_ms: 0.539\n",
      "    sample_throughput: 18.033\n",
      "    sample_time_ms: 221819.949\n",
      "    update_time_ms: 25.132\n",
      "  timestamp: 1650286391\n",
      "  timesteps_since_restore: 1248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1248000\n",
      "  training_iteration: 312\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-53-12\n",
      "  done: false\n",
      "  episode_len_mean: 1429.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 699\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.689272580192903e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.2530595182179064e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0004749986401293427\n",
      "          total_loss: -0.00031601061346009374\n",
      "          vf_explained_var: 0.11742130666971207\n",
      "          vf_loss: 0.00015899066056590527\n",
      "    num_agent_steps_sampled: 1248000\n",
      "    num_agent_steps_trained: 1248000\n",
      "    num_steps_sampled: 1248000\n",
      "    num_steps_trained: 1248000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.44285714285714\n",
      "    ram_util_percent: 95.92653061224489\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10111716146720434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9329628789916405\n",
      "    mean_inference_ms: 1.983828905357134\n",
      "    mean_raw_obs_processing_ms: 0.12414244570084826\n",
      "  time_since_restore: 68031.30681324005\n",
      "  time_this_iter_s: 222.02060508728027\n",
      "  time_total_s: 68031.30681324005\n",
      "  timers:\n",
      "    learn_throughput: 18.861\n",
      "    learn_time_ms: 212077.366\n",
      "    load_throughput: 6247101.579\n",
      "    load_time_ms: 0.64\n",
      "    sample_throughput: 18.036\n",
      "    sample_time_ms: 221775.158\n",
      "    update_time_ms: 8.778\n",
      "  timestamp: 1650286392\n",
      "  timesteps_since_restore: 1248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1248000\n",
      "  training_iteration: 312\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-56-53\n",
      "  done: false\n",
      "  episode_len_mean: 562.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3376\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7559584975242615\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015491998754441738\n",
      "          model: {}\n",
      "          policy_loss: -0.040356822311878204\n",
      "          total_loss: 0.04177402704954147\n",
      "          vf_explained_var: 0.7126811146736145\n",
      "          vf_loss: 0.08213084936141968\n",
      "    num_agent_steps_sampled: 1252000\n",
      "    num_agent_steps_trained: 1252000\n",
      "    num_steps_sampled: 1252000\n",
      "    num_steps_trained: 1252000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.11023890784983\n",
      "    ram_util_percent: 95.9481228668942\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09946761308293178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9432266551014892\n",
      "    mean_inference_ms: 1.9548897975840864\n",
      "    mean_raw_obs_processing_ms: 0.1830155590964354\n",
      "  time_since_restore: 68258.67233991623\n",
      "  time_this_iter_s: 222.3115952014923\n",
      "  time_total_s: 68258.67233991623\n",
      "  timers:\n",
      "    learn_throughput: 18.878\n",
      "    learn_time_ms: 211887.466\n",
      "    load_throughput: 7414360.969\n",
      "    load_time_ms: 0.539\n",
      "    sample_throughput: 18.03\n",
      "    sample_time_ms: 221857.019\n",
      "    update_time_ms: 24.863\n",
      "  timestamp: 1650286613\n",
      "  timesteps_since_restore: 1252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1252000\n",
      "  training_iteration: 313\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_13-56-54\n",
      "  done: false\n",
      "  episode_len_mean: 1429.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 699\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128298498690128\n",
      "          total_loss: 0.014128516428172588\n",
      "          vf_explained_var: 3.651265103599144e-07\n",
      "          vf_loss: 2.2111193231921789e-07\n",
      "    num_agent_steps_sampled: 1252000\n",
      "    num_agent_steps_trained: 1252000\n",
      "    num_steps_sampled: 1252000\n",
      "    num_steps_trained: 1252000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.11734693877551\n",
      "    ram_util_percent: 95.93843537414966\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10111716146720434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9329628789916405\n",
      "    mean_inference_ms: 1.983828905357134\n",
      "    mean_raw_obs_processing_ms: 0.12414244570084826\n",
      "  time_since_restore: 68253.75653624535\n",
      "  time_this_iter_s: 222.4497230052948\n",
      "  time_total_s: 68253.75653624535\n",
      "  timers:\n",
      "    learn_throughput: 18.842\n",
      "    learn_time_ms: 212293.769\n",
      "    load_throughput: 6194054.493\n",
      "    load_time_ms: 0.646\n",
      "    sample_throughput: 18.038\n",
      "    sample_time_ms: 221753.381\n",
      "    update_time_ms: 8.712\n",
      "  timestamp: 1650286614\n",
      "  timesteps_since_restore: 1252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1252000\n",
      "  training_iteration: 313\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-00-36\n",
      "  done: false\n",
      "  episode_len_mean: 565.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 8.03\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3383\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7482684850692749\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014437097124755383\n",
      "          model: {}\n",
      "          policy_loss: -0.047462377697229385\n",
      "          total_loss: 0.016323110088706017\n",
      "          vf_explained_var: 0.7723596096038818\n",
      "          vf_loss: 0.06378548592329025\n",
      "    num_agent_steps_sampled: 1256000\n",
      "    num_agent_steps_trained: 1256000\n",
      "    num_steps_sampled: 1256000\n",
      "    num_steps_trained: 1256000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.84440677966101\n",
      "    ram_util_percent: 95.7457627118644\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09941528793608004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.942825332366127\n",
      "    mean_inference_ms: 1.9531998817997553\n",
      "    mean_raw_obs_processing_ms: 0.18286627073635434\n",
      "  time_since_restore: 68481.30507707596\n",
      "  time_this_iter_s: 222.632737159729\n",
      "  time_total_s: 68481.30507707596\n",
      "  timers:\n",
      "    learn_throughput: 18.852\n",
      "    learn_time_ms: 212174.529\n",
      "    load_throughput: 7369093.864\n",
      "    load_time_ms: 0.543\n",
      "    sample_throughput: 18.017\n",
      "    sample_time_ms: 222012.823\n",
      "    update_time_ms: 24.713\n",
      "  timestamp: 1650286836\n",
      "  timesteps_since_restore: 1256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1256000\n",
      "  training_iteration: 314\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-00-37\n",
      "  done: false\n",
      "  episode_len_mean: 1523.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 702\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.075147844039393e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2169092353827747e-27\n",
      "          model: {}\n",
      "          policy_loss: -8.329909064741514e-07\n",
      "          total_loss: 2.994508577103261e-05\n",
      "          vf_explained_var: -0.11566480994224548\n",
      "          vf_loss: 3.077440123888664e-05\n",
      "    num_agent_steps_sampled: 1256000\n",
      "    num_agent_steps_trained: 1256000\n",
      "    num_steps_sampled: 1256000\n",
      "    num_steps_trained: 1256000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.12983050847456\n",
      "    ram_util_percent: 95.76745762711863\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10104450118234862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9324542132981959\n",
      "    mean_inference_ms: 1.9814908664782482\n",
      "    mean_raw_obs_processing_ms: 0.12406501801627642\n",
      "  time_since_restore: 68476.70955610275\n",
      "  time_this_iter_s: 222.95301985740662\n",
      "  time_total_s: 68476.70955610275\n",
      "  timers:\n",
      "    learn_throughput: 18.815\n",
      "    learn_time_ms: 212595.399\n",
      "    load_throughput: 6229010.173\n",
      "    load_time_ms: 0.642\n",
      "    sample_throughput: 18.027\n",
      "    sample_time_ms: 221890.554\n",
      "    update_time_ms: 8.569\n",
      "  timestamp: 1650286837\n",
      "  timesteps_since_restore: 1256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1256000\n",
      "  training_iteration: 314\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-04-19\n",
      "  done: false\n",
      "  episode_len_mean: 553.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 7.79\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3392\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7287444472312927\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014880869537591934\n",
      "          model: {}\n",
      "          policy_loss: -0.03969595208764076\n",
      "          total_loss: 0.04282326251268387\n",
      "          vf_explained_var: 0.6918440461158752\n",
      "          vf_loss: 0.08251920342445374\n",
      "    num_agent_steps_sampled: 1260000\n",
      "    num_agent_steps_trained: 1260000\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.6\n",
      "    ram_util_percent: 95.80844594594593\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09934794254862687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9423127477976553\n",
      "    mean_inference_ms: 1.9510554219712417\n",
      "    mean_raw_obs_processing_ms: 0.18267676013172973\n",
      "  time_since_restore: 68704.51060891151\n",
      "  time_this_iter_s: 223.20553183555603\n",
      "  time_total_s: 68704.51060891151\n",
      "  timers:\n",
      "    learn_throughput: 18.819\n",
      "    learn_time_ms: 212551.834\n",
      "    load_throughput: 7411740.59\n",
      "    load_time_ms: 0.54\n",
      "    sample_throughput: 17.998\n",
      "    sample_time_ms: 222253.018\n",
      "    update_time_ms: 24.445\n",
      "  timestamp: 1650287059\n",
      "  timesteps_since_restore: 1260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 315\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-04-20\n",
      "  done: false\n",
      "  episode_len_mean: 1523.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 702\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.174468971152284e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -6.297684606813331e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128092676401138\n",
      "          total_loss: 0.014128485694527626\n",
      "          vf_explained_var: -9.543152401647603e-08\n",
      "          vf_loss: 3.9310938859671296e-07\n",
      "    num_agent_steps_sampled: 1260000\n",
      "    num_agent_steps_trained: 1260000\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.66101694915254\n",
      "    ram_util_percent: 95.79694915254237\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10104450118234862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9324542132981959\n",
      "    mean_inference_ms: 1.9814908664782482\n",
      "    mean_raw_obs_processing_ms: 0.12406501801627642\n",
      "  time_since_restore: 68699.26948809624\n",
      "  time_this_iter_s: 222.5599319934845\n",
      "  time_total_s: 68699.26948809624\n",
      "  timers:\n",
      "    learn_throughput: 18.791\n",
      "    learn_time_ms: 212864.178\n",
      "    load_throughput: 5621449.489\n",
      "    load_time_ms: 0.712\n",
      "    sample_throughput: 18.006\n",
      "    sample_time_ms: 222151.622\n",
      "    update_time_ms: 9.827\n",
      "  timestamp: 1650287060\n",
      "  timesteps_since_restore: 1260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 315\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 542.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 20.0\n",
      "  episode_reward_mean: 7.53\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3401\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7913564443588257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017251770943403244\n",
      "          model: {}\n",
      "          policy_loss: -0.04739682003855705\n",
      "          total_loss: 0.041199564933776855\n",
      "          vf_explained_var: 0.6399622559547424\n",
      "          vf_loss: 0.0885963886976242\n",
      "    num_agent_steps_sampled: 1264000\n",
      "    num_agent_steps_trained: 1264000\n",
      "    num_steps_sampled: 1264000\n",
      "    num_steps_trained: 1264000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.3627986348123\n",
      "    ram_util_percent: 95.86211604095563\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09928282561180554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9418125842989372\n",
      "    mean_inference_ms: 1.9489680510839293\n",
      "    mean_raw_obs_processing_ms: 0.18249512851513075\n",
      "  time_since_restore: 68925.10982775688\n",
      "  time_this_iter_s: 220.59921884536743\n",
      "  time_total_s: 68925.10982775688\n",
      "  timers:\n",
      "    learn_throughput: 18.822\n",
      "    learn_time_ms: 212519.309\n",
      "    load_throughput: 7414360.969\n",
      "    load_time_ms: 0.539\n",
      "    sample_throughput: 17.976\n",
      "    sample_time_ms: 222519.018\n",
      "    update_time_ms: 23.363\n",
      "  timestamp: 1650287280\n",
      "  timesteps_since_restore: 1264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1264000\n",
      "  training_iteration: 316\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 1523.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 702\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.174468971152284e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -6.297684606813331e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128110371530056\n",
      "          total_loss: 0.014128129929304123\n",
      "          vf_explained_var: 1.49569200402766e-06\n",
      "          vf_loss: 2.9680968793854845e-08\n",
      "    num_agent_steps_sampled: 1264000\n",
      "    num_agent_steps_trained: 1264000\n",
      "    num_steps_sampled: 1264000\n",
      "    num_steps_trained: 1264000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.90955631399318\n",
      "    ram_util_percent: 95.88703071672353\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10104450118234862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9324542132981959\n",
      "    mean_inference_ms: 1.9814908664782482\n",
      "    mean_raw_obs_processing_ms: 0.12406501801627642\n",
      "  time_since_restore: 68919.74022912979\n",
      "  time_this_iter_s: 220.47074103355408\n",
      "  time_total_s: 68919.74022912979\n",
      "  timers:\n",
      "    learn_throughput: 18.79\n",
      "    learn_time_ms: 212878.588\n",
      "    load_throughput: 5743261.673\n",
      "    load_time_ms: 0.696\n",
      "    sample_throughput: 17.993\n",
      "    sample_time_ms: 222305.818\n",
      "    update_time_ms: 10.093\n",
      "  timestamp: 1650287280\n",
      "  timesteps_since_restore: 1264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1264000\n",
      "  training_iteration: 316\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 14:09:13 (running for 19:10:33.46)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.53 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-11-41\n",
      "  done: false\n",
      "  episode_len_mean: 531.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3409\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7031684517860413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015599537640810013\n",
      "          model: {}\n",
      "          policy_loss: -0.05467879772186279\n",
      "          total_loss: 0.01862509734928608\n",
      "          vf_explained_var: 0.7424340844154358\n",
      "          vf_loss: 0.07330389320850372\n",
      "    num_agent_steps_sampled: 1268000\n",
      "    num_agent_steps_trained: 1268000\n",
      "    num_steps_sampled: 1268000\n",
      "    num_steps_trained: 1268000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.2530612244898\n",
      "    ram_util_percent: 95.83571428571427\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09922557301120868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9413729557492766\n",
      "    mean_inference_ms: 1.9471428075319215\n",
      "    mean_raw_obs_processing_ms: 0.1823401475620443\n",
      "  time_since_restore: 69146.43129873276\n",
      "  time_this_iter_s: 221.32147097587585\n",
      "  time_total_s: 69146.43129873276\n",
      "  timers:\n",
      "    learn_throughput: 18.821\n",
      "    learn_time_ms: 212528.227\n",
      "    load_throughput: 7074218.249\n",
      "    load_time_ms: 0.565\n",
      "    sample_throughput: 17.984\n",
      "    sample_time_ms: 222424.304\n",
      "    update_time_ms: 23.674\n",
      "  timestamp: 1650287501\n",
      "  timesteps_since_restore: 1268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1268000\n",
      "  training_iteration: 317\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-11-42\n",
      "  done: false\n",
      "  episode_len_mean: 1620.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 703\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1896237566159736e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.3203545421707108e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0029871242586523294\n",
      "          total_loss: 0.0029968128073960543\n",
      "          vf_explained_var: -0.035512421280145645\n",
      "          vf_loss: 9.684498763817828e-06\n",
      "    num_agent_steps_sampled: 1268000\n",
      "    num_agent_steps_trained: 1268000\n",
      "    num_steps_sampled: 1268000\n",
      "    num_steps_trained: 1268000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.64266211604095\n",
      "    ram_util_percent: 95.82423208191125\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10101853820944635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9322710659633464\n",
      "    mean_inference_ms: 1.9806488755035292\n",
      "    mean_raw_obs_processing_ms: 0.1240357892521444\n",
      "  time_since_restore: 69140.93607020378\n",
      "  time_this_iter_s: 221.19584107398987\n",
      "  time_total_s: 69140.93607020378\n",
      "  timers:\n",
      "    learn_throughput: 18.79\n",
      "    learn_time_ms: 212876.634\n",
      "    load_throughput: 5842259.289\n",
      "    load_time_ms: 0.685\n",
      "    sample_throughput: 17.999\n",
      "    sample_time_ms: 222238.046\n",
      "    update_time_ms: 10.03\n",
      "  timestamp: 1650287502\n",
      "  timesteps_since_restore: 1268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1268000\n",
      "  training_iteration: 317\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-15-23\n",
      "  done: false\n",
      "  episode_len_mean: 538.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3416\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8340312242507935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013802514411509037\n",
      "          model: {}\n",
      "          policy_loss: -0.05068954452872276\n",
      "          total_loss: 0.014255399815738201\n",
      "          vf_explained_var: 0.7773223519325256\n",
      "          vf_loss: 0.06494495272636414\n",
      "    num_agent_steps_sampled: 1272000\n",
      "    num_agent_steps_trained: 1272000\n",
      "    num_steps_sampled: 1272000\n",
      "    num_steps_trained: 1272000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.10680272108844\n",
      "    ram_util_percent: 95.67414965986394\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09917430222385884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9409790934434998\n",
      "    mean_inference_ms: 1.9455155750336115\n",
      "    mean_raw_obs_processing_ms: 0.18220459498368258\n",
      "  time_since_restore: 69367.85999584198\n",
      "  time_this_iter_s: 221.4286971092224\n",
      "  time_total_s: 69367.85999584198\n",
      "  timers:\n",
      "    learn_throughput: 18.811\n",
      "    learn_time_ms: 212644.799\n",
      "    load_throughput: 7064090.947\n",
      "    load_time_ms: 0.566\n",
      "    sample_throughput: 17.991\n",
      "    sample_time_ms: 222337.048\n",
      "    update_time_ms: 23.712\n",
      "  timestamp: 1650287723\n",
      "  timesteps_since_restore: 1272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1272000\n",
      "  training_iteration: 318\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-15-24\n",
      "  done: false\n",
      "  episode_len_mean: 1620.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 703\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128190465271473\n",
      "          total_loss: 0.014128213748335838\n",
      "          vf_explained_var: 7.580685519315011e-07\n",
      "          vf_loss: 3.004532089789791e-08\n",
      "    num_agent_steps_sampled: 1272000\n",
      "    num_agent_steps_trained: 1272000\n",
      "    num_steps_sampled: 1272000\n",
      "    num_steps_trained: 1272000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.96644067796609\n",
      "    ram_util_percent: 95.67593220338983\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10101853820944635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9322710659633464\n",
      "    mean_inference_ms: 1.9806488755035292\n",
      "    mean_raw_obs_processing_ms: 0.1240357892521444\n",
      "  time_since_restore: 69363.29467511177\n",
      "  time_this_iter_s: 222.3586049079895\n",
      "  time_total_s: 69363.29467511177\n",
      "  timers:\n",
      "    learn_throughput: 18.767\n",
      "    learn_time_ms: 213136.941\n",
      "    load_throughput: 5928763.87\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 18.006\n",
      "    sample_time_ms: 222151.602\n",
      "    update_time_ms: 10.09\n",
      "  timestamp: 1650287724\n",
      "  timesteps_since_restore: 1272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1272000\n",
      "  training_iteration: 318\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-19-04\n",
      "  done: false\n",
      "  episode_len_mean: 531.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3424\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7119448184967041\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01579601690173149\n",
      "          model: {}\n",
      "          policy_loss: -0.04356546700000763\n",
      "          total_loss: 0.021517649292945862\n",
      "          vf_explained_var: 0.753808319568634\n",
      "          vf_loss: 0.06508312374353409\n",
      "    num_agent_steps_sampled: 1276000\n",
      "    num_agent_steps_trained: 1276000\n",
      "    num_steps_sampled: 1276000\n",
      "    num_steps_trained: 1276000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.64115646258503\n",
      "    ram_util_percent: 95.8142857142857\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09911494113922024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9405219590958626\n",
      "    mean_inference_ms: 1.9436229091783477\n",
      "    mean_raw_obs_processing_ms: 0.18204865227788772\n",
      "  time_since_restore: 69588.8435177803\n",
      "  time_this_iter_s: 220.98352193832397\n",
      "  time_total_s: 69588.8435177803\n",
      "  timers:\n",
      "    learn_throughput: 18.81\n",
      "    learn_time_ms: 212649.632\n",
      "    load_throughput: 7086469.271\n",
      "    load_time_ms: 0.564\n",
      "    sample_throughput: 17.985\n",
      "    sample_time_ms: 222411.187\n",
      "    update_time_ms: 24.02\n",
      "  timestamp: 1650287944\n",
      "  timesteps_since_restore: 1276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1276000\n",
      "  training_iteration: 319\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-19-05\n",
      "  done: false\n",
      "  episode_len_mean: 1716.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 707\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.3638528090380178e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.0805086141614553e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0002952535869553685\n",
      "          total_loss: 0.0003411710204090923\n",
      "          vf_explained_var: -0.16129015386104584\n",
      "          vf_loss: 4.591186734614894e-05\n",
      "    num_agent_steps_sampled: 1276000\n",
      "    num_agent_steps_trained: 1276000\n",
      "    num_steps_sampled: 1276000\n",
      "    num_steps_trained: 1276000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.97244897959185\n",
      "    ram_util_percent: 95.81088435374147\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10090980678861597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9315033280666873\n",
      "    mean_inference_ms: 1.977123178382601\n",
      "    mean_raw_obs_processing_ms: 0.12391368739408612\n",
      "  time_since_restore: 69584.3398361206\n",
      "  time_this_iter_s: 221.04516100883484\n",
      "  time_total_s: 69584.3398361206\n",
      "  timers:\n",
      "    learn_throughput: 18.772\n",
      "    learn_time_ms: 213081.917\n",
      "    load_throughput: 6389859.842\n",
      "    load_time_ms: 0.626\n",
      "    sample_throughput: 17.986\n",
      "    sample_time_ms: 222394.268\n",
      "    update_time_ms: 9.864\n",
      "  timestamp: 1650287945\n",
      "  timesteps_since_restore: 1276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1276000\n",
      "  training_iteration: 319\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-22-46\n",
      "  done: false\n",
      "  episode_len_mean: 524.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.05\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3433\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8049702048301697\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01497886423021555\n",
      "          model: {}\n",
      "          policy_loss: -0.04807230457663536\n",
      "          total_loss: 0.03489460423588753\n",
      "          vf_explained_var: 0.7535663843154907\n",
      "          vf_loss: 0.08296690136194229\n",
      "    num_agent_steps_sampled: 1280000\n",
      "    num_agent_steps_trained: 1280000\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.28122866894198\n",
      "    ram_util_percent: 95.8938566552901\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0990478002781589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9400077842313982\n",
      "    mean_inference_ms: 1.9415150945565398\n",
      "    mean_raw_obs_processing_ms: 0.18188104665246738\n",
      "  time_since_restore: 69810.26216483116\n",
      "  time_this_iter_s: 221.41864705085754\n",
      "  time_total_s: 69810.26216483116\n",
      "  timers:\n",
      "    learn_throughput: 18.847\n",
      "    learn_time_ms: 212239.094\n",
      "    load_throughput: 7062901.406\n",
      "    load_time_ms: 0.566\n",
      "    sample_throughput: 17.988\n",
      "    sample_time_ms: 222368.775\n",
      "    update_time_ms: 20.984\n",
      "  timestamp: 1650288166\n",
      "  timesteps_since_restore: 1280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 320\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-22-47\n",
      "  done: false\n",
      "  episode_len_mean: 1716.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 707\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.010639471933245659\n",
      "          total_loss: -0.010639469139277935\n",
      "          vf_explained_var: -1.5836890270293225e-06\n",
      "          vf_loss: 7.893646625234396e-09\n",
      "    num_agent_steps_sampled: 1280000\n",
      "    num_agent_steps_trained: 1280000\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.99317406143345\n",
      "    ram_util_percent: 95.91706484641637\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10090980678861597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9315033280666873\n",
      "    mean_inference_ms: 1.977123178382601\n",
      "    mean_raw_obs_processing_ms: 0.12391368739408612\n",
      "  time_since_restore: 69806.078592062\n",
      "  time_this_iter_s: 221.738755941391\n",
      "  time_total_s: 69806.078592062\n",
      "  timers:\n",
      "    learn_throughput: 18.804\n",
      "    learn_time_ms: 212718.288\n",
      "    load_throughput: 6271621.995\n",
      "    load_time_ms: 0.638\n",
      "    sample_throughput: 17.993\n",
      "    sample_time_ms: 222309.989\n",
      "    update_time_ms: 9.507\n",
      "  timestamp: 1650288167\n",
      "  timesteps_since_restore: 1280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 320\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 14:25:54 (running for 19:27:14.43)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.05 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-26-28\n",
      "  done: false\n",
      "  episode_len_mean: 518.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 6.97\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3441\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7568092942237854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014337236061692238\n",
      "          model: {}\n",
      "          policy_loss: -0.04994550719857216\n",
      "          total_loss: 0.02429361641407013\n",
      "          vf_explained_var: 0.7428864240646362\n",
      "          vf_loss: 0.07423912733793259\n",
      "    num_agent_steps_sampled: 1284000\n",
      "    num_agent_steps_trained: 1284000\n",
      "    num_steps_sampled: 1284000\n",
      "    num_steps_trained: 1284000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.07372013651877\n",
      "    ram_util_percent: 95.98976109215018\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09898892494735197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9395573680136541\n",
      "    mean_inference_ms: 1.9396643373049045\n",
      "    mean_raw_obs_processing_ms: 0.18173530694320558\n",
      "  time_since_restore: 70032.32990407944\n",
      "  time_this_iter_s: 222.06773924827576\n",
      "  time_total_s: 70032.32990407944\n",
      "  timers:\n",
      "    learn_throughput: 18.857\n",
      "    learn_time_ms: 212125.286\n",
      "    load_throughput: 7040671.451\n",
      "    load_time_ms: 0.568\n",
      "    sample_throughput: 18.023\n",
      "    sample_time_ms: 221940.158\n",
      "    update_time_ms: 20.684\n",
      "  timestamp: 1650288388\n",
      "  timesteps_since_restore: 1284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1284000\n",
      "  training_iteration: 321\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-26-28\n",
      "  done: false\n",
      "  episode_len_mean: 1716.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 707\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.00873402040451765\n",
      "          total_loss: -0.008734019473195076\n",
      "          vf_explained_var: -2.7775124635809334e-06\n",
      "          vf_loss: 6.187907963806083e-09\n",
      "    num_agent_steps_sampled: 1284000\n",
      "    num_agent_steps_trained: 1284000\n",
      "    num_steps_sampled: 1284000\n",
      "    num_steps_trained: 1284000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.07918088737202\n",
      "    ram_util_percent: 95.97918088737201\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10090980678861597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9315033280666873\n",
      "    mean_inference_ms: 1.977123178382601\n",
      "    mean_raw_obs_processing_ms: 0.12391368739408612\n",
      "  time_since_restore: 70027.49078512192\n",
      "  time_this_iter_s: 221.41219305992126\n",
      "  time_total_s: 70027.49078512192\n",
      "  timers:\n",
      "    learn_throughput: 18.811\n",
      "    learn_time_ms: 212646.149\n",
      "    load_throughput: 6360547.447\n",
      "    load_time_ms: 0.629\n",
      "    sample_throughput: 18.023\n",
      "    sample_time_ms: 221944.339\n",
      "    update_time_ms: 9.77\n",
      "  timestamp: 1650288388\n",
      "  timesteps_since_restore: 1284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1284000\n",
      "  training_iteration: 321\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-30-09\n",
      "  done: false\n",
      "  episode_len_mean: 524.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.05\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3447\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6535767912864685\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01695307157933712\n",
      "          model: {}\n",
      "          policy_loss: -0.049997780472040176\n",
      "          total_loss: 0.031233297660946846\n",
      "          vf_explained_var: 0.7555854320526123\n",
      "          vf_loss: 0.08123107254505157\n",
      "    num_agent_steps_sampled: 1288000\n",
      "    num_agent_steps_trained: 1288000\n",
      "    num_steps_sampled: 1288000\n",
      "    num_steps_trained: 1288000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.98424657534247\n",
      "    ram_util_percent: 95.90958904109588\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09894461212049167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9392174649594844\n",
      "    mean_inference_ms: 1.9382468075257595\n",
      "    mean_raw_obs_processing_ms: 0.18162250492874016\n",
      "  time_since_restore: 70253.11308717728\n",
      "  time_this_iter_s: 220.78318309783936\n",
      "  time_total_s: 70253.11308717728\n",
      "  timers:\n",
      "    learn_throughput: 18.863\n",
      "    learn_time_ms: 212053.489\n",
      "    load_throughput: 6782783.909\n",
      "    load_time_ms: 0.59\n",
      "    sample_throughput: 18.03\n",
      "    sample_time_ms: 221846.77\n",
      "    update_time_ms: 20.334\n",
      "  timestamp: 1650288609\n",
      "  timesteps_since_restore: 1288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1288000\n",
      "  training_iteration: 322\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-30-10\n",
      "  done: false\n",
      "  episode_len_mean: 1813.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 708\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.764005085628324e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.270080914311602e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0002270129043608904\n",
      "          total_loss: -0.00020622847659979016\n",
      "          vf_explained_var: -0.03225371241569519\n",
      "          vf_loss: 2.078236866509542e-05\n",
      "    num_agent_steps_sampled: 1288000\n",
      "    num_agent_steps_trained: 1288000\n",
      "    num_steps_sampled: 1288000\n",
      "    num_steps_trained: 1288000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.49625850340136\n",
      "    ram_util_percent: 95.89013605442176\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1008808392589005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9312982814062566\n",
      "    mean_inference_ms: 1.9761817438013531\n",
      "    mean_raw_obs_processing_ms: 0.12388009046734665\n",
      "  time_since_restore: 70249.10642194748\n",
      "  time_this_iter_s: 221.61563682556152\n",
      "  time_total_s: 70249.10642194748\n",
      "  timers:\n",
      "    learn_throughput: 18.815\n",
      "    learn_time_ms: 212595.833\n",
      "    load_throughput: 6234565.589\n",
      "    load_time_ms: 0.642\n",
      "    sample_throughput: 18.028\n",
      "    sample_time_ms: 221880.949\n",
      "    update_time_ms: 9.715\n",
      "  timestamp: 1650288610\n",
      "  timesteps_since_restore: 1288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1288000\n",
      "  training_iteration: 322\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-33-50\n",
      "  done: false\n",
      "  episode_len_mean: 531.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.22\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3455\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6662055850028992\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01786264404654503\n",
      "          model: {}\n",
      "          policy_loss: -0.0448983870446682\n",
      "          total_loss: 0.018037602305412292\n",
      "          vf_explained_var: 0.783137321472168\n",
      "          vf_loss: 0.06293599307537079\n",
      "    num_agent_steps_sampled: 1292000\n",
      "    num_agent_steps_trained: 1292000\n",
      "    num_steps_sampled: 1292000\n",
      "    num_steps_trained: 1292000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.60338983050846\n",
      "    ram_util_percent: 95.86677966101695\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09888517300192412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9387573094283451\n",
      "    mean_inference_ms: 1.9363187261569983\n",
      "    mean_raw_obs_processing_ms: 0.18146853933225182\n",
      "  time_since_restore: 70474.40952134132\n",
      "  time_this_iter_s: 221.29643416404724\n",
      "  time_total_s: 70474.40952134132\n",
      "  timers:\n",
      "    learn_throughput: 18.875\n",
      "    learn_time_ms: 211924.916\n",
      "    load_throughput: 6839468.406\n",
      "    load_time_ms: 0.585\n",
      "    sample_throughput: 18.034\n",
      "    sample_time_ms: 221802.188\n",
      "    update_time_ms: 20.641\n",
      "  timestamp: 1650288830\n",
      "  timesteps_since_restore: 1292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1292000\n",
      "  training_iteration: 323\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 1813.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.2\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 708\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.005784993525594473\n",
      "          total_loss: -0.005784993525594473\n",
      "          vf_explained_var: 2.124168531736359e-06\n",
      "          vf_loss: 2.335811988274372e-09\n",
      "    num_agent_steps_sampled: 1292000\n",
      "    num_agent_steps_trained: 1292000\n",
      "    num_steps_sampled: 1292000\n",
      "    num_steps_trained: 1292000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.96779661016949\n",
      "    ram_util_percent: 95.85966101694915\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1008808392589005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9312982814062566\n",
      "    mean_inference_ms: 1.9761817438013531\n",
      "    mean_raw_obs_processing_ms: 0.12388009046734665\n",
      "  time_since_restore: 70470.39720892906\n",
      "  time_this_iter_s: 221.29078698158264\n",
      "  time_total_s: 70470.39720892906\n",
      "  timers:\n",
      "    learn_throughput: 18.829\n",
      "    learn_time_ms: 212442.064\n",
      "    load_throughput: 6123742.016\n",
      "    load_time_ms: 0.653\n",
      "    sample_throughput: 18.029\n",
      "    sample_time_ms: 221869.742\n",
      "    update_time_ms: 9.512\n",
      "  timestamp: 1650288831\n",
      "  timesteps_since_restore: 1292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1292000\n",
      "  training_iteration: 323\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 533.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.24\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3462\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6438954472541809\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016794748604297638\n",
      "          model: {}\n",
      "          policy_loss: -0.04032985866069794\n",
      "          total_loss: 0.037931449711322784\n",
      "          vf_explained_var: 0.7241372466087341\n",
      "          vf_loss: 0.07826130837202072\n",
      "    num_agent_steps_sampled: 1296000\n",
      "    num_agent_steps_trained: 1296000\n",
      "    num_steps_sampled: 1296000\n",
      "    num_steps_trained: 1296000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.82662116040956\n",
      "    ram_util_percent: 95.82081911262797\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09883333236063717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9383576524665537\n",
      "    mean_inference_ms: 1.9346344938897113\n",
      "    mean_raw_obs_processing_ms: 0.18133198830407715\n",
      "  time_since_restore: 70694.75096130371\n",
      "  time_this_iter_s: 220.34143996238708\n",
      "  time_total_s: 70694.75096130371\n",
      "  timers:\n",
      "    learn_throughput: 18.896\n",
      "    learn_time_ms: 211683.528\n",
      "    load_throughput: 6869995.496\n",
      "    load_time_ms: 0.582\n",
      "    sample_throughput: 18.044\n",
      "    sample_time_ms: 221685.852\n",
      "    update_time_ms: 19.593\n",
      "  timestamp: 1650289050\n",
      "  timesteps_since_restore: 1296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1296000\n",
      "  training_iteration: 324\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 1910.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.18\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 709\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.678857244792602e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.528309408662048e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0023633595556020737\n",
      "          total_loss: 0.0025374791584908962\n",
      "          vf_explained_var: 0.03202815353870392\n",
      "          vf_loss: 0.00017411992303095758\n",
      "    num_agent_steps_sampled: 1296000\n",
      "    num_agent_steps_trained: 1296000\n",
      "    num_steps_sampled: 1296000\n",
      "    num_steps_trained: 1296000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.52945205479453\n",
      "    ram_util_percent: 95.81301369863014\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10085083141628243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9310853397508713\n",
      "    mean_inference_ms: 1.975204411911268\n",
      "    mean_raw_obs_processing_ms: 0.12384447865564377\n",
      "  time_since_restore: 70689.50561904907\n",
      "  time_this_iter_s: 219.10841012001038\n",
      "  time_total_s: 70689.50561904907\n",
      "  timers:\n",
      "    learn_throughput: 18.865\n",
      "    learn_time_ms: 212031.349\n",
      "    load_throughput: 6310070.709\n",
      "    load_time_ms: 0.634\n",
      "    sample_throughput: 18.039\n",
      "    sample_time_ms: 221742.392\n",
      "    update_time_ms: 9.312\n",
      "  timestamp: 1650289050\n",
      "  timesteps_since_restore: 1296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1296000\n",
      "  training_iteration: 324\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-41-07\n",
      "  done: false\n",
      "  episode_len_mean: 1912.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 710\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.985815505588979e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.357610907243113e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.005806287284940481\n",
      "          total_loss: -0.0053980047814548016\n",
      "          vf_explained_var: 0.09617101401090622\n",
      "          vf_loss: 0.0004082796804141253\n",
      "    num_agent_steps_sampled: 1300000\n",
      "    num_agent_steps_trained: 1300000\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.39166666666667\n",
      "    ram_util_percent: 95.58020833333333\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10082023613560262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9308678688175192\n",
      "    mean_inference_ms: 1.9742056508984094\n",
      "    mean_raw_obs_processing_ms: 0.12380786247911796\n",
      "  time_since_restore: 70905.74752211571\n",
      "  time_this_iter_s: 216.24190306663513\n",
      "  time_total_s: 70905.74752211571\n",
      "  timers:\n",
      "    learn_throughput: 18.917\n",
      "    learn_time_ms: 211449.931\n",
      "    load_throughput: 7135293.667\n",
      "    load_time_ms: 0.561\n",
      "    sample_throughput: 18.076\n",
      "    sample_time_ms: 221282.564\n",
      "    update_time_ms: 9.341\n",
      "  timestamp: 1650289267\n",
      "  timesteps_since_restore: 1300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 325\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-41-07\n",
      "  done: false\n",
      "  episode_len_mean: 536.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.29\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3471\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7557789087295532\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013470371253788471\n",
      "          model: {}\n",
      "          policy_loss: -0.04721618443727493\n",
      "          total_loss: 0.031971633434295654\n",
      "          vf_explained_var: 0.7481604218482971\n",
      "          vf_loss: 0.07918782532215118\n",
      "    num_agent_steps_sampled: 1300000\n",
      "    num_agent_steps_trained: 1300000\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.4517361111111\n",
      "    ram_util_percent: 95.58090277777777\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09876744266037588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9378501957960953\n",
      "    mean_inference_ms: 1.9324910622742806\n",
      "    mean_raw_obs_processing_ms: 0.18115956751392168\n",
      "  time_since_restore: 70911.54124116898\n",
      "  time_this_iter_s: 216.7902798652649\n",
      "  time_total_s: 70911.54124116898\n",
      "  timers:\n",
      "    learn_throughput: 18.948\n",
      "    learn_time_ms: 211101.253\n",
      "    load_throughput: 6880419.948\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 18.068\n",
      "    sample_time_ms: 221386.238\n",
      "    update_time_ms: 18.716\n",
      "  timestamp: 1650289267\n",
      "  timesteps_since_restore: 1300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 325\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 14:42:35 (running for 19:43:54.81)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.29 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 1912.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 710\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.071090718159807e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.905086538442317e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128086157143116\n",
      "          total_loss: -0.014077471569180489\n",
      "          vf_explained_var: -8.959924002738262e-08\n",
      "          vf_loss: 5.0608316087163985e-05\n",
      "    num_agent_steps_sampled: 1304000\n",
      "    num_agent_steps_trained: 1304000\n",
      "    num_steps_sampled: 1304000\n",
      "    num_steps_trained: 1304000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.86056338028168\n",
      "    ram_util_percent: 95.48274647887324\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10082023613560262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9308678688175192\n",
      "    mean_inference_ms: 1.9742056508984094\n",
      "    mean_raw_obs_processing_ms: 0.12380786247911796\n",
      "  time_since_restore: 71119.21571683884\n",
      "  time_this_iter_s: 213.46819472312927\n",
      "  time_total_s: 71119.21571683884\n",
      "  timers:\n",
      "    learn_throughput: 18.981\n",
      "    learn_time_ms: 210740.65\n",
      "    load_throughput: 7087367.354\n",
      "    load_time_ms: 0.564\n",
      "    sample_throughput: 18.124\n",
      "    sample_time_ms: 220705.231\n",
      "    update_time_ms: 11.998\n",
      "  timestamp: 1650289480\n",
      "  timesteps_since_restore: 1304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1304000\n",
      "  training_iteration: 326\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-44-41\n",
      "  done: false\n",
      "  episode_len_mean: 533.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3479\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6598713994026184\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016891349107027054\n",
      "          model: {}\n",
      "          policy_loss: -0.04450187087059021\n",
      "          total_loss: 0.03311914950609207\n",
      "          vf_explained_var: 0.7460682988166809\n",
      "          vf_loss: 0.07762102037668228\n",
      "    num_agent_steps_sampled: 1304000\n",
      "    num_agent_steps_trained: 1304000\n",
      "    num_steps_sampled: 1304000\n",
      "    num_steps_trained: 1304000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.69052631578946\n",
      "    ram_util_percent: 95.46736842105264\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0987087513350048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9374043613234556\n",
      "    mean_inference_ms: 1.9305779879175018\n",
      "    mean_raw_obs_processing_ms: 0.18100632654568727\n",
      "  time_since_restore: 71125.61122012138\n",
      "  time_this_iter_s: 214.06997895240784\n",
      "  time_total_s: 71125.61122012138\n",
      "  timers:\n",
      "    learn_throughput: 19.005\n",
      "    learn_time_ms: 210470.644\n",
      "    load_throughput: 6994878.466\n",
      "    load_time_ms: 0.572\n",
      "    sample_throughput: 18.118\n",
      "    sample_time_ms: 220771.2\n",
      "    update_time_ms: 19.019\n",
      "  timestamp: 1650289481\n",
      "  timesteps_since_restore: 1304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1304000\n",
      "  training_iteration: 326\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-48-18\n",
      "  done: false\n",
      "  episode_len_mean: 1914.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 719\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.743622806612657e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.6299586974662476e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0028649168089032173\n",
      "          total_loss: -0.0025824294425547123\n",
      "          vf_explained_var: 0.3300033211708069\n",
      "          vf_loss: 0.0002824869879987091\n",
      "    num_agent_steps_sampled: 1308000\n",
      "    num_agent_steps_trained: 1308000\n",
      "    num_steps_sampled: 1308000\n",
      "    num_steps_trained: 1308000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.44201388888888\n",
      "    ram_util_percent: 95.93125\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10054425960101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9289330447007912\n",
      "    mean_inference_ms: 1.9653007684622659\n",
      "    mean_raw_obs_processing_ms: 0.12347759974873254\n",
      "  time_since_restore: 71336.47984981537\n",
      "  time_this_iter_s: 217.26413297653198\n",
      "  time_total_s: 71336.47984981537\n",
      "  timers:\n",
      "    learn_throughput: 19.022\n",
      "    learn_time_ms: 210287.236\n",
      "    load_throughput: 6938181.217\n",
      "    load_time_ms: 0.577\n",
      "    sample_throughput: 18.18\n",
      "    sample_time_ms: 220017.125\n",
      "    update_time_ms: 13.909\n",
      "  timestamp: 1650289698\n",
      "  timesteps_since_restore: 1308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1308000\n",
      "  training_iteration: 327\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-48-18\n",
      "  done: false\n",
      "  episode_len_mean: 530.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3487\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8053647875785828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012224631384015083\n",
      "          model: {}\n",
      "          policy_loss: -0.04768512025475502\n",
      "          total_loss: 0.00902068242430687\n",
      "          vf_explained_var: 0.7527484893798828\n",
      "          vf_loss: 0.05670580267906189\n",
      "    num_agent_steps_sampled: 1308000\n",
      "    num_agent_steps_trained: 1308000\n",
      "    num_steps_sampled: 1308000\n",
      "    num_steps_trained: 1308000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.25243055555558\n",
      "    ram_util_percent: 95.90347222222222\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0986496209840882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9369642874399208\n",
      "    mean_inference_ms: 1.9286742048732168\n",
      "    mean_raw_obs_processing_ms: 0.18085555194901393\n",
      "  time_since_restore: 71342.50144195557\n",
      "  time_this_iter_s: 216.89022183418274\n",
      "  time_total_s: 71342.50144195557\n",
      "  timers:\n",
      "    learn_throughput: 19.046\n",
      "    learn_time_ms: 210019.725\n",
      "    load_throughput: 7248429.966\n",
      "    load_time_ms: 0.552\n",
      "    sample_throughput: 18.174\n",
      "    sample_time_ms: 220098.849\n",
      "    update_time_ms: 17.964\n",
      "  timestamp: 1650289698\n",
      "  timesteps_since_restore: 1308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1308000\n",
      "  training_iteration: 327\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-52-05\n",
      "  done: false\n",
      "  episode_len_mean: 532.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.11\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3496\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.750503420829773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014721546322107315\n",
      "          model: {}\n",
      "          policy_loss: -0.04385796934366226\n",
      "          total_loss: 0.028017714619636536\n",
      "          vf_explained_var: 0.6689608097076416\n",
      "          vf_loss: 0.0718756839632988\n",
      "    num_agent_steps_sampled: 1312000\n",
      "    num_agent_steps_trained: 1312000\n",
      "    num_steps_sampled: 1312000\n",
      "    num_steps_trained: 1312000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.3364238410596\n",
      "    ram_util_percent: 95.77682119205298\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0985817530071601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9364670203666663\n",
      "    mean_inference_ms: 1.9264942802636822\n",
      "    mean_raw_obs_processing_ms: 0.18068567210489064\n",
      "  time_since_restore: 71569.42986869812\n",
      "  time_this_iter_s: 226.9284267425537\n",
      "  time_total_s: 71569.42986869812\n",
      "  timers:\n",
      "    learn_throughput: 18.993\n",
      "    learn_time_ms: 210602.147\n",
      "    load_throughput: 7148975.626\n",
      "    load_time_ms: 0.56\n",
      "    sample_throughput: 18.215\n",
      "    sample_time_ms: 219604.248\n",
      "    update_time_ms: 21.643\n",
      "  timestamp: 1650289925\n",
      "  timesteps_since_restore: 1312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1312000\n",
      "  training_iteration: 328\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 1808.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 737\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0787167371780561e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.5854677080853546e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0013736148830503225\n",
      "          total_loss: 0.0019114279421046376\n",
      "          vf_explained_var: 0.4172608554363251\n",
      "          vf_loss: 0.0005378188216127455\n",
      "    num_agent_steps_sampled: 1312000\n",
      "    num_agent_steps_trained: 1312000\n",
      "    num_steps_sampled: 1312000\n",
      "    num_steps_trained: 1312000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.80592105263158\n",
      "    ram_util_percent: 95.7467105263158\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10001290522615475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9252073935964674\n",
      "    mean_inference_ms: 1.948116703391138\n",
      "    mean_raw_obs_processing_ms: 0.12287049111033826\n",
      "  time_since_restore: 71564.37773394585\n",
      "  time_this_iter_s: 227.8978841304779\n",
      "  time_total_s: 71564.37773394585\n",
      "  timers:\n",
      "    learn_throughput: 18.976\n",
      "    learn_time_ms: 210788.495\n",
      "    load_throughput: 6806448.943\n",
      "    load_time_ms: 0.588\n",
      "    sample_throughput: 18.213\n",
      "    sample_time_ms: 219623.68\n",
      "    update_time_ms: 14.862\n",
      "  timestamp: 1650289926\n",
      "  timesteps_since_restore: 1312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1312000\n",
      "  training_iteration: 328\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 1710.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 738\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.140112861051612e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.1853508510175482e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.004541514907032251\n",
      "          total_loss: -0.004234933760017157\n",
      "          vf_explained_var: 0.0960082858800888\n",
      "          vf_loss: 0.00030657832394354045\n",
      "    num_agent_steps_sampled: 1316000\n",
      "    num_agent_steps_trained: 1316000\n",
      "    num_steps_sampled: 1316000\n",
      "    num_steps_trained: 1316000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.76464646464646\n",
      "    ram_util_percent: 95.71212121212122\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09998539069204268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9250127966878959\n",
      "    mean_inference_ms: 1.9472162906010737\n",
      "    mean_raw_obs_processing_ms: 0.12283861908719801\n",
      "  time_since_restore: 71787.39614772797\n",
      "  time_this_iter_s: 223.01841378211975\n",
      "  time_total_s: 71787.39614772797\n",
      "  timers:\n",
      "    learn_throughput: 18.954\n",
      "    learn_time_ms: 211032.857\n",
      "    load_throughput: 6941051.673\n",
      "    load_time_ms: 0.576\n",
      "    sample_throughput: 18.175\n",
      "    sample_time_ms: 220077.091\n",
      "    update_time_ms: 16.156\n",
      "  timestamp: 1650290149\n",
      "  timesteps_since_restore: 1316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1316000\n",
      "  training_iteration: 329\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 525.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.03\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3506\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.722177267074585\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015616850927472115\n",
      "          model: {}\n",
      "          policy_loss: -0.03836632892489433\n",
      "          total_loss: 0.047717612236738205\n",
      "          vf_explained_var: 0.6609528660774231\n",
      "          vf_loss: 0.08608394861221313\n",
      "    num_agent_steps_sampled: 1316000\n",
      "    num_agent_steps_trained: 1316000\n",
      "    num_steps_sampled: 1316000\n",
      "    num_steps_trained: 1316000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.2117845117845\n",
      "    ram_util_percent: 95.73939393939393\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09850628665912303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9359230392840083\n",
      "    mean_inference_ms: 1.9240929769399964\n",
      "    mean_raw_obs_processing_ms: 0.1805027471570454\n",
      "  time_since_restore: 71792.84009575844\n",
      "  time_this_iter_s: 223.410227060318\n",
      "  time_total_s: 71792.84009575844\n",
      "  timers:\n",
      "    learn_throughput: 18.97\n",
      "    learn_time_ms: 210862.289\n",
      "    load_throughput: 6892574.668\n",
      "    load_time_ms: 0.58\n",
      "    sample_throughput: 18.167\n",
      "    sample_time_ms: 220177.58\n",
      "    update_time_ms: 18.92\n",
      "  timestamp: 1650290149\n",
      "  timesteps_since_restore: 1316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1316000\n",
      "  training_iteration: 329\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 14:59:15 (running for 20:00:35.65)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.03 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-59-33\n",
      "  done: false\n",
      "  episode_len_mean: 1710.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 738\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1825775995136996e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.1389332430558937e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128235168755054\n",
      "          total_loss: -0.014105455949902534\n",
      "          vf_explained_var: 4.672235220937182e-08\n",
      "          vf_loss: 2.2780770450481214e-05\n",
      "    num_agent_steps_sampled: 1320000\n",
      "    num_agent_steps_trained: 1320000\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.2554054054054\n",
      "    ram_util_percent: 95.79898648648648\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09998539069204268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9250127966878959\n",
      "    mean_inference_ms: 1.9472162906010737\n",
      "    mean_raw_obs_processing_ms: 0.12283861908719801\n",
      "  time_since_restore: 72011.22878575325\n",
      "  time_this_iter_s: 223.8326380252838\n",
      "  time_total_s: 72011.22878575325\n",
      "  timers:\n",
      "    learn_throughput: 18.934\n",
      "    learn_time_ms: 211264.542\n",
      "    load_throughput: 7156904.701\n",
      "    load_time_ms: 0.559\n",
      "    sample_throughput: 18.157\n",
      "    sample_time_ms: 220305.763\n",
      "    update_time_ms: 16.91\n",
      "  timestamp: 1650290373\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 330\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_14-59-33\n",
      "  done: false\n",
      "  episode_len_mean: 524.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3514\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.655746579170227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01934649981558323\n",
      "          model: {}\n",
      "          policy_loss: -0.04633276164531708\n",
      "          total_loss: 0.015503966249525547\n",
      "          vf_explained_var: 0.7932506799697876\n",
      "          vf_loss: 0.06183673068881035\n",
      "    num_agent_steps_sampled: 1320000\n",
      "    num_agent_steps_trained: 1320000\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.28080808080809\n",
      "    ram_util_percent: 95.83569023569022\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09844738773375183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9354986879388061\n",
      "    mean_inference_ms: 1.922205101247402\n",
      "    mean_raw_obs_processing_ms: 0.18036052610356967\n",
      "  time_since_restore: 72017.11394572258\n",
      "  time_this_iter_s: 224.27384996414185\n",
      "  time_total_s: 72017.11394572258\n",
      "  timers:\n",
      "    learn_throughput: 18.942\n",
      "    learn_time_ms: 211168.265\n",
      "    load_throughput: 6885502.75\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 18.148\n",
      "    sample_time_ms: 220413.775\n",
      "    update_time_ms: 16.473\n",
      "  timestamp: 1650290373\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 330\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-03-16\n",
      "  done: false\n",
      "  episode_len_mean: 1711.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 741\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.467992650959942e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.0523456748208807e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0016700654523447156\n",
      "          total_loss: -0.0016138316132128239\n",
      "          vf_explained_var: 0.08764108270406723\n",
      "          vf_loss: 5.622610842692666e-05\n",
      "    num_agent_steps_sampled: 1324000\n",
      "    num_agent_steps_trained: 1324000\n",
      "    num_steps_sampled: 1324000\n",
      "    num_steps_trained: 1324000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.1464406779661\n",
      "    ram_util_percent: 95.84677966101694\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0999060002810592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9244485888939148\n",
      "    mean_inference_ms: 1.944610265802745\n",
      "    mean_raw_obs_processing_ms: 0.12274698464662638\n",
      "  time_since_restore: 72234.35261464119\n",
      "  time_this_iter_s: 223.12382888793945\n",
      "  time_total_s: 72234.35261464119\n",
      "  timers:\n",
      "    learn_throughput: 18.918\n",
      "    learn_time_ms: 211441.994\n",
      "    load_throughput: 7211044.443\n",
      "    load_time_ms: 0.555\n",
      "    sample_throughput: 18.137\n",
      "    sample_time_ms: 220547.781\n",
      "    update_time_ms: 17.706\n",
      "  timestamp: 1650290596\n",
      "  timesteps_since_restore: 1324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1324000\n",
      "  training_iteration: 331\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-03-16\n",
      "  done: false\n",
      "  episode_len_mean: 524.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.05\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3521\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6348167061805725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017083877697587013\n",
      "          model: {}\n",
      "          policy_loss: -0.039518605917692184\n",
      "          total_loss: 0.05572438985109329\n",
      "          vf_explained_var: 0.6786409616470337\n",
      "          vf_loss: 0.09524299949407578\n",
      "    num_agent_steps_sampled: 1324000\n",
      "    num_agent_steps_trained: 1324000\n",
      "    num_steps_sampled: 1324000\n",
      "    num_steps_trained: 1324000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.48610169491525\n",
      "    ram_util_percent: 95.85457627118643\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09839614104724996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9351323349722197\n",
      "    mean_inference_ms: 1.9205659591968507\n",
      "    mean_raw_obs_processing_ms: 0.1802373979235732\n",
      "  time_since_restore: 72240.26350164413\n",
      "  time_this_iter_s: 223.14955592155457\n",
      "  time_total_s: 72240.26350164413\n",
      "  timers:\n",
      "    learn_throughput: 18.927\n",
      "    learn_time_ms: 211333.526\n",
      "    load_throughput: 6837517.219\n",
      "    load_time_ms: 0.585\n",
      "    sample_throughput: 18.127\n",
      "    sample_time_ms: 220667.33\n",
      "    update_time_ms: 14.497\n",
      "  timestamp: 1650290596\n",
      "  timesteps_since_restore: 1324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1324000\n",
      "  training_iteration: 331\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-06-59\n",
      "  done: false\n",
      "  episode_len_mean: 533.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3528\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6355471611022949\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013747001998126507\n",
      "          model: {}\n",
      "          policy_loss: -0.03866487741470337\n",
      "          total_loss: 0.0561193972826004\n",
      "          vf_explained_var: 0.7404367327690125\n",
      "          vf_loss: 0.09478427469730377\n",
      "    num_agent_steps_sampled: 1328000\n",
      "    num_agent_steps_trained: 1328000\n",
      "    num_steps_sampled: 1328000\n",
      "    num_steps_trained: 1328000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.22610169491524\n",
      "    ram_util_percent: 95.88101694915252\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09834461613570862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9347661234479197\n",
      "    mean_inference_ms: 1.9189227094459635\n",
      "    mean_raw_obs_processing_ms: 0.18011117125014928\n",
      "  time_since_restore: 72462.96627354622\n",
      "  time_this_iter_s: 222.70277190208435\n",
      "  time_total_s: 72462.96627354622\n",
      "  timers:\n",
      "    learn_throughput: 18.913\n",
      "    learn_time_ms: 211492.57\n",
      "    load_throughput: 6518967.983\n",
      "    load_time_ms: 0.614\n",
      "    sample_throughput: 18.111\n",
      "    sample_time_ms: 220854.484\n",
      "    update_time_ms: 14.805\n",
      "  timestamp: 1650290819\n",
      "  timesteps_since_restore: 1328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1328000\n",
      "  training_iteration: 332\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-07-00\n",
      "  done: false\n",
      "  episode_len_mean: 1711.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 741\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128330163657665\n",
      "          total_loss: -0.014121451415121555\n",
      "          vf_explained_var: -1.9419577057533388e-08\n",
      "          vf_loss: 6.873948677821318e-06\n",
      "    num_agent_steps_sampled: 1328000\n",
      "    num_agent_steps_trained: 1328000\n",
      "    num_steps_sampled: 1328000\n",
      "    num_steps_trained: 1328000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.55420875420874\n",
      "    ram_util_percent: 95.85353535353535\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0999060002810592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9244485888939148\n",
      "    mean_inference_ms: 1.944610265802745\n",
      "    mean_raw_obs_processing_ms: 0.12274698464662638\n",
      "  time_since_restore: 72458.22275781631\n",
      "  time_this_iter_s: 223.87014317512512\n",
      "  time_total_s: 72458.22275781631\n",
      "  timers:\n",
      "    learn_throughput: 18.899\n",
      "    learn_time_ms: 211655.58\n",
      "    load_throughput: 7237485.872\n",
      "    load_time_ms: 0.553\n",
      "    sample_throughput: 18.121\n",
      "    sample_time_ms: 220742.588\n",
      "    update_time_ms: 17.57\n",
      "  timestamp: 1650290820\n",
      "  timesteps_since_restore: 1328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1328000\n",
      "  training_iteration: 332\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-10-42\n",
      "  done: false\n",
      "  episode_len_mean: 540.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3535\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7144574522972107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01566488668322563\n",
      "          model: {}\n",
      "          policy_loss: -0.04794422909617424\n",
      "          total_loss: 0.02603762038052082\n",
      "          vf_explained_var: 0.7362238764762878\n",
      "          vf_loss: 0.07398184388875961\n",
      "    num_agent_steps_sampled: 1332000\n",
      "    num_agent_steps_trained: 1332000\n",
      "    num_steps_sampled: 1332000\n",
      "    num_steps_trained: 1332000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.59191919191919\n",
      "    ram_util_percent: 95.80909090909091\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09829273902035726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9343926998518893\n",
      "    mean_inference_ms: 1.917238225933898\n",
      "    mean_raw_obs_processing_ms: 0.17998088381040517\n",
      "  time_since_restore: 72686.15267467499\n",
      "  time_this_iter_s: 223.18640112876892\n",
      "  time_total_s: 72686.15267467499\n",
      "  timers:\n",
      "    learn_throughput: 18.893\n",
      "    learn_time_ms: 211722.917\n",
      "    load_throughput: 6482946.018\n",
      "    load_time_ms: 0.617\n",
      "    sample_throughput: 18.102\n",
      "    sample_time_ms: 220970.562\n",
      "    update_time_ms: 14.653\n",
      "  timestamp: 1650291042\n",
      "  timesteps_since_restore: 1332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1332000\n",
      "  training_iteration: 333\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 1711.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 741\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.0745895265513365e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.706899764092311e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128047041594982\n",
      "          total_loss: -0.014126290567219257\n",
      "          vf_explained_var: 5.351599696723497e-08\n",
      "          vf_loss: 1.7554424402987934e-06\n",
      "    num_agent_steps_sampled: 1332000\n",
      "    num_agent_steps_trained: 1332000\n",
      "    num_steps_sampled: 1332000\n",
      "    num_steps_trained: 1332000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.81784511784512\n",
      "    ram_util_percent: 95.81750841750842\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0999060002810592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9244485888939148\n",
      "    mean_inference_ms: 1.944610265802745\n",
      "    mean_raw_obs_processing_ms: 0.12274698464662638\n",
      "  time_since_restore: 72681.90860581398\n",
      "  time_this_iter_s: 223.6858479976654\n",
      "  time_total_s: 72681.90860581398\n",
      "  timers:\n",
      "    learn_throughput: 18.874\n",
      "    learn_time_ms: 211929.947\n",
      "    load_throughput: 7333340.327\n",
      "    load_time_ms: 0.545\n",
      "    sample_throughput: 18.106\n",
      "    sample_time_ms: 220920.598\n",
      "    update_time_ms: 19.143\n",
      "  timestamp: 1650291044\n",
      "  timesteps_since_restore: 1332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1332000\n",
      "  training_iteration: 333\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-14-31\n",
      "  done: false\n",
      "  episode_len_mean: 533.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3544\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7765057682991028\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01628117635846138\n",
      "          model: {}\n",
      "          policy_loss: -0.05995777249336243\n",
      "          total_loss: 0.009694643318653107\n",
      "          vf_explained_var: 0.7565569877624512\n",
      "          vf_loss: 0.06965241581201553\n",
      "    num_agent_steps_sampled: 1336000\n",
      "    num_agent_steps_trained: 1336000\n",
      "    num_steps_sampled: 1336000\n",
      "    num_steps_trained: 1336000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.62772277227722\n",
      "    ram_util_percent: 95.54785478547855\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09822721932998864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.933917638183156\n",
      "    mean_inference_ms: 1.9150817784152843\n",
      "    mean_raw_obs_processing_ms: 0.17981842507304968\n",
      "  time_since_restore: 72914.3603117466\n",
      "  time_this_iter_s: 228.2076370716095\n",
      "  time_total_s: 72914.3603117466\n",
      "  timers:\n",
      "    learn_throughput: 18.821\n",
      "    learn_time_ms: 212524.284\n",
      "    load_throughput: 6412328.39\n",
      "    load_time_ms: 0.624\n",
      "    sample_throughput: 18.085\n",
      "    sample_time_ms: 221181.248\n",
      "    update_time_ms: 20.887\n",
      "  timestamp: 1650291271\n",
      "  timesteps_since_restore: 1336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1336000\n",
      "  training_iteration: 334\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-14-33\n",
      "  done: false\n",
      "  episode_len_mean: 1614.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 756\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.2067740541475535e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.887752840123832e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0012219303753226995\n",
      "          total_loss: -0.0005513524520210922\n",
      "          vf_explained_var: 0.3220756947994232\n",
      "          vf_loss: 0.0006705854320898652\n",
      "    num_agent_steps_sampled: 1336000\n",
      "    num_agent_steps_trained: 1336000\n",
      "    num_steps_sampled: 1336000\n",
      "    num_steps_trained: 1336000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.78725490196078\n",
      "    ram_util_percent: 95.54640522875818\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0995355767396736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9217841535563085\n",
      "    mean_inference_ms: 1.932335004872393\n",
      "    mean_raw_obs_processing_ms: 0.12233117994603168\n",
      "  time_since_restore: 72911.48395109177\n",
      "  time_this_iter_s: 229.57534527778625\n",
      "  time_total_s: 72911.48395109177\n",
      "  timers:\n",
      "    learn_throughput: 18.788\n",
      "    learn_time_ms: 212898.775\n",
      "    load_throughput: 2919655.43\n",
      "    load_time_ms: 1.37\n",
      "    sample_throughput: 18.077\n",
      "    sample_time_ms: 221273.784\n",
      "    update_time_ms: 19.65\n",
      "  timestamp: 1650291273\n",
      "  timesteps_since_restore: 1336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1336000\n",
      "  training_iteration: 334\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 15:15:56 (running for 20:17:16.31)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.29 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-18-11\n",
      "  done: false\n",
      "  episode_len_mean: 521.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3554\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8009654879570007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013340868055820465\n",
      "          model: {}\n",
      "          policy_loss: -0.05037557706236839\n",
      "          total_loss: 0.02308579348027706\n",
      "          vf_explained_var: 0.7210606932640076\n",
      "          vf_loss: 0.07346136122941971\n",
      "    num_agent_steps_sampled: 1340000\n",
      "    num_agent_steps_trained: 1340000\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.76440677966102\n",
      "    ram_util_percent: 95.60847457627119\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09815626253323463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9334182047575925\n",
      "    mean_inference_ms: 1.9128112616175605\n",
      "    mean_raw_obs_processing_ms: 0.17965230589328252\n",
      "  time_since_restore: 73134.38165903091\n",
      "  time_this_iter_s: 220.02134728431702\n",
      "  time_total_s: 73134.38165903091\n",
      "  timers:\n",
      "    learn_throughput: 18.804\n",
      "    learn_time_ms: 212722.144\n",
      "    load_throughput: 6344192.097\n",
      "    load_time_ms: 0.63\n",
      "    sample_throughput: 18.01\n",
      "    sample_time_ms: 222099.874\n",
      "    update_time_ms: 24.034\n",
      "  timestamp: 1650291491\n",
      "  timesteps_since_restore: 1340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 335\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-18-13\n",
      "  done: false\n",
      "  episode_len_mean: 1614.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 756\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.014127971604466438\n",
      "          total_loss: -0.014127800241112709\n",
      "          vf_explained_var: 2.5508224865689044e-08\n",
      "          vf_loss: 1.7488973469426128e-07\n",
      "    num_agent_steps_sampled: 1340000\n",
      "    num_agent_steps_trained: 1340000\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.8027303754266\n",
      "    ram_util_percent: 95.58976109215018\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0995355767396736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9217841535563085\n",
      "    mean_inference_ms: 1.932335004872393\n",
      "    mean_raw_obs_processing_ms: 0.12233117994603168\n",
      "  time_since_restore: 73131.29131698608\n",
      "  time_this_iter_s: 219.80736589431763\n",
      "  time_total_s: 73131.29131698608\n",
      "  timers:\n",
      "    learn_throughput: 18.764\n",
      "    learn_time_ms: 213169.17\n",
      "    load_throughput: 2901026.421\n",
      "    load_time_ms: 1.379\n",
      "    sample_throughput: 17.993\n",
      "    sample_time_ms: 222313.889\n",
      "    update_time_ms: 18.976\n",
      "  timestamp: 1650291493\n",
      "  timesteps_since_restore: 1340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 335\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-21-46\n",
      "  done: false\n",
      "  episode_len_mean: 512.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 6.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3561\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6754960417747498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015106849372386932\n",
      "          model: {}\n",
      "          policy_loss: -0.0505102276802063\n",
      "          total_loss: 0.024541473016142845\n",
      "          vf_explained_var: 0.7616651058197021\n",
      "          vf_loss: 0.0750517025589943\n",
      "    num_agent_steps_sampled: 1344000\n",
      "    num_agent_steps_trained: 1344000\n",
      "    num_steps_sampled: 1344000\n",
      "    num_steps_trained: 1344000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.09548611111111\n",
      "    ram_util_percent: 95.54097222222224\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09810775968506348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.933081271890793\n",
      "    mean_inference_ms: 1.9112507513534718\n",
      "    mean_raw_obs_processing_ms: 0.17954110930557227\n",
      "  time_since_restore: 73349.61284303665\n",
      "  time_this_iter_s: 215.2311840057373\n",
      "  time_total_s: 73349.61284303665\n",
      "  timers:\n",
      "    learn_throughput: 18.801\n",
      "    learn_time_ms: 212750.101\n",
      "    load_throughput: 6277253.713\n",
      "    load_time_ms: 0.637\n",
      "    sample_throughput: 17.986\n",
      "    sample_time_ms: 222395.181\n",
      "    update_time_ms: 23.413\n",
      "  timestamp: 1650291706\n",
      "  timesteps_since_restore: 1344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1344000\n",
      "  training_iteration: 336\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-21-48\n",
      "  done: false\n",
      "  episode_len_mean: 1614.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 756\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.007781271357089281\n",
      "          total_loss: -0.007781270891427994\n",
      "          vf_explained_var: -1.8995935988641577e-06\n",
      "          vf_loss: 4.365692696239876e-09\n",
      "    num_agent_steps_sampled: 1344000\n",
      "    num_agent_steps_trained: 1344000\n",
      "    num_steps_sampled: 1344000\n",
      "    num_steps_trained: 1344000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.65679442508711\n",
      "    ram_util_percent: 95.53519163763066\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0995355767396736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9217841535563085\n",
      "    mean_inference_ms: 1.932335004872393\n",
      "    mean_raw_obs_processing_ms: 0.12233117994603168\n",
      "  time_since_restore: 73345.91719889641\n",
      "  time_this_iter_s: 214.6258819103241\n",
      "  time_total_s: 73345.91719889641\n",
      "  timers:\n",
      "    learn_throughput: 18.761\n",
      "    learn_time_ms: 213212.385\n",
      "    load_throughput: 1885186.359\n",
      "    load_time_ms: 2.122\n",
      "    sample_throughput: 17.965\n",
      "    sample_time_ms: 222660.664\n",
      "    update_time_ms: 16.548\n",
      "  timestamp: 1650291708\n",
      "  timesteps_since_restore: 1344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1344000\n",
      "  training_iteration: 336\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 511.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 6.94\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3569\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6721289753913879\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017909733578562737\n",
      "          model: {}\n",
      "          policy_loss: -0.042911503463983536\n",
      "          total_loss: 0.02126881666481495\n",
      "          vf_explained_var: 0.7452079057693481\n",
      "          vf_loss: 0.06418031454086304\n",
      "    num_agent_steps_sampled: 1348000\n",
      "    num_agent_steps_trained: 1348000\n",
      "    num_steps_sampled: 1348000\n",
      "    num_steps_trained: 1348000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.12702702702703\n",
      "    ram_util_percent: 95.81858108108109\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09805234681709304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9327024510834314\n",
      "    mean_inference_ms: 1.9094997847441801\n",
      "    mean_raw_obs_processing_ms: 0.17941577597641345\n",
      "  time_since_restore: 73571.80406594276\n",
      "  time_this_iter_s: 222.19122290611267\n",
      "  time_total_s: 73571.80406594276\n",
      "  timers:\n",
      "    learn_throughput: 18.758\n",
      "    learn_time_ms: 213246.342\n",
      "    load_throughput: 6358377.928\n",
      "    load_time_ms: 0.629\n",
      "    sample_throughput: 17.977\n",
      "    sample_time_ms: 222501.961\n",
      "    update_time_ms: 25.754\n",
      "  timestamp: 1650291928\n",
      "  timesteps_since_restore: 1348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1348000\n",
      "  training_iteration: 337\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-25-30\n",
      "  done: false\n",
      "  episode_len_mean: 1619.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 766\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.246376101455354e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.0112814587698428e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.00020171563664916903\n",
      "          total_loss: 9.28553199628368e-05\n",
      "          vf_explained_var: 0.2892155349254608\n",
      "          vf_loss: 0.000294575875159353\n",
      "    num_agent_steps_sampled: 1348000\n",
      "    num_agent_steps_trained: 1348000\n",
      "    num_steps_sampled: 1348000\n",
      "    num_steps_trained: 1348000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.1806779661017\n",
      "    ram_util_percent: 95.83898305084746\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09928901997181078\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9200182431803461\n",
      "    mean_inference_ms: 1.9241886072243861\n",
      "    mean_raw_obs_processing_ms: 0.12204425883963467\n",
      "  time_since_restore: 73567.96623897552\n",
      "  time_this_iter_s: 222.04904007911682\n",
      "  time_total_s: 73567.96623897552\n",
      "  timers:\n",
      "    learn_throughput: 18.72\n",
      "    learn_time_ms: 213678.51\n",
      "    load_throughput: 1841181.712\n",
      "    load_time_ms: 2.173\n",
      "    sample_throughput: 17.957\n",
      "    sample_time_ms: 222752.315\n",
      "    update_time_ms: 14.948\n",
      "  timestamp: 1650291930\n",
      "  timesteps_since_restore: 1348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1348000\n",
      "  training_iteration: 337\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-29-06\n",
      "  done: false\n",
      "  episode_len_mean: 514.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.14\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3578\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7289261817932129\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015094629488885403\n",
      "          model: {}\n",
      "          policy_loss: -0.03987095504999161\n",
      "          total_loss: 0.03567184507846832\n",
      "          vf_explained_var: 0.7367103099822998\n",
      "          vf_loss: 0.07554280012845993\n",
      "    num_agent_steps_sampled: 1352000\n",
      "    num_agent_steps_trained: 1352000\n",
      "    num_steps_sampled: 1352000\n",
      "    num_steps_trained: 1352000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.2553264604811\n",
      "    ram_util_percent: 95.8917525773196\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09798990105868131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9322804882730569\n",
      "    mean_inference_ms: 1.907553304164976\n",
      "    mean_raw_obs_processing_ms: 0.17927522769592827\n",
      "  time_since_restore: 73789.52071380615\n",
      "  time_this_iter_s: 217.71664786338806\n",
      "  time_total_s: 73789.52071380615\n",
      "  timers:\n",
      "    learn_throughput: 18.846\n",
      "    learn_time_ms: 212249.098\n",
      "    load_throughput: 6325773.32\n",
      "    load_time_ms: 0.632\n",
      "    sample_throughput: 17.93\n",
      "    sample_time_ms: 223084.973\n",
      "    update_time_ms: 21.032\n",
      "  timestamp: 1650292146\n",
      "  timesteps_since_restore: 1352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1352000\n",
      "  training_iteration: 338\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-29-08\n",
      "  done: false\n",
      "  episode_len_mean: 1424.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 780\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.6402562493165e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.8385339670580664e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0006928370567038655\n",
      "          total_loss: -0.0004658125981222838\n",
      "          vf_explained_var: 0.33856552839279175\n",
      "          vf_loss: 0.00022702630667481571\n",
      "    num_agent_steps_sampled: 1352000\n",
      "    num_agent_steps_trained: 1352000\n",
      "    num_steps_sampled: 1352000\n",
      "    num_steps_trained: 1352000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.70650684931506\n",
      "    ram_util_percent: 95.91643835616439\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09899473289158513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9179120735655617\n",
      "    mean_inference_ms: 1.9144622657794717\n",
      "    mean_raw_obs_processing_ms: 0.12173395240906924\n",
      "  time_since_restore: 73786.5919201374\n",
      "  time_this_iter_s: 218.6256811618805\n",
      "  time_total_s: 73786.5919201374\n",
      "  timers:\n",
      "    learn_throughput: 18.806\n",
      "    learn_time_ms: 212700.045\n",
      "    load_throughput: 1840575.742\n",
      "    load_time_ms: 2.173\n",
      "    sample_throughput: 17.916\n",
      "    sample_time_ms: 223259.883\n",
      "    update_time_ms: 14.238\n",
      "  timestamp: 1650292148\n",
      "  timesteps_since_restore: 1352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1352000\n",
      "  training_iteration: 338\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 15:32:37 (running for 20:33:57.06)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.14 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-32-49\n",
      "  done: false\n",
      "  episode_len_mean: 513.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.11\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3587\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7819505333900452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014095451682806015\n",
      "          model: {}\n",
      "          policy_loss: -0.056945521384477615\n",
      "          total_loss: 0.009139267727732658\n",
      "          vf_explained_var: 0.7313005924224854\n",
      "          vf_loss: 0.06608479470014572\n",
      "    num_agent_steps_sampled: 1356000\n",
      "    num_agent_steps_trained: 1356000\n",
      "    num_steps_sampled: 1356000\n",
      "    num_steps_trained: 1356000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.8989898989899\n",
      "    ram_util_percent: 95.77171717171716\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09793164339307252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9318789120648592\n",
      "    mean_inference_ms: 1.9056963407781513\n",
      "    mean_raw_obs_processing_ms: 0.17914335567420267\n",
      "  time_since_restore: 74011.88701868057\n",
      "  time_this_iter_s: 222.36630487442017\n",
      "  time_total_s: 74011.88701868057\n",
      "  timers:\n",
      "    learn_throughput: 18.865\n",
      "    learn_time_ms: 212027.322\n",
      "    load_throughput: 6480692.213\n",
      "    load_time_ms: 0.617\n",
      "    sample_throughput: 18.003\n",
      "    sample_time_ms: 222187.76\n",
      "    update_time_ms: 25.611\n",
      "  timestamp: 1650292369\n",
      "  timesteps_since_restore: 1356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1356000\n",
      "  training_iteration: 339\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-32-51\n",
      "  done: false\n",
      "  episode_len_mean: 1424.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 780\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128093607723713\n",
      "          total_loss: 0.014134032651782036\n",
      "          vf_explained_var: 2.8200046742199447e-09\n",
      "          vf_loss: 5.936439265497029e-06\n",
      "    num_agent_steps_sampled: 1356000\n",
      "    num_agent_steps_trained: 1356000\n",
      "    num_steps_sampled: 1356000\n",
      "    num_steps_trained: 1356000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.01689189189189\n",
      "    ram_util_percent: 95.74527027027027\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09899473289158513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9179120735655617\n",
      "    mean_inference_ms: 1.9144622657794717\n",
      "    mean_raw_obs_processing_ms: 0.12173395240906924\n",
      "  time_since_restore: 74008.60094618797\n",
      "  time_this_iter_s: 222.00902605056763\n",
      "  time_total_s: 74008.60094618797\n",
      "  timers:\n",
      "    learn_throughput: 18.825\n",
      "    learn_time_ms: 212484.626\n",
      "    load_throughput: 1823808.675\n",
      "    load_time_ms: 2.193\n",
      "    sample_throughput: 17.986\n",
      "    sample_time_ms: 222392.587\n",
      "    update_time_ms: 13.43\n",
      "  timestamp: 1650292371\n",
      "  timesteps_since_restore: 1356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1356000\n",
      "  training_iteration: 339\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-36-28\n",
      "  done: false\n",
      "  episode_len_mean: 508.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.04\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3596\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8498947024345398\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013108327984809875\n",
      "          model: {}\n",
      "          policy_loss: -0.04618202894926071\n",
      "          total_loss: 0.015071754343807697\n",
      "          vf_explained_var: 0.7479451298713684\n",
      "          vf_loss: 0.061253778636455536\n",
      "    num_agent_steps_sampled: 1360000\n",
      "    num_agent_steps_trained: 1360000\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.78088737201367\n",
      "    ram_util_percent: 95.82901023890786\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09787560435246129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9314947211068673\n",
      "    mean_inference_ms: 1.9039215304790833\n",
      "    mean_raw_obs_processing_ms: 0.17901627922934576\n",
      "  time_since_restore: 74231.41979265213\n",
      "  time_this_iter_s: 219.53277397155762\n",
      "  time_total_s: 74231.41979265213\n",
      "  timers:\n",
      "    learn_throughput: 18.912\n",
      "    learn_time_ms: 211504.576\n",
      "    load_throughput: 6525052.894\n",
      "    load_time_ms: 0.613\n",
      "    sample_throughput: 18.017\n",
      "    sample_time_ms: 222014.827\n",
      "    update_time_ms: 26.821\n",
      "  timestamp: 1650292588\n",
      "  timesteps_since_restore: 1360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 340\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-36-29\n",
      "  done: false\n",
      "  episode_len_mean: 1424.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 780\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.091566118463341e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6369134954272165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014127939939498901\n",
      "          total_loss: 0.014128216542303562\n",
      "          vf_explained_var: 4.427407418461371e-07\n",
      "          vf_loss: 2.7537001301425335e-07\n",
      "    num_agent_steps_sampled: 1360000\n",
      "    num_agent_steps_trained: 1360000\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.76609589041097\n",
      "    ram_util_percent: 95.82465753424658\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09899473289158513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9179120735655617\n",
      "    mean_inference_ms: 1.9144622657794717\n",
      "    mean_raw_obs_processing_ms: 0.12173395240906924\n",
      "  time_since_restore: 74227.09230208397\n",
      "  time_this_iter_s: 218.4913558959961\n",
      "  time_total_s: 74227.09230208397\n",
      "  timers:\n",
      "    learn_throughput: 18.877\n",
      "    learn_time_ms: 211901.11\n",
      "    load_throughput: 1544635.781\n",
      "    load_time_ms: 2.59\n",
      "    sample_throughput: 18.001\n",
      "    sample_time_ms: 222212.334\n",
      "    update_time_ms: 12.796\n",
      "  timestamp: 1650292589\n",
      "  timesteps_since_restore: 1360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 340\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-40-05\n",
      "  done: false\n",
      "  episode_len_mean: 517.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.19\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3603\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6377465128898621\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018117116764187813\n",
      "          model: {}\n",
      "          policy_loss: -0.04650174081325531\n",
      "          total_loss: 0.039791397750377655\n",
      "          vf_explained_var: 0.722694993019104\n",
      "          vf_loss: 0.08629314601421356\n",
      "    num_agent_steps_sampled: 1364000\n",
      "    num_agent_steps_trained: 1364000\n",
      "    num_steps_sampled: 1364000\n",
      "    num_steps_trained: 1364000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.32896551724137\n",
      "    ram_util_percent: 95.77379310344828\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09783272557721757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9311996791450656\n",
      "    mean_inference_ms: 1.9025450651772189\n",
      "    mean_raw_obs_processing_ms: 0.1789139545757481\n",
      "  time_since_restore: 74448.33786273003\n",
      "  time_this_iter_s: 216.91807007789612\n",
      "  time_total_s: 74448.33786273003\n",
      "  timers:\n",
      "    learn_throughput: 18.967\n",
      "    learn_time_ms: 210892.53\n",
      "    load_throughput: 6488963.837\n",
      "    load_time_ms: 0.616\n",
      "    sample_throughput: 18.061\n",
      "    sample_time_ms: 221476.71\n",
      "    update_time_ms: 28.969\n",
      "  timestamp: 1650292805\n",
      "  timesteps_since_restore: 1364000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1364000\n",
      "  training_iteration: 341\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-40-06\n",
      "  done: false\n",
      "  episode_len_mean: 1425.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 794\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.17362427872973e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.0772978578325685e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.00010436366574140266\n",
      "          total_loss: 0.00011654505942715332\n",
      "          vf_explained_var: 0.3341541588306427\n",
      "          vf_loss: 0.0002209093072451651\n",
      "    num_agent_steps_sampled: 1364000\n",
      "    num_agent_steps_trained: 1364000\n",
      "    num_steps_sampled: 1364000\n",
      "    num_steps_trained: 1364000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.26310344827586\n",
      "    ram_util_percent: 95.75137931034483\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09870564078726328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.915871603265675\n",
      "    mean_inference_ms: 1.9050241375937709\n",
      "    mean_raw_obs_processing_ms: 0.12143540639058582\n",
      "  time_since_restore: 74444.14463925362\n",
      "  time_this_iter_s: 217.05233716964722\n",
      "  time_total_s: 74444.14463925362\n",
      "  timers:\n",
      "    learn_throughput: 18.935\n",
      "    learn_time_ms: 211245.435\n",
      "    load_throughput: 1544394.061\n",
      "    load_time_ms: 2.59\n",
      "    sample_throughput: 18.046\n",
      "    sample_time_ms: 221660.689\n",
      "    update_time_ms: 11.726\n",
      "  timestamp: 1650292806\n",
      "  timesteps_since_restore: 1364000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1364000\n",
      "  training_iteration: 341\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 524.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.34\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3611\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7405923008918762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015025018714368343\n",
      "          model: {}\n",
      "          policy_loss: -0.044276513159275055\n",
      "          total_loss: 0.03222694993019104\n",
      "          vf_explained_var: 0.7017959952354431\n",
      "          vf_loss: 0.0765034556388855\n",
      "    num_agent_steps_sampled: 1368000\n",
      "    num_agent_steps_trained: 1368000\n",
      "    num_steps_sampled: 1368000\n",
      "    num_steps_trained: 1368000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.68006644518273\n",
      "    ram_util_percent: 95.7501661129568\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09778235507902494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9308542449750005\n",
      "    mean_inference_ms: 1.9009345155677595\n",
      "    mean_raw_obs_processing_ms: 0.17879174280613908\n",
      "  time_since_restore: 74673.73777985573\n",
      "  time_this_iter_s: 225.3999171257019\n",
      "  time_total_s: 74673.73777985573\n",
      "  timers:\n",
      "    learn_throughput: 18.942\n",
      "    learn_time_ms: 211176.16\n",
      "    load_throughput: 6980617.459\n",
      "    load_time_ms: 0.573\n",
      "    sample_throughput: 18.111\n",
      "    sample_time_ms: 220856.122\n",
      "    update_time_ms: 29.275\n",
      "  timestamp: 1650293031\n",
      "  timesteps_since_restore: 1368000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1368000\n",
      "  training_iteration: 342\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-43-53\n",
      "  done: false\n",
      "  episode_len_mean: 1134.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 801\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.071579525584916e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.1895766302050255e-27\n",
      "          model: {}\n",
      "          policy_loss: 2.7749358196160756e-05\n",
      "          total_loss: 7.39702009013854e-05\n",
      "          vf_explained_var: 0.14859530329704285\n",
      "          vf_loss: 4.622271444532089e-05\n",
      "    num_agent_steps_sampled: 1368000\n",
      "    num_agent_steps_trained: 1368000\n",
      "    num_steps_sampled: 1368000\n",
      "    num_steps_trained: 1368000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.03311258278146\n",
      "    ram_util_percent: 95.7291390728477\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09858727946616504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9150529195746253\n",
      "    mean_inference_ms: 1.9012407746727502\n",
      "    mean_raw_obs_processing_ms: 0.12133656030701706\n",
      "  time_since_restore: 74670.72602510452\n",
      "  time_this_iter_s: 226.58138585090637\n",
      "  time_total_s: 74670.72602510452\n",
      "  timers:\n",
      "    learn_throughput: 18.914\n",
      "    learn_time_ms: 211480.057\n",
      "    load_throughput: 1535250.366\n",
      "    load_time_ms: 2.605\n",
      "    sample_throughput: 18.097\n",
      "    sample_time_ms: 221033.524\n",
      "    update_time_ms: 11.906\n",
      "  timestamp: 1650293033\n",
      "  timesteps_since_restore: 1368000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1368000\n",
      "  training_iteration: 342\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-47-42\n",
      "  done: false\n",
      "  episode_len_mean: 521.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: 7.33\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3619\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7234982848167419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013524217531085014\n",
      "          model: {}\n",
      "          policy_loss: -0.045725345611572266\n",
      "          total_loss: 0.048792168498039246\n",
      "          vf_explained_var: 0.6882964968681335\n",
      "          vf_loss: 0.09451751410961151\n",
      "    num_agent_steps_sampled: 1372000\n",
      "    num_agent_steps_trained: 1372000\n",
      "    num_steps_sampled: 1372000\n",
      "    num_steps_trained: 1372000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.10548387096775\n",
      "    ram_util_percent: 95.87903225806451\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0977328029689712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9305210694946839\n",
      "    mean_inference_ms: 1.8993748969245257\n",
      "    mean_raw_obs_processing_ms: 0.17867303273850388\n",
      "  time_since_restore: 74905.3719637394\n",
      "  time_this_iter_s: 231.634183883667\n",
      "  time_total_s: 74905.3719637394\n",
      "  timers:\n",
      "    learn_throughput: 18.874\n",
      "    learn_time_ms: 211926.321\n",
      "    load_throughput: 6988177.274\n",
      "    load_time_ms: 0.572\n",
      "    sample_throughput: 18.081\n",
      "    sample_time_ms: 221229.329\n",
      "    update_time_ms: 32.079\n",
      "  timestamp: 1650293262\n",
      "  timesteps_since_restore: 1372000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1372000\n",
      "  training_iteration: 343\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-47-44\n",
      "  done: false\n",
      "  episode_len_mean: 1134.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 801\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.01412796787917614\n",
      "          total_loss: -0.014127946458756924\n",
      "          vf_explained_var: -5.951491743871884e-07\n",
      "          vf_loss: 2.7748708220087792e-08\n",
      "    num_agent_steps_sampled: 1372000\n",
      "    num_agent_steps_trained: 1372000\n",
      "    num_steps_sampled: 1372000\n",
      "    num_steps_trained: 1372000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.16612903225807\n",
      "    ram_util_percent: 95.8667741935484\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09858727946616504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9150529195746253\n",
      "    mean_inference_ms: 1.9012407746727502\n",
      "    mean_raw_obs_processing_ms: 0.12133656030701706\n",
      "  time_since_restore: 74902.19746899605\n",
      "  time_this_iter_s: 231.47144389152527\n",
      "  time_total_s: 74902.19746899605\n",
      "  timers:\n",
      "    learn_throughput: 18.859\n",
      "    learn_time_ms: 212097.786\n",
      "    load_throughput: 1417269.909\n",
      "    load_time_ms: 2.822\n",
      "    sample_throughput: 18.064\n",
      "    sample_time_ms: 221431.386\n",
      "    update_time_ms: 10.841\n",
      "  timestamp: 1650293264\n",
      "  timesteps_since_restore: 1372000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1372000\n",
      "  training_iteration: 343\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 15:49:17 (running for 20:50:37.51)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.33 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-51-26\n",
      "  done: false\n",
      "  episode_len_mean: 516.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.04\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3627\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7309678196907043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015684470534324646\n",
      "          model: {}\n",
      "          policy_loss: -0.045274768024683\n",
      "          total_loss: 0.027089983224868774\n",
      "          vf_explained_var: 0.68356853723526\n",
      "          vf_loss: 0.07236474752426147\n",
      "    num_agent_steps_sampled: 1376000\n",
      "    num_agent_steps_trained: 1376000\n",
      "    num_steps_sampled: 1376000\n",
      "    num_steps_trained: 1376000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.90401337792642\n",
      "    ram_util_percent: 95.54782608695652\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09768480857569434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9302013037974879\n",
      "    mean_inference_ms: 1.897857181664055\n",
      "    mean_raw_obs_processing_ms: 0.1785634570316865\n",
      "  time_since_restore: 75128.35145068169\n",
      "  time_this_iter_s: 222.97948694229126\n",
      "  time_total_s: 75128.35145068169\n",
      "  timers:\n",
      "    learn_throughput: 18.926\n",
      "    learn_time_ms: 211347.138\n",
      "    load_throughput: 7006563.374\n",
      "    load_time_ms: 0.571\n",
      "    sample_throughput: 18.014\n",
      "    sample_time_ms: 222044.701\n",
      "    update_time_ms: 26.905\n",
      "  timestamp: 1650293486\n",
      "  timesteps_since_restore: 1376000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1376000\n",
      "  training_iteration: 344\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-51-27\n",
      "  done: false\n",
      "  episode_len_mean: 1036.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 804\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1517660264248943e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.6075848949737032e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.004098379053175449\n",
      "          total_loss: -0.004087131470441818\n",
      "          vf_explained_var: -0.09677653759717941\n",
      "          vf_loss: 1.1246494977967814e-05\n",
      "    num_agent_steps_sampled: 1376000\n",
      "    num_agent_steps_trained: 1376000\n",
      "    num_steps_sampled: 1376000\n",
      "    num_steps_trained: 1376000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.38127090301003\n",
      "    ram_util_percent: 95.53779264214047\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09854297160051241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9147448105732089\n",
      "    mean_inference_ms: 1.8997945597863173\n",
      "    mean_raw_obs_processing_ms: 0.12130405609777808\n",
      "  time_since_restore: 75125.28504800797\n",
      "  time_this_iter_s: 223.08757901191711\n",
      "  time_total_s: 75125.28504800797\n",
      "  timers:\n",
      "    learn_throughput: 18.915\n",
      "    learn_time_ms: 211475.945\n",
      "    load_throughput: 1973813.339\n",
      "    load_time_ms: 2.027\n",
      "    sample_throughput: 18.016\n",
      "    sample_time_ms: 222019.012\n",
      "    update_time_ms: 10.747\n",
      "  timestamp: 1650293487\n",
      "  timesteps_since_restore: 1376000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1376000\n",
      "  training_iteration: 344\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-55-06\n",
      "  done: false\n",
      "  episode_len_mean: 509.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 6.94\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3635\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7336012125015259\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01495579443871975\n",
      "          model: {}\n",
      "          policy_loss: -0.04877796769142151\n",
      "          total_loss: 0.030395731329917908\n",
      "          vf_explained_var: 0.7570126056671143\n",
      "          vf_loss: 0.07917370647192001\n",
      "    num_agent_steps_sampled: 1380000\n",
      "    num_agent_steps_trained: 1380000\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.8891891891892\n",
      "    ram_util_percent: 95.51993243243244\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09763784387229336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9298959449502695\n",
      "    mean_inference_ms: 1.8963847076652143\n",
      "    mean_raw_obs_processing_ms: 0.17845839529017538\n",
      "  time_since_restore: 75348.90429353714\n",
      "  time_this_iter_s: 220.5528428554535\n",
      "  time_total_s: 75348.90429353714\n",
      "  timers:\n",
      "    learn_throughput: 18.917\n",
      "    learn_time_ms: 211453.666\n",
      "    load_throughput: 6873654.539\n",
      "    load_time_ms: 0.582\n",
      "    sample_throughput: 18.065\n",
      "    sample_time_ms: 221420.44\n",
      "    update_time_ms: 27.226\n",
      "  timestamp: 1650293706\n",
      "  timesteps_since_restore: 1380000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 345\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-55-08\n",
      "  done: false\n",
      "  episode_len_mean: 1036.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 806\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.5791959690782153e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.7971311227308085e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0016691932687535882\n",
      "          total_loss: 0.0016796640120446682\n",
      "          vf_explained_var: -0.07127214968204498\n",
      "          vf_loss: 1.0470745110069402e-05\n",
      "    num_agent_steps_sampled: 1380000\n",
      "    num_agent_steps_trained: 1380000\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.82432432432432\n",
      "    ram_util_percent: 95.52466216216216\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09851524546588256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9145539303375076\n",
      "    mean_inference_ms: 1.8988990476200696\n",
      "    mean_raw_obs_processing_ms: 0.12128613041707798\n",
      "  time_since_restore: 75346.10632705688\n",
      "  time_this_iter_s: 220.82127904891968\n",
      "  time_total_s: 75346.10632705688\n",
      "  timers:\n",
      "    learn_throughput: 18.908\n",
      "    learn_time_ms: 211551.424\n",
      "    load_throughput: 1976021.86\n",
      "    load_time_ms: 2.024\n",
      "    sample_throughput: 18.063\n",
      "    sample_time_ms: 221441.393\n",
      "    update_time_ms: 10.114\n",
      "  timestamp: 1650293708\n",
      "  timesteps_since_restore: 1380000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 345\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-58-41\n",
      "  done: false\n",
      "  episode_len_mean: 510.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3643\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.680812656879425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01680549420416355\n",
      "          model: {}\n",
      "          policy_loss: -0.04639868810772896\n",
      "          total_loss: 0.026306262239813805\n",
      "          vf_explained_var: 0.7230419516563416\n",
      "          vf_loss: 0.07270494848489761\n",
      "    num_agent_steps_sampled: 1384000\n",
      "    num_agent_steps_trained: 1384000\n",
      "    num_steps_sampled: 1384000\n",
      "    num_steps_trained: 1384000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.62812500000001\n",
      "    ram_util_percent: 95.40729166666667\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09759189567514691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9296035885977707\n",
      "    mean_inference_ms: 1.894976274342661\n",
      "    mean_raw_obs_processing_ms: 0.17835499257397525\n",
      "  time_since_restore: 75563.75863051414\n",
      "  time_this_iter_s: 214.854336977005\n",
      "  time_total_s: 75563.75863051414\n",
      "  timers:\n",
      "    learn_throughput: 18.923\n",
      "    learn_time_ms: 211388.488\n",
      "    load_throughput: 6922436.046\n",
      "    load_time_ms: 0.578\n",
      "    sample_throughput: 18.054\n",
      "    sample_time_ms: 221551.734\n",
      "    update_time_ms: 27.449\n",
      "  timestamp: 1650293921\n",
      "  timesteps_since_restore: 1384000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1384000\n",
      "  training_iteration: 346\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_15-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 1036.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 806\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128153212368488\n",
      "          total_loss: 0.014128188602626324\n",
      "          vf_explained_var: -4.388311936054379e-07\n",
      "          vf_loss: 4.049084623147792e-08\n",
      "    num_agent_steps_sampled: 1384000\n",
      "    num_agent_steps_trained: 1384000\n",
      "    num_steps_sampled: 1384000\n",
      "    num_steps_trained: 1384000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.34236111111112\n",
      "    ram_util_percent: 95.40763888888888\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09851524546588256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9145539303375076\n",
      "    mean_inference_ms: 1.8988990476200696\n",
      "    mean_raw_obs_processing_ms: 0.12128613041707798\n",
      "  time_since_restore: 75560.36257314682\n",
      "  time_this_iter_s: 214.2562460899353\n",
      "  time_total_s: 75560.36257314682\n",
      "  timers:\n",
      "    learn_throughput: 18.913\n",
      "    learn_time_ms: 211494.099\n",
      "    load_throughput: 3105569.109\n",
      "    load_time_ms: 1.288\n",
      "    sample_throughput: 18.056\n",
      "    sample_time_ms: 221535.117\n",
      "    update_time_ms: 10.009\n",
      "  timestamp: 1650293923\n",
      "  timesteps_since_restore: 1384000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1384000\n",
      "  training_iteration: 346\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-02-18\n",
      "  done: false\n",
      "  episode_len_mean: 520.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3650\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6619998216629028\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0185199324041605\n",
      "          model: {}\n",
      "          policy_loss: -0.037933528423309326\n",
      "          total_loss: 0.031006217002868652\n",
      "          vf_explained_var: 0.7657176852226257\n",
      "          vf_loss: 0.06893975287675858\n",
      "    num_agent_steps_sampled: 1388000\n",
      "    num_agent_steps_trained: 1388000\n",
      "    num_steps_sampled: 1388000\n",
      "    num_steps_trained: 1388000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.74620689655173\n",
      "    ram_util_percent: 95.64758620689656\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0975510036465931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9293407486457074\n",
      "    mean_inference_ms: 1.8937111640524775\n",
      "    mean_raw_obs_processing_ms: 0.17825932618344786\n",
      "  time_since_restore: 75780.23599863052\n",
      "  time_this_iter_s: 216.47736811637878\n",
      "  time_total_s: 75780.23599863052\n",
      "  timers:\n",
      "    learn_throughput: 18.972\n",
      "    learn_time_ms: 210832.041\n",
      "    load_throughput: 6887764.184\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 18.061\n",
      "    sample_time_ms: 221477.337\n",
      "    update_time_ms: 25.605\n",
      "  timestamp: 1650294138\n",
      "  timesteps_since_restore: 1388000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1388000\n",
      "  training_iteration: 347\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-02-20\n",
      "  done: false\n",
      "  episode_len_mean: 843.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 811\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.481889672991745e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.958122710816844e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0017730953404679894\n",
      "          total_loss: -0.0014946499140933156\n",
      "          vf_explained_var: 0.12832818925380707\n",
      "          vf_loss: 0.00027844341821037233\n",
      "    num_agent_steps_sampled: 1388000\n",
      "    num_agent_steps_trained: 1388000\n",
      "    num_steps_sampled: 1388000\n",
      "    num_steps_trained: 1388000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.6948275862069\n",
      "    ram_util_percent: 95.66724137931033\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09845411188362321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9141417178383705\n",
      "    mean_inference_ms: 1.8969567538160086\n",
      "    mean_raw_obs_processing_ms: 0.12125636726318241\n",
      "  time_since_restore: 75777.26204013824\n",
      "  time_this_iter_s: 216.89946699142456\n",
      "  time_total_s: 75777.26204013824\n",
      "  timers:\n",
      "    learn_throughput: 18.96\n",
      "    learn_time_ms: 210973.686\n",
      "    load_throughput: 3224589.364\n",
      "    load_time_ms: 1.24\n",
      "    sample_throughput: 18.06\n",
      "    sample_time_ms: 221484.339\n",
      "    update_time_ms: 9.508\n",
      "  timestamp: 1650294140\n",
      "  timesteps_since_restore: 1388000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1388000\n",
      "  training_iteration: 347\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 538.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.51\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3656\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5827415585517883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016178611665964127\n",
      "          model: {}\n",
      "          policy_loss: -0.04950970038771629\n",
      "          total_loss: 0.0032187809702008963\n",
      "          vf_explained_var: 0.805686891078949\n",
      "          vf_loss: 0.05272848159074783\n",
      "    num_agent_steps_sampled: 1392000\n",
      "    num_agent_steps_trained: 1392000\n",
      "    num_steps_sampled: 1392000\n",
      "    num_steps_trained: 1392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.09119718309859\n",
      "    ram_util_percent: 95.5031690140845\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09751393765300385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9291044102863424\n",
      "    mean_inference_ms: 1.892571161119214\n",
      "    mean_raw_obs_processing_ms: 0.17817274107306233\n",
      "  time_since_restore: 75992.00536465645\n",
      "  time_this_iter_s: 211.76936602592468\n",
      "  time_total_s: 75992.00536465645\n",
      "  timers:\n",
      "    learn_throughput: 19.025\n",
      "    learn_time_ms: 210248.972\n",
      "    load_throughput: 7079292.797\n",
      "    load_time_ms: 0.565\n",
      "    sample_throughput: 18.107\n",
      "    sample_time_ms: 220906.06\n",
      "    update_time_ms: 27.962\n",
      "  timestamp: 1650294350\n",
      "  timesteps_since_restore: 1392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1392000\n",
      "  training_iteration: 348\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-05-52\n",
      "  done: false\n",
      "  episode_len_mean: 843.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 811\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0022939261980354786\n",
      "          total_loss: 0.002293926663696766\n",
      "          vf_explained_var: 5.256681106402539e-06\n",
      "          vf_loss: 7.406611990390388e-10\n",
      "    num_agent_steps_sampled: 1392000\n",
      "    num_agent_steps_trained: 1392000\n",
      "    num_steps_sampled: 1392000\n",
      "    num_steps_trained: 1392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.94295774647887\n",
      "    ram_util_percent: 95.49894366197182\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09845411188362321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9141417178383705\n",
      "    mean_inference_ms: 1.8969567538160086\n",
      "    mean_raw_obs_processing_ms: 0.12125636726318241\n",
      "  time_since_restore: 75989.42123699188\n",
      "  time_this_iter_s: 212.1591968536377\n",
      "  time_total_s: 75989.42123699188\n",
      "  timers:\n",
      "    learn_throughput: 19.011\n",
      "    learn_time_ms: 210400.743\n",
      "    load_throughput: 3244230.963\n",
      "    load_time_ms: 1.233\n",
      "    sample_throughput: 18.108\n",
      "    sample_time_ms: 220890.912\n",
      "    update_time_ms: 9.889\n",
      "  timestamp: 1650294352\n",
      "  timesteps_since_restore: 1392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1392000\n",
      "  training_iteration: 348\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 16:05:58 (running for 21:07:18.04)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.51 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-09-24\n",
      "  done: false\n",
      "  episode_len_mean: 534.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3665\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7850093841552734\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014815869741141796\n",
      "          model: {}\n",
      "          policy_loss: -0.046159062534570694\n",
      "          total_loss: 0.03884581848978996\n",
      "          vf_explained_var: 0.758166491985321\n",
      "          vf_loss: 0.08500487357378006\n",
      "    num_agent_steps_sampled: 1396000\n",
      "    num_agent_steps_trained: 1396000\n",
      "    num_steps_sampled: 1396000\n",
      "    num_steps_trained: 1396000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.13157894736842\n",
      "    ram_util_percent: 95.64877192982456\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09745841307700776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9287548909635399\n",
      "    mean_inference_ms: 1.8908929874276381\n",
      "    mean_raw_obs_processing_ms: 0.1780472566467376\n",
      "  time_since_restore: 76205.96439433098\n",
      "  time_this_iter_s: 213.95902967453003\n",
      "  time_total_s: 76205.96439433098\n",
      "  timers:\n",
      "    learn_throughput: 19.098\n",
      "    learn_time_ms: 209450.431\n",
      "    load_throughput: 7054289.198\n",
      "    load_time_ms: 0.567\n",
      "    sample_throughput: 18.158\n",
      "    sample_time_ms: 220288.404\n",
      "    update_time_ms: 24.952\n",
      "  timestamp: 1650294564\n",
      "  timesteps_since_restore: 1396000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1396000\n",
      "  training_iteration: 349\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-09-25\n",
      "  done: false\n",
      "  episode_len_mean: 843.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 811\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4082793872685338e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.787362189755294e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.0010602325201034546\n",
      "          total_loss: 0.0010602325201034546\n",
      "          vf_explained_var: -2.355985770918778e-06\n",
      "          vf_loss: 8.52068277046314e-11\n",
      "    num_agent_steps_sampled: 1396000\n",
      "    num_agent_steps_trained: 1396000\n",
      "    num_steps_sampled: 1396000\n",
      "    num_steps_trained: 1396000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.83649122807019\n",
      "    ram_util_percent: 95.63508771929826\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09845411188362321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9141417178383705\n",
      "    mean_inference_ms: 1.8969567538160086\n",
      "    mean_raw_obs_processing_ms: 0.12125636726318241\n",
      "  time_since_restore: 76202.97974705696\n",
      "  time_this_iter_s: 213.55851006507874\n",
      "  time_total_s: 76202.97974705696\n",
      "  timers:\n",
      "    learn_throughput: 19.083\n",
      "    learn_time_ms: 209606.779\n",
      "    load_throughput: 2482828.349\n",
      "    load_time_ms: 1.611\n",
      "    sample_throughput: 18.16\n",
      "    sample_time_ms: 220269.211\n",
      "    update_time_ms: 9.561\n",
      "  timestamp: 1650294565\n",
      "  timesteps_since_restore: 1396000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1396000\n",
      "  training_iteration: 349\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-12-55\n",
      "  done: false\n",
      "  episode_len_mean: 529.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3674\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7348189353942871\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015278041362762451\n",
      "          model: {}\n",
      "          policy_loss: -0.0465986505150795\n",
      "          total_loss: 0.04577621445059776\n",
      "          vf_explained_var: 0.6363183259963989\n",
      "          vf_loss: 0.09237486124038696\n",
      "    num_agent_steps_sampled: 1400000\n",
      "    num_agent_steps_trained: 1400000\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.21802120141344\n",
      "    ram_util_percent: 95.5053003533569\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09740381160155556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9284110098405453\n",
      "    mean_inference_ms: 1.8892383150912437\n",
      "    mean_raw_obs_processing_ms: 0.17792808210479613\n",
      "  time_since_restore: 76417.72799253464\n",
      "  time_this_iter_s: 211.76359820365906\n",
      "  time_total_s: 76417.72799253464\n",
      "  timers:\n",
      "    learn_throughput: 19.167\n",
      "    learn_time_ms: 208689.087\n",
      "    load_throughput: 7105978.823\n",
      "    load_time_ms: 0.563\n",
      "    sample_throughput: 18.225\n",
      "    sample_time_ms: 219474.915\n",
      "    update_time_ms: 25.031\n",
      "  timestamp: 1650294775\n",
      "  timesteps_since_restore: 1400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 350\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-12-58\n",
      "  done: false\n",
      "  episode_len_mean: 941.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 815\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.2256305409394947e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1050716390701745e-26\n",
      "          model: {}\n",
      "          policy_loss: 3.274342816439457e-05\n",
      "          total_loss: 0.00011399998766137287\n",
      "          vf_explained_var: 0.041729580610990524\n",
      "          vf_loss: 8.125800377456471e-05\n",
      "    num_agent_steps_sampled: 1400000\n",
      "    num_agent_steps_trained: 1400000\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.73226950354609\n",
      "    ram_util_percent: 95.48368794326241\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09840724931997732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9138337610449608\n",
      "    mean_inference_ms: 1.8954993145537902\n",
      "    mean_raw_obs_processing_ms: 0.12123373528348416\n",
      "  time_since_restore: 76415.26373124123\n",
      "  time_this_iter_s: 212.28398418426514\n",
      "  time_total_s: 76415.26373124123\n",
      "  timers:\n",
      "    learn_throughput: 19.142\n",
      "    learn_time_ms: 208968.275\n",
      "    load_throughput: 3265129.712\n",
      "    load_time_ms: 1.225\n",
      "    sample_throughput: 18.223\n",
      "    sample_time_ms: 219503.739\n",
      "    update_time_ms: 8.732\n",
      "  timestamp: 1650294778\n",
      "  timesteps_since_restore: 1400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 350\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1404000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-16-28\n",
      "  done: false\n",
      "  episode_len_mean: 538.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3681\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.652489185333252\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015871120616793633\n",
      "          model: {}\n",
      "          policy_loss: -0.04581815004348755\n",
      "          total_loss: 0.02053966373205185\n",
      "          vf_explained_var: 0.7390786409378052\n",
      "          vf_loss: 0.0663578137755394\n",
      "    num_agent_steps_sampled: 1404000\n",
      "    num_agent_steps_trained: 1404000\n",
      "    num_steps_sampled: 1404000\n",
      "    num_steps_trained: 1404000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.50494699646643\n",
      "    ram_util_percent: 95.43533568904593\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0973612238520754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9281439761915458\n",
      "    mean_inference_ms: 1.8879438224052407\n",
      "    mean_raw_obs_processing_ms: 0.17783105625983836\n",
      "  time_since_restore: 76629.88345551491\n",
      "  time_this_iter_s: 212.15546298027039\n",
      "  time_total_s: 76629.88345551491\n",
      "  timers:\n",
      "    learn_throughput: 19.218\n",
      "    learn_time_ms: 208135.26\n",
      "    load_throughput: 7218801.256\n",
      "    load_time_ms: 0.554\n",
      "    sample_throughput: 18.283\n",
      "    sample_time_ms: 218787.821\n",
      "    update_time_ms: 23.573\n",
      "  timestamp: 1650294988\n",
      "  timesteps_since_restore: 1404000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1404000\n",
      "  training_iteration: 351\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1404000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-16-30\n",
      "  done: false\n",
      "  episode_len_mean: 941.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 815\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.012868044897913933\n",
      "          total_loss: 0.012868048623204231\n",
      "          vf_explained_var: -2.924344926213962e-06\n",
      "          vf_loss: 1.233333257744107e-08\n",
      "    num_agent_steps_sampled: 1404000\n",
      "    num_agent_steps_trained: 1404000\n",
      "    num_steps_sampled: 1404000\n",
      "    num_steps_trained: 1404000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.3419014084507\n",
      "    ram_util_percent: 95.43556338028168\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09840724931997732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9138337610449608\n",
      "    mean_inference_ms: 1.8954993145537902\n",
      "    mean_raw_obs_processing_ms: 0.12123373528348416\n",
      "  time_since_restore: 76627.61697411537\n",
      "  time_this_iter_s: 212.3532428741455\n",
      "  time_total_s: 76627.61697411537\n",
      "  timers:\n",
      "    learn_throughput: 19.185\n",
      "    learn_time_ms: 208498.084\n",
      "    load_throughput: 3110693.811\n",
      "    load_time_ms: 1.286\n",
      "    sample_throughput: 18.276\n",
      "    sample_time_ms: 218862.681\n",
      "    update_time_ms: 9.3\n",
      "  timestamp: 1650294990\n",
      "  timesteps_since_restore: 1404000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1404000\n",
      "  training_iteration: 351\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-19-59\n",
      "  done: false\n",
      "  episode_len_mean: 540.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3689\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7429940700531006\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015571803785860538\n",
      "          model: {}\n",
      "          policy_loss: -0.045661572366952896\n",
      "          total_loss: 0.031064607203006744\n",
      "          vf_explained_var: 0.7740674614906311\n",
      "          vf_loss: 0.07672616839408875\n",
      "    num_agent_steps_sampled: 1408000\n",
      "    num_agent_steps_trained: 1408000\n",
      "    num_steps_sampled: 1408000\n",
      "    num_steps_trained: 1408000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.30283687943263\n",
      "    ram_util_percent: 95.56063829787234\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09731062969845908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9278319738056579\n",
      "    mean_inference_ms: 1.8864503067914347\n",
      "    mean_raw_obs_processing_ms: 0.17771526849293173\n",
      "  time_since_restore: 76841.36165046692\n",
      "  time_this_iter_s: 211.4781949520111\n",
      "  time_total_s: 76841.36165046692\n",
      "  timers:\n",
      "    learn_throughput: 19.353\n",
      "    learn_time_ms: 206691.066\n",
      "    load_throughput: 7296345.134\n",
      "    load_time_ms: 0.548\n",
      "    sample_throughput: 18.325\n",
      "    sample_time_ms: 218284.849\n",
      "    update_time_ms: 24.128\n",
      "  timestamp: 1650295199\n",
      "  timesteps_since_restore: 1408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1408000\n",
      "  training_iteration: 352\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-20-01\n",
      "  done: false\n",
      "  episode_len_mean: 1038.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 816\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1384491940446148e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7977564176933487e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0178717952221632\n",
      "          total_loss: -0.017870116978883743\n",
      "          vf_explained_var: 0.016804732382297516\n",
      "          vf_loss: 1.6762669474701397e-06\n",
      "    num_agent_steps_sampled: 1408000\n",
      "    num_agent_steps_trained: 1408000\n",
      "    num_steps_sampled: 1408000\n",
      "    num_steps_trained: 1408000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.10854092526691\n",
      "    ram_util_percent: 95.53558718861211\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09839448803781908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.913750551732561\n",
      "    mean_inference_ms: 1.8951068131131004\n",
      "    mean_raw_obs_processing_ms: 0.1212264137080483\n",
      "  time_since_restore: 76838.59082341194\n",
      "  time_this_iter_s: 210.97384929656982\n",
      "  time_total_s: 76838.59082341194\n",
      "  timers:\n",
      "    learn_throughput: 19.327\n",
      "    learn_time_ms: 206959.477\n",
      "    load_throughput: 3175157.743\n",
      "    load_time_ms: 1.26\n",
      "    sample_throughput: 18.317\n",
      "    sample_time_ms: 218374.098\n",
      "    update_time_ms: 9.063\n",
      "  timestamp: 1650295201\n",
      "  timesteps_since_restore: 1408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1408000\n",
      "  training_iteration: 352\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 16:22:39 (running for 21:23:59.00)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.41 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1412000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-23-30\n",
      "  done: false\n",
      "  episode_len_mean: 556.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.8\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3695\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5999850630760193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018396977335214615\n",
      "          model: {}\n",
      "          policy_loss: -0.03256804496049881\n",
      "          total_loss: 0.05124316364526749\n",
      "          vf_explained_var: 0.747033953666687\n",
      "          vf_loss: 0.0838111937046051\n",
      "    num_agent_steps_sampled: 1412000\n",
      "    num_agent_steps_trained: 1412000\n",
      "    num_steps_sampled: 1412000\n",
      "    num_steps_trained: 1412000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.28078291814947\n",
      "    ram_util_percent: 95.60035587188612\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09727200511826796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9275948017823762\n",
      "    mean_inference_ms: 1.885308401620179\n",
      "    mean_raw_obs_processing_ms: 0.17762451693344744\n",
      "  time_since_restore: 77052.2371532917\n",
      "  time_this_iter_s: 210.87550282478333\n",
      "  time_total_s: 77052.2371532917\n",
      "  timers:\n",
      "    learn_throughput: 19.542\n",
      "    learn_time_ms: 204686.492\n",
      "    load_throughput: 7310970.891\n",
      "    load_time_ms: 0.547\n",
      "    sample_throughput: 18.453\n",
      "    sample_time_ms: 216770.535\n",
      "    update_time_ms: 21.785\n",
      "  timestamp: 1650295410\n",
      "  timesteps_since_restore: 1412000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1412000\n",
      "  training_iteration: 353\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1412000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 1038.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 816\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128176495432854\n",
      "          total_loss: 0.014138270169496536\n",
      "          vf_explained_var: -3.46091488800937e-09\n",
      "          vf_loss: 1.0097698577737901e-05\n",
      "    num_agent_steps_sampled: 1412000\n",
      "    num_agent_steps_trained: 1412000\n",
      "    num_steps_sampled: 1412000\n",
      "    num_steps_trained: 1412000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.5220640569395\n",
      "    ram_util_percent: 95.58078291814948\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09839448803781908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.913750551732561\n",
      "    mean_inference_ms: 1.8951068131131004\n",
      "    mean_raw_obs_processing_ms: 0.1212264137080483\n",
      "  time_since_restore: 77049.34287643433\n",
      "  time_this_iter_s: 210.75205302238464\n",
      "  time_total_s: 77049.34287643433\n",
      "  timers:\n",
      "    learn_throughput: 19.513\n",
      "    learn_time_ms: 204995.477\n",
      "    load_throughput: 3857274.629\n",
      "    load_time_ms: 1.037\n",
      "    sample_throughput: 18.457\n",
      "    sample_time_ms: 216725.168\n",
      "    update_time_ms: 8.351\n",
      "  timestamp: 1650295412\n",
      "  timesteps_since_restore: 1412000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1412000\n",
      "  training_iteration: 353\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-27-01\n",
      "  done: false\n",
      "  episode_len_mean: 550.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.74\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3703\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7313105463981628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01684236340224743\n",
      "          model: {}\n",
      "          policy_loss: -0.038670413196086884\n",
      "          total_loss: 0.049805622547864914\n",
      "          vf_explained_var: 0.7244218587875366\n",
      "          vf_loss: 0.0884760394692421\n",
      "    num_agent_steps_sampled: 1416000\n",
      "    num_agent_steps_trained: 1416000\n",
      "    num_steps_sampled: 1416000\n",
      "    num_steps_trained: 1416000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.94270462633453\n",
      "    ram_util_percent: 95.71103202846976\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09722032289590736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.927278995881327\n",
      "    mean_inference_ms: 1.883804665863051\n",
      "    mean_raw_obs_processing_ms: 0.17750508150536157\n",
      "  time_since_restore: 77263.17637324333\n",
      "  time_this_iter_s: 210.93921995162964\n",
      "  time_total_s: 77263.17637324333\n",
      "  timers:\n",
      "    learn_throughput: 19.653\n",
      "    learn_time_ms: 203527.205\n",
      "    load_throughput: 7337830.651\n",
      "    load_time_ms: 0.545\n",
      "    sample_throughput: 18.629\n",
      "    sample_time_ms: 214722.373\n",
      "    update_time_ms: 21.178\n",
      "  timestamp: 1650295621\n",
      "  timesteps_since_restore: 1416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1416000\n",
      "  training_iteration: 354\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-27-03\n",
      "  done: false\n",
      "  episode_len_mean: 1038.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 816\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128302223980427\n",
      "          total_loss: 0.014132440090179443\n",
      "          vf_explained_var: 3.46091488800937e-09\n",
      "          vf_loss: 4.13605494031799e-06\n",
      "    num_agent_steps_sampled: 1416000\n",
      "    num_agent_steps_trained: 1416000\n",
      "    num_steps_sampled: 1416000\n",
      "    num_steps_trained: 1416000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.39255319148937\n",
      "    ram_util_percent: 95.69822695035461\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09839448803781908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.913750551732561\n",
      "    mean_inference_ms: 1.8951068131131004\n",
      "    mean_raw_obs_processing_ms: 0.1212264137080483\n",
      "  time_since_restore: 77260.7317442894\n",
      "  time_this_iter_s: 211.38886785507202\n",
      "  time_total_s: 77260.7317442894\n",
      "  timers:\n",
      "    learn_throughput: 19.618\n",
      "    learn_time_ms: 203893.878\n",
      "    load_throughput: 3907766.986\n",
      "    load_time_ms: 1.024\n",
      "    sample_throughput: 18.631\n",
      "    sample_time_ms: 214693.98\n",
      "    update_time_ms: 8.507\n",
      "  timestamp: 1650295623\n",
      "  timesteps_since_restore: 1416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1416000\n",
      "  training_iteration: 354\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-30-45\n",
      "  done: false\n",
      "  episode_len_mean: 543.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.54\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3712\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7884781956672668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01612376794219017\n",
      "          model: {}\n",
      "          policy_loss: -0.045195139944553375\n",
      "          total_loss: 0.026958679780364037\n",
      "          vf_explained_var: 0.7321726083755493\n",
      "          vf_loss: 0.07215380668640137\n",
      "    num_agent_steps_sampled: 1420000\n",
      "    num_agent_steps_trained: 1420000\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.12046979865772\n",
      "    ram_util_percent: 95.93053691275169\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09716448312815794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9269448793550589\n",
      "    mean_inference_ms: 1.882215758967707\n",
      "    mean_raw_obs_processing_ms: 0.1773818225465768\n",
      "  time_since_restore: 77487.33100223541\n",
      "  time_this_iter_s: 224.1546289920807\n",
      "  time_total_s: 77487.33100223541\n",
      "  timers:\n",
      "    learn_throughput: 19.628\n",
      "    learn_time_ms: 203791.714\n",
      "    load_throughput: 7395079.12\n",
      "    load_time_ms: 0.541\n",
      "    sample_throughput: 18.721\n",
      "    sample_time_ms: 213658.915\n",
      "    update_time_ms: 18.714\n",
      "  timestamp: 1650295845\n",
      "  timesteps_since_restore: 1420000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 355\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 1134.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 823\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.5864856274733095e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.0303375228125856e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0006621258216910064\n",
      "          total_loss: 0.0008435823838226497\n",
      "          vf_explained_var: 0.00486365333199501\n",
      "          vf_loss: 0.00018145937065128237\n",
      "    num_agent_steps_sampled: 1420000\n",
      "    num_agent_steps_trained: 1420000\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.4989898989899\n",
      "    ram_util_percent: 95.90673400673401\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09829715474775788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.913126612297967\n",
      "    mean_inference_ms: 1.892149967198967\n",
      "    mean_raw_obs_processing_ms: 0.12115800517240205\n",
      "  time_since_restore: 77484.11214327812\n",
      "  time_this_iter_s: 223.38039898872375\n",
      "  time_total_s: 77484.11214327812\n",
      "  timers:\n",
      "    learn_throughput: 19.598\n",
      "    learn_time_ms: 204105.191\n",
      "    load_throughput: 3902494.941\n",
      "    load_time_ms: 1.025\n",
      "    sample_throughput: 18.724\n",
      "    sample_time_ms: 213633.627\n",
      "    update_time_ms: 8.649\n",
      "  timestamp: 1650295847\n",
      "  timesteps_since_restore: 1420000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 355\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-34-31\n",
      "  done: false\n",
      "  episode_len_mean: 542.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.48\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3722\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7974709868431091\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0151289701461792\n",
      "          model: {}\n",
      "          policy_loss: -0.05152580514550209\n",
      "          total_loss: 0.010225670412182808\n",
      "          vf_explained_var: 0.6942762732505798\n",
      "          vf_loss: 0.06175146996974945\n",
      "    num_agent_steps_sampled: 1424000\n",
      "    num_agent_steps_trained: 1424000\n",
      "    num_steps_sampled: 1424000\n",
      "    num_steps_trained: 1424000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.2569536423841\n",
      "    ram_util_percent: 95.6387417218543\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09710307278722359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9265743420684507\n",
      "    mean_inference_ms: 1.8804597404625303\n",
      "    mean_raw_obs_processing_ms: 0.17724652055471757\n",
      "  time_since_restore: 77712.47648906708\n",
      "  time_this_iter_s: 225.14548683166504\n",
      "  time_total_s: 77712.47648906708\n",
      "  timers:\n",
      "    learn_throughput: 19.524\n",
      "    learn_time_ms: 204878.179\n",
      "    load_throughput: 7337188.839\n",
      "    load_time_ms: 0.545\n",
      "    sample_throughput: 18.703\n",
      "    sample_time_ms: 213865.086\n",
      "    update_time_ms: 21.918\n",
      "  timestamp: 1650296071\n",
      "  timesteps_since_restore: 1424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1424000\n",
      "  training_iteration: 356\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-34-32\n",
      "  done: false\n",
      "  episode_len_mean: 1134.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 823\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.010208015330135822\n",
      "          total_loss: 0.010208015330135822\n",
      "          vf_explained_var: -2.0212382878526114e-06\n",
      "          vf_loss: 6.572534072546432e-09\n",
      "    num_agent_steps_sampled: 1424000\n",
      "    num_agent_steps_trained: 1424000\n",
      "    num_steps_sampled: 1424000\n",
      "    num_steps_trained: 1424000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.87242524916942\n",
      "    ram_util_percent: 95.65714285714287\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09829715474775788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.913126612297967\n",
      "    mean_inference_ms: 1.892149967198967\n",
      "    mean_raw_obs_processing_ms: 0.12115800517240205\n",
      "  time_since_restore: 77709.23380923271\n",
      "  time_this_iter_s: 225.12166595458984\n",
      "  time_total_s: 77709.23380923271\n",
      "  timers:\n",
      "    learn_throughput: 19.49\n",
      "    learn_time_ms: 205237.642\n",
      "    load_throughput: 3591474.933\n",
      "    load_time_ms: 1.114\n",
      "    sample_throughput: 18.709\n",
      "    sample_time_ms: 213798.977\n",
      "    update_time_ms: 9.014\n",
      "  timestamp: 1650296072\n",
      "  timesteps_since_restore: 1424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1424000\n",
      "  training_iteration: 356\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1428000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-38-07\n",
      "  done: false\n",
      "  episode_len_mean: 533.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: 7.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3729\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8321832418441772\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013136745430529118\n",
      "          model: {}\n",
      "          policy_loss: -0.050685152411460876\n",
      "          total_loss: 0.01008857786655426\n",
      "          vf_explained_var: 0.7827594876289368\n",
      "          vf_loss: 0.06077372282743454\n",
      "    num_agent_steps_sampled: 1428000\n",
      "    num_agent_steps_trained: 1428000\n",
      "    num_steps_sampled: 1428000\n",
      "    num_steps_trained: 1428000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.4446366782007\n",
      "    ram_util_percent: 95.52629757785466\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09706001424295346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9263138512226533\n",
      "    mean_inference_ms: 1.8792219223522724\n",
      "    mean_raw_obs_processing_ms: 0.17715053006778297\n",
      "  time_since_restore: 77928.2275249958\n",
      "  time_this_iter_s: 215.7510359287262\n",
      "  time_total_s: 77928.2275249958\n",
      "  timers:\n",
      "    learn_throughput: 19.532\n",
      "    learn_time_ms: 204791.365\n",
      "    load_throughput: 7303968.655\n",
      "    load_time_ms: 0.548\n",
      "    sample_throughput: 18.607\n",
      "    sample_time_ms: 214970.312\n",
      "    update_time_ms: 22.109\n",
      "  timestamp: 1650296287\n",
      "  timesteps_since_restore: 1428000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1428000\n",
      "  training_iteration: 357\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1428000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-38-07\n",
      "  done: false\n",
      "  episode_len_mean: 1134.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 823\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.003436039900407195\n",
      "          total_loss: 0.003436039900407195\n",
      "          vf_explained_var: -6.59003035252681e-06\n",
      "          vf_loss: 9.750766860605609e-10\n",
      "    num_agent_steps_sampled: 1428000\n",
      "    num_agent_steps_trained: 1428000\n",
      "    num_steps_sampled: 1428000\n",
      "    num_steps_trained: 1428000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.49515570934257\n",
      "    ram_util_percent: 95.53737024221454\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09829715474775788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.913126612297967\n",
      "    mean_inference_ms: 1.892149967198967\n",
      "    mean_raw_obs_processing_ms: 0.12115800517240205\n",
      "  time_since_restore: 77924.72924613953\n",
      "  time_this_iter_s: 215.49543690681458\n",
      "  time_total_s: 77924.72924613953\n",
      "  timers:\n",
      "    learn_throughput: 19.499\n",
      "    learn_time_ms: 205133.489\n",
      "    load_throughput: 3613209.571\n",
      "    load_time_ms: 1.107\n",
      "    sample_throughput: 18.614\n",
      "    sample_time_ms: 214892.767\n",
      "    update_time_ms: 8.988\n",
      "  timestamp: 1650296287\n",
      "  timesteps_since_restore: 1428000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1428000\n",
      "  training_iteration: 357\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 16:39:19 (running for 21:40:39.28)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.29 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-41-50\n",
      "  done: false\n",
      "  episode_len_mean: 530.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.17\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3738\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7211142182350159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0147912772372365\n",
      "          model: {}\n",
      "          policy_loss: -0.045510489493608475\n",
      "          total_loss: 0.0289252121001482\n",
      "          vf_explained_var: 0.7763241529464722\n",
      "          vf_loss: 0.07443570345640182\n",
      "    num_agent_steps_sampled: 1432000\n",
      "    num_agent_steps_trained: 1432000\n",
      "    num_steps_sampled: 1432000\n",
      "    num_steps_trained: 1432000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.28456375838925\n",
      "    ram_util_percent: 95.86812080536913\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09700334942740937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9259743290629199\n",
      "    mean_inference_ms: 1.8776169461452432\n",
      "    mean_raw_obs_processing_ms: 0.1770303199706685\n",
      "  time_since_restore: 78152.05848908424\n",
      "  time_this_iter_s: 223.83096408843994\n",
      "  time_total_s: 78152.05848908424\n",
      "  timers:\n",
      "    learn_throughput: 19.416\n",
      "    learn_time_ms: 206010.519\n",
      "    load_throughput: 7280197.874\n",
      "    load_time_ms: 0.549\n",
      "    sample_throughput: 18.617\n",
      "    sample_time_ms: 214862.568\n",
      "    update_time_ms: 27.136\n",
      "  timestamp: 1650296510\n",
      "  timesteps_since_restore: 1432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1432000\n",
      "  training_iteration: 358\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 1230.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 826\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.9359561238389805e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.583799988979521e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.000333117030095309\n",
      "          total_loss: -0.0002260988694615662\n",
      "          vf_explained_var: -0.06472448259592056\n",
      "          vf_loss: 0.00010701615974539891\n",
      "    num_agent_steps_sampled: 1432000\n",
      "    num_agent_steps_trained: 1432000\n",
      "    num_steps_sampled: 1432000\n",
      "    num_steps_trained: 1432000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.65536912751679\n",
      "    ram_util_percent: 95.85167785234898\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09825148158789336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9128341330700316\n",
      "    mean_inference_ms: 1.890768344004345\n",
      "    mean_raw_obs_processing_ms: 0.12111898073412802\n",
      "  time_since_restore: 78148.7773668766\n",
      "  time_this_iter_s: 224.0481207370758\n",
      "  time_total_s: 78148.7773668766\n",
      "  timers:\n",
      "    learn_throughput: 19.385\n",
      "    learn_time_ms: 206344.708\n",
      "    load_throughput: 3634106.485\n",
      "    load_time_ms: 1.101\n",
      "    sample_throughput: 18.625\n",
      "    sample_time_ms: 214767.495\n",
      "    update_time_ms: 8.619\n",
      "  timestamp: 1650296512\n",
      "  timesteps_since_restore: 1432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1432000\n",
      "  training_iteration: 358\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1436000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-45-49\n",
      "  done: false\n",
      "  episode_len_mean: 538.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.34\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3745\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7479526400566101\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019513435661792755\n",
      "          model: {}\n",
      "          policy_loss: -0.03845595940947533\n",
      "          total_loss: 0.03907468169927597\n",
      "          vf_explained_var: 0.7423855662345886\n",
      "          vf_loss: 0.07753065228462219\n",
      "    num_agent_steps_sampled: 1436000\n",
      "    num_agent_steps_trained: 1436000\n",
      "    num_steps_sampled: 1436000\n",
      "    num_steps_trained: 1436000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.17272727272727\n",
      "    ram_util_percent: 95.87429467084638\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09695974398052916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9257070807355626\n",
      "    mean_inference_ms: 1.8763594107097086\n",
      "    mean_raw_obs_processing_ms: 0.17693622330574638\n",
      "  time_since_restore: 78390.96652793884\n",
      "  time_this_iter_s: 238.908038854599\n",
      "  time_total_s: 78390.96652793884\n",
      "  timers:\n",
      "    learn_throughput: 19.181\n",
      "    learn_time_ms: 208536.838\n",
      "    load_throughput: 7307149.826\n",
      "    load_time_ms: 0.547\n",
      "    sample_throughput: 18.514\n",
      "    sample_time_ms: 216058.494\n",
      "    update_time_ms: 32.349\n",
      "  timestamp: 1650296749\n",
      "  timesteps_since_restore: 1436000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1436000\n",
      "  training_iteration: 359\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1436000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-45-50\n",
      "  done: false\n",
      "  episode_len_mean: 1230.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 826\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.456906072571781e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.68953841411823e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.005261782091110945\n",
      "          total_loss: 0.005261782091110945\n",
      "          vf_explained_var: 2.0063050669705262e-06\n",
      "          vf_loss: 1.6650175682642043e-09\n",
      "    num_agent_steps_sampled: 1436000\n",
      "    num_agent_steps_trained: 1436000\n",
      "    num_steps_sampled: 1436000\n",
      "    num_steps_trained: 1436000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.88238993710691\n",
      "    ram_util_percent: 95.89654088050312\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09825148158789336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9128341330700316\n",
      "    mean_inference_ms: 1.890768344004345\n",
      "    mean_raw_obs_processing_ms: 0.12111898073412802\n",
      "  time_since_restore: 78387.04359793663\n",
      "  time_this_iter_s: 238.26623106002808\n",
      "  time_total_s: 78387.04359793663\n",
      "  timers:\n",
      "    learn_throughput: 19.156\n",
      "    learn_time_ms: 208813.817\n",
      "    load_throughput: 5544171.045\n",
      "    load_time_ms: 0.721\n",
      "    sample_throughput: 18.521\n",
      "    sample_time_ms: 215976.565\n",
      "    update_time_ms: 11.98\n",
      "  timestamp: 1650296750\n",
      "  timesteps_since_restore: 1436000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1436000\n",
      "  training_iteration: 359\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-49-44\n",
      "  done: false\n",
      "  episode_len_mean: 532.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.18\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3753\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6753448247909546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017894092947244644\n",
      "          model: {}\n",
      "          policy_loss: -0.05650732293725014\n",
      "          total_loss: 0.0009099570452235639\n",
      "          vf_explained_var: 0.7715069651603699\n",
      "          vf_loss: 0.05741728097200394\n",
      "    num_agent_steps_sampled: 1440000\n",
      "    num_agent_steps_trained: 1440000\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.81570512820512\n",
      "    ram_util_percent: 95.87307692307692\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09691083426906356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9254017330377371\n",
      "    mean_inference_ms: 1.8749253692598222\n",
      "    mean_raw_obs_processing_ms: 0.17683034589968868\n",
      "  time_since_restore: 78624.93822097778\n",
      "  time_this_iter_s: 233.97169303894043\n",
      "  time_total_s: 78624.93822097778\n",
      "  timers:\n",
      "    learn_throughput: 18.976\n",
      "    learn_time_ms: 210793.315\n",
      "    load_throughput: 7218490.663\n",
      "    load_time_ms: 0.554\n",
      "    sample_throughput: 18.302\n",
      "    sample_time_ms: 218553.992\n",
      "    update_time_ms: 35.754\n",
      "  timestamp: 1650296984\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 360\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-49-44\n",
      "  done: false\n",
      "  episode_len_mean: 1324.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 829\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.368411705701444e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.6897057774303972e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.02366284281015396\n",
      "          total_loss: 0.023675324395298958\n",
      "          vf_explained_var: -0.03208880499005318\n",
      "          vf_loss: 1.2482320926210377e-05\n",
      "    num_agent_steps_sampled: 1440000\n",
      "    num_agent_steps_trained: 1440000\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.46346153846153\n",
      "    ram_util_percent: 95.87467948717948\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09820275291228722\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9125214782241267\n",
      "    mean_inference_ms: 1.8892956057267134\n",
      "    mean_raw_obs_processing_ms: 0.12107505704345976\n",
      "  time_since_restore: 78621.33569812775\n",
      "  time_this_iter_s: 234.29210019111633\n",
      "  time_total_s: 78621.33569812775\n",
      "  timers:\n",
      "    learn_throughput: 18.953\n",
      "    learn_time_ms: 211051.915\n",
      "    load_throughput: 5459378.478\n",
      "    load_time_ms: 0.733\n",
      "    sample_throughput: 18.314\n",
      "    sample_time_ms: 218411.211\n",
      "    update_time_ms: 12.445\n",
      "  timestamp: 1650296984\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 360\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1444000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-53-34\n",
      "  done: false\n",
      "  episode_len_mean: 525.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.12\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3761\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.770419716835022\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014295323751866817\n",
      "          model: {}\n",
      "          policy_loss: -0.0416884683072567\n",
      "          total_loss: 0.024837329983711243\n",
      "          vf_explained_var: 0.7852417230606079\n",
      "          vf_loss: 0.06652579456567764\n",
      "    num_agent_steps_sampled: 1444000\n",
      "    num_agent_steps_trained: 1444000\n",
      "    num_steps_sampled: 1444000\n",
      "    num_steps_trained: 1444000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.2685064935065\n",
      "    ram_util_percent: 95.96461038961039\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09686330705578766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.925099007977856\n",
      "    mean_inference_ms: 1.8735221433795284\n",
      "    mean_raw_obs_processing_ms: 0.17672717078579883\n",
      "  time_since_restore: 78855.62994408607\n",
      "  time_this_iter_s: 230.69172310829163\n",
      "  time_total_s: 78855.62994408607\n",
      "  timers:\n",
      "    learn_throughput: 18.808\n",
      "    learn_time_ms: 212673.649\n",
      "    load_throughput: 7169444.041\n",
      "    load_time_ms: 0.558\n",
      "    sample_throughput: 18.117\n",
      "    sample_time_ms: 220789.795\n",
      "    update_time_ms: 38.71\n",
      "  timestamp: 1650297214\n",
      "  timesteps_since_restore: 1444000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1444000\n",
      "  training_iteration: 361\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1444000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-53-35\n",
      "  done: false\n",
      "  episode_len_mean: 1326.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 834\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1889640800774625e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8246015314835746e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.00036926509346812963\n",
      "          total_loss: -0.0002938892866950482\n",
      "          vf_explained_var: 0.06427481025457382\n",
      "          vf_loss: 7.537513738498092e-05\n",
      "    num_agent_steps_sampled: 1444000\n",
      "    num_agent_steps_trained: 1444000\n",
      "    num_steps_sampled: 1444000\n",
      "    num_steps_trained: 1444000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.05439739413683\n",
      "    ram_util_percent: 95.97687296416939\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09811921128340653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9119833236006082\n",
      "    mean_inference_ms: 1.886758544931019\n",
      "    mean_raw_obs_processing_ms: 0.12100088903440076\n",
      "  time_since_restore: 78851.65383315086\n",
      "  time_this_iter_s: 230.31813502311707\n",
      "  time_total_s: 78851.65383315086\n",
      "  timers:\n",
      "    learn_throughput: 18.789\n",
      "    learn_time_ms: 212890.735\n",
      "    load_throughput: 5289326.902\n",
      "    load_time_ms: 0.756\n",
      "    sample_throughput: 18.132\n",
      "    sample_time_ms: 220604.658\n",
      "    update_time_ms: 12.358\n",
      "  timestamp: 1650297215\n",
      "  timesteps_since_restore: 1444000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1444000\n",
      "  training_iteration: 361\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 16:55:59 (running for 21:57:19.34)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.12 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-57-28\n",
      "  done: false\n",
      "  episode_len_mean: 530.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.23\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3769\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6584980487823486\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01555037871003151\n",
      "          model: {}\n",
      "          policy_loss: -0.05438445881009102\n",
      "          total_loss: -0.007320111617445946\n",
      "          vf_explained_var: 0.8022962212562561\n",
      "          vf_loss: 0.047064345329999924\n",
      "    num_agent_steps_sampled: 1448000\n",
      "    num_agent_steps_trained: 1448000\n",
      "    num_steps_sampled: 1448000\n",
      "    num_steps_trained: 1448000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.14903846153847\n",
      "    ram_util_percent: 95.97916666666667\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09681646110063705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9247926173006601\n",
      "    mean_inference_ms: 1.8720973010851147\n",
      "    mean_raw_obs_processing_ms: 0.1766190351738513\n",
      "  time_since_restore: 79089.63914012909\n",
      "  time_this_iter_s: 234.00919604301453\n",
      "  time_total_s: 79089.63914012909\n",
      "  timers:\n",
      "    learn_throughput: 18.607\n",
      "    learn_time_ms: 214967.682\n",
      "    load_throughput: 7075710.008\n",
      "    load_time_ms: 0.565\n",
      "    sample_throughput: 17.967\n",
      "    sample_time_ms: 222632.301\n",
      "    update_time_ms: 39.138\n",
      "  timestamp: 1650297448\n",
      "  timesteps_since_restore: 1448000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1448000\n",
      "  training_iteration: 362\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_16-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 1326.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 834\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.243035067262464e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.709387152407484e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.01412808708846569\n",
      "          total_loss: -0.014119738712906837\n",
      "          vf_explained_var: 5.9604645663569045e-09\n",
      "          vf_loss: 8.335008715221193e-06\n",
      "    num_agent_steps_sampled: 1448000\n",
      "    num_agent_steps_trained: 1448000\n",
      "    num_steps_sampled: 1448000\n",
      "    num_steps_trained: 1448000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.73525641025641\n",
      "    ram_util_percent: 96.00576923076925\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09811921128340653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9119833236006082\n",
      "    mean_inference_ms: 1.886758544931019\n",
      "    mean_raw_obs_processing_ms: 0.12100088903440076\n",
      "  time_since_restore: 79085.73729538918\n",
      "  time_this_iter_s: 234.08346223831177\n",
      "  time_total_s: 79085.73729538918\n",
      "  timers:\n",
      "    learn_throughput: 18.587\n",
      "    learn_time_ms: 215203.376\n",
      "    load_throughput: 5203851.117\n",
      "    load_time_ms: 0.769\n",
      "    sample_throughput: 17.982\n",
      "    sample_time_ms: 222440.135\n",
      "    update_time_ms: 12.473\n",
      "  timestamp: 1650297449\n",
      "  timesteps_since_restore: 1448000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1448000\n",
      "  training_iteration: 362\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1452000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-01-26\n",
      "  done: false\n",
      "  episode_len_mean: 540.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.43\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3775\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0728835775353218e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6308113932609558\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021336229518055916\n",
      "          model: {}\n",
      "          policy_loss: -0.04300465062260628\n",
      "          total_loss: 0.01709006167948246\n",
      "          vf_explained_var: 0.798694372177124\n",
      "          vf_loss: 0.06009471416473389\n",
      "    num_agent_steps_sampled: 1452000\n",
      "    num_agent_steps_trained: 1452000\n",
      "    num_steps_sampled: 1452000\n",
      "    num_steps_trained: 1452000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.70599369085173\n",
      "    ram_util_percent: 95.98864353312304\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0967810290758493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9245555002448748\n",
      "    mean_inference_ms: 1.8709940083708658\n",
      "    mean_raw_obs_processing_ms: 0.1765341231049164\n",
      "  time_since_restore: 79326.90435099602\n",
      "  time_this_iter_s: 237.2652108669281\n",
      "  time_total_s: 79326.90435099602\n",
      "  timers:\n",
      "    learn_throughput: 18.38\n",
      "    learn_time_ms: 217625.35\n",
      "    load_throughput: 7014766.066\n",
      "    load_time_ms: 0.57\n",
      "    sample_throughput: 17.785\n",
      "    sample_time_ms: 224911.886\n",
      "    update_time_ms: 40.745\n",
      "  timestamp: 1650297686\n",
      "  timesteps_since_restore: 1452000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1452000\n",
      "  training_iteration: 363\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1452000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-01-27\n",
      "  done: false\n",
      "  episode_len_mean: 1424.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 836\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.4241416094900038e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0287559604156526e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0075949132442474365\n",
      "          total_loss: -0.007563972380012274\n",
      "          vf_explained_var: -0.06785217672586441\n",
      "          vf_loss: 3.093573104706593e-05\n",
      "    num_agent_steps_sampled: 1452000\n",
      "    num_agent_steps_trained: 1452000\n",
      "    num_steps_sampled: 1452000\n",
      "    num_steps_trained: 1452000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.33438485804416\n",
      "    ram_util_percent: 96.00157728706624\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09808395903427558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9117544651028421\n",
      "    mean_inference_ms: 1.88568348280259\n",
      "    mean_raw_obs_processing_ms: 0.12096806657894657\n",
      "  time_since_restore: 79323.7013862133\n",
      "  time_this_iter_s: 237.9640908241272\n",
      "  time_total_s: 79323.7013862133\n",
      "  timers:\n",
      "    learn_throughput: 18.353\n",
      "    learn_time_ms: 217945.667\n",
      "    load_throughput: 5235844.334\n",
      "    load_time_ms: 0.764\n",
      "    sample_throughput: 17.799\n",
      "    sample_time_ms: 224733.476\n",
      "    update_time_ms: 12.613\n",
      "  timestamp: 1650297687\n",
      "  timesteps_since_restore: 1452000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1452000\n",
      "  training_iteration: 363\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 1424.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 836\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.00888586975634098\n",
      "          total_loss: 0.00888586975634098\n",
      "          vf_explained_var: -1.1662001497825258e-06\n",
      "          vf_loss: 4.507507700424185e-09\n",
      "    num_agent_steps_sampled: 1456000\n",
      "    num_agent_steps_trained: 1456000\n",
      "    num_steps_sampled: 1456000\n",
      "    num_steps_trained: 1456000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.92163009404389\n",
      "    ram_util_percent: 95.83510971786833\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09808395903427558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9117544651028421\n",
      "    mean_inference_ms: 1.88568348280259\n",
      "    mean_raw_obs_processing_ms: 0.12096806657894657\n",
      "  time_since_restore: 79563.58237409592\n",
      "  time_this_iter_s: 239.88098788261414\n",
      "  time_total_s: 79563.58237409592\n",
      "  timers:\n",
      "    learn_throughput: 18.122\n",
      "    learn_time_ms: 220727.852\n",
      "    load_throughput: 4965436.25\n",
      "    load_time_ms: 0.806\n",
      "    sample_throughput: 17.579\n",
      "    sample_time_ms: 227540.547\n",
      "    update_time_ms: 14.113\n",
      "  timestamp: 1650297927\n",
      "  timesteps_since_restore: 1456000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1456000\n",
      "  training_iteration: 364\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 539.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.39\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3782\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6093254373572563e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5702950954437256\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020999876782298088\n",
      "          model: {}\n",
      "          policy_loss: -0.04214955493807793\n",
      "          total_loss: 0.03488963097333908\n",
      "          vf_explained_var: 0.7809996604919434\n",
      "          vf_loss: 0.07703918218612671\n",
      "    num_agent_steps_sampled: 1456000\n",
      "    num_agent_steps_trained: 1456000\n",
      "    num_steps_sampled: 1456000\n",
      "    num_steps_trained: 1456000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.3778816199377\n",
      "    ram_util_percent: 95.8233644859813\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09674062870478024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9242789224044352\n",
      "    mean_inference_ms: 1.869718806497474\n",
      "    mean_raw_obs_processing_ms: 0.17643722968476297\n",
      "  time_since_restore: 79567.76546192169\n",
      "  time_this_iter_s: 240.86111092567444\n",
      "  time_total_s: 79567.76546192169\n",
      "  timers:\n",
      "    learn_throughput: 18.135\n",
      "    learn_time_ms: 220570.068\n",
      "    load_throughput: 6965835.998\n",
      "    load_time_ms: 0.574\n",
      "    sample_throughput: 17.574\n",
      "    sample_time_ms: 227614.591\n",
      "    update_time_ms: 41.191\n",
      "  timestamp: 1650297927\n",
      "  timesteps_since_restore: 1456000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1456000\n",
      "  training_iteration: 364\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-09-12\n",
      "  done: false\n",
      "  episode_len_mean: 1424.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 836\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.003908977843821049\n",
      "          total_loss: 0.003908977843821049\n",
      "          vf_explained_var: -5.351599696723497e-08\n",
      "          vf_loss: 9.508599463359246e-10\n",
      "    num_agent_steps_sampled: 1460000\n",
      "    num_agent_steps_trained: 1460000\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.79039735099337\n",
      "    ram_util_percent: 95.70331125827813\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09808395903427558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9117544651028421\n",
      "    mean_inference_ms: 1.88568348280259\n",
      "    mean_raw_obs_processing_ms: 0.12096806657894657\n",
      "  time_since_restore: 79788.48862195015\n",
      "  time_this_iter_s: 224.9062478542328\n",
      "  time_total_s: 79788.48862195015\n",
      "  timers:\n",
      "    learn_throughput: 18.099\n",
      "    learn_time_ms: 221011.056\n",
      "    load_throughput: 4972205.56\n",
      "    load_time_ms: 0.804\n",
      "    sample_throughput: 17.376\n",
      "    sample_time_ms: 230196.145\n",
      "    update_time_ms: 16.124\n",
      "  timestamp: 1650298152\n",
      "  timesteps_since_restore: 1460000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 365\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-09-12\n",
      "  done: false\n",
      "  episode_len_mean: 541.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.44\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3790\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.413988227090158e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7455483675003052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01988166570663452\n",
      "          model: {}\n",
      "          policy_loss: -0.03598380088806152\n",
      "          total_loss: 0.07786975800991058\n",
      "          vf_explained_var: 0.672568678855896\n",
      "          vf_loss: 0.11385355144739151\n",
      "    num_agent_steps_sampled: 1460000\n",
      "    num_agent_steps_trained: 1460000\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.83597359735974\n",
      "    ram_util_percent: 95.70858085808581\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09669469926842543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9239627556276594\n",
      "    mean_inference_ms: 1.868253278558159\n",
      "    mean_raw_obs_processing_ms: 0.17632767131930882\n",
      "  time_since_restore: 79793.18737506866\n",
      "  time_this_iter_s: 225.42191314697266\n",
      "  time_total_s: 79793.18737506866\n",
      "  timers:\n",
      "    learn_throughput: 18.115\n",
      "    learn_time_ms: 220811.312\n",
      "    load_throughput: 7076903.868\n",
      "    load_time_ms: 0.565\n",
      "    sample_throughput: 17.358\n",
      "    sample_time_ms: 230443.972\n",
      "    update_time_ms: 40.145\n",
      "  timestamp: 1650298152\n",
      "  timesteps_since_restore: 1460000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 365\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 17:12:39 (running for 22:13:59.65)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.44 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 1429.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 840\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.372157314647138e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.9962819086301706e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0022400955203920603\n",
      "          total_loss: -0.0019453826826065779\n",
      "          vf_explained_var: 0.1319963037967682\n",
      "          vf_loss: 0.0002947123721241951\n",
      "    num_agent_steps_sampled: 1464000\n",
      "    num_agent_steps_trained: 1464000\n",
      "    num_steps_sampled: 1464000\n",
      "    num_steps_trained: 1464000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.68543689320389\n",
      "    ram_util_percent: 95.7326860841424\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0980131487605622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9112946309589094\n",
      "    mean_inference_ms: 1.8835125765815488\n",
      "    mean_raw_obs_processing_ms: 0.12090122405204401\n",
      "  time_since_restore: 80021.72417092323\n",
      "  time_this_iter_s: 233.2355489730835\n",
      "  time_total_s: 80021.72417092323\n",
      "  timers:\n",
      "    learn_throughput: 18.036\n",
      "    learn_time_ms: 221778.045\n",
      "    load_throughput: 5612798.501\n",
      "    load_time_ms: 0.713\n",
      "    sample_throughput: 17.352\n",
      "    sample_time_ms: 230519.702\n",
      "    update_time_ms: 22.142\n",
      "  timestamp: 1650298385\n",
      "  timesteps_since_restore: 1464000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1464000\n",
      "  training_iteration: 366\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 539.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.31\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3796\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.413988227090158e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6680153608322144\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016104107722640038\n",
      "          model: {}\n",
      "          policy_loss: -0.04057913273572922\n",
      "          total_loss: 0.04294436424970627\n",
      "          vf_explained_var: 0.767475962638855\n",
      "          vf_loss: 0.08352349698543549\n",
      "    num_agent_steps_sampled: 1464000\n",
      "    num_agent_steps_trained: 1464000\n",
      "    num_steps_sampled: 1464000\n",
      "    num_steps_trained: 1464000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.70450160771703\n",
      "    ram_util_percent: 95.7282958199357\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09666138437545158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9237274844069734\n",
      "    mean_inference_ms: 1.8671659709520685\n",
      "    mean_raw_obs_processing_ms: 0.1762475332881639\n",
      "  time_since_restore: 80026.3726272583\n",
      "  time_this_iter_s: 233.18525218963623\n",
      "  time_total_s: 80026.3726272583\n",
      "  timers:\n",
      "    learn_throughput: 18.05\n",
      "    learn_time_ms: 221610.913\n",
      "    load_throughput: 6992546.159\n",
      "    load_time_ms: 0.572\n",
      "    sample_throughput: 17.34\n",
      "    sample_time_ms: 230682.35\n",
      "    update_time_ms: 36.825\n",
      "  timestamp: 1650298385\n",
      "  timesteps_since_restore: 1464000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1464000\n",
      "  training_iteration: 366\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1468000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-16-48\n",
      "  done: false\n",
      "  episode_len_mean: 1429.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 840\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0223031486489103e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.735102420806745e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128110371530056\n",
      "          total_loss: -0.014123249799013138\n",
      "          vf_explained_var: 1.0094335323174164e-07\n",
      "          vf_loss: 4.856757641391596e-06\n",
      "    num_agent_steps_sampled: 1468000\n",
      "    num_agent_steps_trained: 1468000\n",
      "    num_steps_sampled: 1468000\n",
      "    num_steps_trained: 1468000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.86889632107024\n",
      "    ram_util_percent: 95.56789297658862\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0980131487605622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9112946309589094\n",
      "    mean_inference_ms: 1.8835125765815488\n",
      "    mean_raw_obs_processing_ms: 0.12090122405204401\n",
      "  time_since_restore: 80244.71387004852\n",
      "  time_this_iter_s: 222.98969912528992\n",
      "  time_total_s: 80244.71387004852\n",
      "  timers:\n",
      "    learn_throughput: 17.977\n",
      "    learn_time_ms: 222511.794\n",
      "    load_throughput: 5515555.263\n",
      "    load_time_ms: 0.725\n",
      "    sample_throughput: 17.292\n",
      "    sample_time_ms: 231319.248\n",
      "    update_time_ms: 23.111\n",
      "  timestamp: 1650298608\n",
      "  timesteps_since_restore: 1468000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1468000\n",
      "  training_iteration: 367\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1468000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-16-48\n",
      "  done: false\n",
      "  episode_len_mean: 551.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.56\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3803\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.413988227090158e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6735447645187378\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016927413642406464\n",
      "          model: {}\n",
      "          policy_loss: -0.049527306109666824\n",
      "          total_loss: 0.031455010175704956\n",
      "          vf_explained_var: 0.7148310542106628\n",
      "          vf_loss: 0.08098231256008148\n",
      "    num_agent_steps_sampled: 1468000\n",
      "    num_agent_steps_trained: 1468000\n",
      "    num_steps_sampled: 1468000\n",
      "    num_steps_trained: 1468000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.43946488294314\n",
      "    ram_util_percent: 95.59498327759198\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09662275736688784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9234528728225834\n",
      "    mean_inference_ms: 1.8658946162232775\n",
      "    mean_raw_obs_processing_ms: 0.17615315074180304\n",
      "  time_since_restore: 80249.35277938843\n",
      "  time_this_iter_s: 222.98015213012695\n",
      "  time_total_s: 80249.35277938843\n",
      "  timers:\n",
      "    learn_throughput: 17.993\n",
      "    learn_time_ms: 222307.624\n",
      "    load_throughput: 7017113.221\n",
      "    load_time_ms: 0.57\n",
      "    sample_throughput: 17.278\n",
      "    sample_time_ms: 231502.141\n",
      "    update_time_ms: 36.124\n",
      "  timestamp: 1650298608\n",
      "  timesteps_since_restore: 1468000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1468000\n",
      "  training_iteration: 367\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-20-26\n",
      "  done: false\n",
      "  episode_len_mean: 1429.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.32\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 840\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0223031486489103e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.735102420806745e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128157868981361\n",
      "          total_loss: -0.014126588590443134\n",
      "          vf_explained_var: -2.7430955285012715e-08\n",
      "          vf_loss: 1.5715830841145362e-06\n",
      "    num_agent_steps_sampled: 1472000\n",
      "    num_agent_steps_trained: 1472000\n",
      "    num_steps_sampled: 1472000\n",
      "    num_steps_trained: 1472000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.73058419243986\n",
      "    ram_util_percent: 95.68144329896909\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0980131487605622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9112946309589094\n",
      "    mean_inference_ms: 1.8835125765815488\n",
      "    mean_raw_obs_processing_ms: 0.12090122405204401\n",
      "  time_since_restore: 80462.2960639\n",
      "  time_this_iter_s: 217.58219385147095\n",
      "  time_total_s: 80462.2960639\n",
      "  timers:\n",
      "    learn_throughput: 18.028\n",
      "    learn_time_ms: 221871.52\n",
      "    load_throughput: 5464357.229\n",
      "    load_time_ms: 0.732\n",
      "    sample_throughput: 17.238\n",
      "    sample_time_ms: 232049.23\n",
      "    update_time_ms: 23.632\n",
      "  timestamp: 1650298826\n",
      "  timesteps_since_restore: 1472000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1472000\n",
      "  training_iteration: 368\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-20-27\n",
      "  done: false\n",
      "  episode_len_mean: 551.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.58\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3811\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.413988227090158e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7214893102645874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015176775865256786\n",
      "          model: {}\n",
      "          policy_loss: -0.0467245988547802\n",
      "          total_loss: 0.0196439977735281\n",
      "          vf_explained_var: 0.7591060400009155\n",
      "          vf_loss: 0.06636859476566315\n",
      "    num_agent_steps_sampled: 1472000\n",
      "    num_agent_steps_trained: 1472000\n",
      "    num_steps_sampled: 1472000\n",
      "    num_steps_trained: 1472000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.55910652920961\n",
      "    ram_util_percent: 95.70756013745705\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09657743336047428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9231303139714708\n",
      "    mean_inference_ms: 1.8644014704504093\n",
      "    mean_raw_obs_processing_ms: 0.17604148894539956\n",
      "  time_since_restore: 80467.54939341545\n",
      "  time_this_iter_s: 218.19661402702332\n",
      "  time_total_s: 80467.54939341545\n",
      "  timers:\n",
      "    learn_throughput: 18.04\n",
      "    learn_time_ms: 221727.094\n",
      "    load_throughput: 7063198.754\n",
      "    load_time_ms: 0.566\n",
      "    sample_throughput: 17.225\n",
      "    sample_time_ms: 232223.887\n",
      "    update_time_ms: 28.129\n",
      "  timestamp: 1650298827\n",
      "  timesteps_since_restore: 1472000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1472000\n",
      "  training_iteration: 368\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1476000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-24-07\n",
      "  done: false\n",
      "  episode_len_mean: 1427.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 847\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.7757114644009402e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.052355882249586e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0007502026855945587\n",
      "          total_loss: 0.0008138042758218944\n",
      "          vf_explained_var: 0.12906628847122192\n",
      "          vf_loss: 6.35942560620606e-05\n",
      "    num_agent_steps_sampled: 1476000\n",
      "    num_agent_steps_trained: 1476000\n",
      "    num_steps_sampled: 1476000\n",
      "    num_steps_trained: 1476000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.94898648648649\n",
      "    ram_util_percent: 95.74054054054054\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09789338295680912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9105349256263909\n",
      "    mean_inference_ms: 1.8798784630914402\n",
      "    mean_raw_obs_processing_ms: 0.12078626153415664\n",
      "  time_since_restore: 80683.53641009331\n",
      "  time_this_iter_s: 221.2403461933136\n",
      "  time_total_s: 80683.53641009331\n",
      "  timers:\n",
      "    learn_throughput: 18.167\n",
      "    learn_time_ms: 220177.528\n",
      "    load_throughput: 5550040.028\n",
      "    load_time_ms: 0.721\n",
      "    sample_throughput: 17.286\n",
      "    sample_time_ms: 231403.237\n",
      "    update_time_ms: 23.86\n",
      "  timestamp: 1650299047\n",
      "  timesteps_since_restore: 1476000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1476000\n",
      "  training_iteration: 369\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1476000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-24-08\n",
      "  done: false\n",
      "  episode_len_mean: 567.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 7.95\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3817\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.413988227090158e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6193326115608215\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02141142636537552\n",
      "          model: {}\n",
      "          policy_loss: -0.04661807417869568\n",
      "          total_loss: 0.03179242089390755\n",
      "          vf_explained_var: 0.7138757109642029\n",
      "          vf_loss: 0.07841048389673233\n",
      "    num_agent_steps_sampled: 1476000\n",
      "    num_agent_steps_trained: 1476000\n",
      "    num_steps_sampled: 1476000\n",
      "    num_steps_trained: 1476000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.54579124579124\n",
      "    ram_util_percent: 95.73905723905725\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09654276354360007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9228847603747934\n",
      "    mean_inference_ms: 1.8632629225551347\n",
      "    mean_raw_obs_processing_ms: 0.17595527292617721\n",
      "  time_since_restore: 80688.84858250618\n",
      "  time_this_iter_s: 221.29918909072876\n",
      "  time_total_s: 80688.84858250618\n",
      "  timers:\n",
      "    learn_throughput: 18.181\n",
      "    learn_time_ms: 220006.63\n",
      "    load_throughput: 7039489.783\n",
      "    load_time_ms: 0.568\n",
      "    sample_throughput: 17.272\n",
      "    sample_time_ms: 231589.83\n",
      "    update_time_ms: 21.486\n",
      "  timestamp: 1650299048\n",
      "  timesteps_since_restore: 1476000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1476000\n",
      "  training_iteration: 369\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-27-53\n",
      "  done: false\n",
      "  episode_len_mean: 1427.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 847\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 2.1448647234423104e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5362842993757668e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.005963442847132683\n",
      "          total_loss: -0.0059634423814713955\n",
      "          vf_explained_var: 3.991716766904574e-06\n",
      "          vf_loss: 2.8559923315185642e-09\n",
      "    num_agent_steps_sampled: 1480000\n",
      "    num_agent_steps_trained: 1480000\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.04455445544555\n",
      "    ram_util_percent: 95.75346534653464\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09789338295680912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9105349256263909\n",
      "    mean_inference_ms: 1.8798784630914402\n",
      "    mean_raw_obs_processing_ms: 0.12078626153415664\n",
      "  time_since_restore: 80909.59649825096\n",
      "  time_this_iter_s: 226.0600881576538\n",
      "  time_total_s: 80909.59649825096\n",
      "  timers:\n",
      "    learn_throughput: 18.239\n",
      "    learn_time_ms: 219307.695\n",
      "    load_throughput: 5623333.669\n",
      "    load_time_ms: 0.711\n",
      "    sample_throughput: 17.409\n",
      "    sample_time_ms: 229766.883\n",
      "    update_time_ms: 24.692\n",
      "  timestamp: 1650299273\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 370\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-27-54\n",
      "  done: false\n",
      "  episode_len_mean: 572.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.05\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3826\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.620982056418143e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.751171350479126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01798926666378975\n",
      "          model: {}\n",
      "          policy_loss: -0.045456063002347946\n",
      "          total_loss: 0.031297825276851654\n",
      "          vf_explained_var: 0.7581465244293213\n",
      "          vf_loss: 0.0767538920044899\n",
      "    num_agent_steps_sampled: 1480000\n",
      "    num_agent_steps_trained: 1480000\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.75544554455446\n",
      "    ram_util_percent: 95.75115511551155\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.096489318285422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9225070898218481\n",
      "    mean_inference_ms: 1.8615288918480406\n",
      "    mean_raw_obs_processing_ms: 0.17582206519762011\n",
      "  time_since_restore: 80914.91389751434\n",
      "  time_this_iter_s: 226.06531500816345\n",
      "  time_total_s: 80914.91389751434\n",
      "  timers:\n",
      "    learn_throughput: 18.251\n",
      "    learn_time_ms: 219164.463\n",
      "    load_throughput: 5422325.07\n",
      "    load_time_ms: 0.738\n",
      "    sample_throughput: 17.398\n",
      "    sample_time_ms: 229914.035\n",
      "    update_time_ms: 16.939\n",
      "  timestamp: 1650299274\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 370\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 17:29:20 (running for 22:30:40.16)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=8.05 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1484000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-31-36\n",
      "  done: false\n",
      "  episode_len_mean: 1522.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 851\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.9882422103584573e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.076840229632581e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.01642392762005329\n",
      "          total_loss: -0.016382599249482155\n",
      "          vf_explained_var: -0.11638934910297394\n",
      "          vf_loss: 4.132896356168203e-05\n",
      "    num_agent_steps_sampled: 1484000\n",
      "    num_agent_steps_trained: 1484000\n",
      "    num_steps_sampled: 1484000\n",
      "    num_steps_trained: 1484000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.355033557047\n",
      "    ram_util_percent: 95.58456375838925\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09782249644591903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9100856554981809\n",
      "    mean_inference_ms: 1.8777214470881396\n",
      "    mean_raw_obs_processing_ms: 0.12071658041257974\n",
      "  time_since_restore: 81132.72749042511\n",
      "  time_this_iter_s: 223.13099217414856\n",
      "  time_total_s: 81132.72749042511\n",
      "  timers:\n",
      "    learn_throughput: 18.301\n",
      "    learn_time_ms: 218568.854\n",
      "    load_throughput: 6497256.603\n",
      "    load_time_ms: 0.616\n",
      "    sample_throughput: 17.473\n",
      "    sample_time_ms: 228920.643\n",
      "    update_time_ms: 26.299\n",
      "  timestamp: 1650299496\n",
      "  timesteps_since_restore: 1484000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1484000\n",
      "  training_iteration: 371\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1484000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-31-37\n",
      "  done: false\n",
      "  episode_len_mean: 579.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.24\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 3831\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.620982056418143e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5650305151939392\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020389515906572342\n",
      "          model: {}\n",
      "          policy_loss: -0.03995471075177193\n",
      "          total_loss: 0.05103962868452072\n",
      "          vf_explained_var: 0.7053685784339905\n",
      "          vf_loss: 0.09099434316158295\n",
      "    num_agent_steps_sampled: 1484000\n",
      "    num_agent_steps_trained: 1484000\n",
      "    num_steps_sampled: 1484000\n",
      "    num_steps_trained: 1484000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.42188552188553\n",
      "    ram_util_percent: 95.56599326599326\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09645928980734349\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9222964211289978\n",
      "    mean_inference_ms: 1.8605697065206976\n",
      "    mean_raw_obs_processing_ms: 0.17574492811248668\n",
      "  time_since_restore: 81137.40820217133\n",
      "  time_this_iter_s: 222.49430465698242\n",
      "  time_total_s: 81137.40820217133\n",
      "  timers:\n",
      "    learn_throughput: 18.318\n",
      "    learn_time_ms: 218360.647\n",
      "    load_throughput: 5414974.664\n",
      "    load_time_ms: 0.739\n",
      "    sample_throughput: 17.464\n",
      "    sample_time_ms: 229045.495\n",
      "    update_time_ms: 14.113\n",
      "  timestamp: 1650299497\n",
      "  timesteps_since_restore: 1484000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1484000\n",
      "  training_iteration: 371\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-35-26\n",
      "  done: false\n",
      "  episode_len_mean: 1428.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 864\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.348483151640051e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.90282359987945e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0005777584738098085\n",
      "          total_loss: 0.0002997901465278119\n",
      "          vf_explained_var: 0.3060431480407715\n",
      "          vf_loss: 0.0008775561000220478\n",
      "    num_agent_steps_sampled: 1488000\n",
      "    num_agent_steps_trained: 1488000\n",
      "    num_steps_sampled: 1488000\n",
      "    num_steps_trained: 1488000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.7202614379085\n",
      "    ram_util_percent: 95.69281045751634\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09759698277269518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9086580010562092\n",
      "    mean_inference_ms: 1.870863113065449\n",
      "    mean_raw_obs_processing_ms: 0.1205144678243947\n",
      "  time_since_restore: 81362.04465436935\n",
      "  time_this_iter_s: 229.31716394424438\n",
      "  time_total_s: 81362.04465436935\n",
      "  timers:\n",
      "    learn_throughput: 18.34\n",
      "    learn_time_ms: 218098.725\n",
      "    load_throughput: 6522769.721\n",
      "    load_time_ms: 0.613\n",
      "    sample_throughput: 17.53\n",
      "    sample_time_ms: 228177.054\n",
      "    update_time_ms: 32.014\n",
      "  timestamp: 1650299726\n",
      "  timesteps_since_restore: 1488000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1488000\n",
      "  training_iteration: 372\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-35-26\n",
      "  done: false\n",
      "  episode_len_mean: 590.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.39\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3838\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6809224486351013\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015173684805631638\n",
      "          model: {}\n",
      "          policy_loss: -0.043071404099464417\n",
      "          total_loss: 0.03804178163409233\n",
      "          vf_explained_var: 0.7302097678184509\n",
      "          vf_loss: 0.08111318200826645\n",
      "    num_agent_steps_sampled: 1488000\n",
      "    num_agent_steps_trained: 1488000\n",
      "    num_steps_sampled: 1488000\n",
      "    num_steps_trained: 1488000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.94397394136809\n",
      "    ram_util_percent: 95.71205211726384\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09641560218045718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9219903620060765\n",
      "    mean_inference_ms: 1.85916764843254\n",
      "    mean_raw_obs_processing_ms: 0.17563143029055375\n",
      "  time_since_restore: 81366.65496993065\n",
      "  time_this_iter_s: 229.24676775932312\n",
      "  time_total_s: 81366.65496993065\n",
      "  timers:\n",
      "    learn_throughput: 18.352\n",
      "    learn_time_ms: 217956.369\n",
      "    load_throughput: 5442906.826\n",
      "    load_time_ms: 0.735\n",
      "    sample_throughput: 17.532\n",
      "    sample_time_ms: 228156.78\n",
      "    update_time_ms: 12.497\n",
      "  timestamp: 1650299726\n",
      "  timesteps_since_restore: 1488000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1488000\n",
      "  training_iteration: 372\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1492000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-39-16\n",
      "  done: false\n",
      "  episode_len_mean: 1428.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 864\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.686394281889638e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.041380143722529e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128079637885094\n",
      "          total_loss: -0.014127745293080807\n",
      "          vf_explained_var: 7.120512179881189e-08\n",
      "          vf_loss: 3.260709604546719e-07\n",
      "    num_agent_steps_sampled: 1492000\n",
      "    num_agent_steps_trained: 1492000\n",
      "    num_steps_sampled: 1492000\n",
      "    num_steps_trained: 1492000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.56026058631922\n",
      "    ram_util_percent: 95.8201954397394\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09759698277269518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9086580010562092\n",
      "    mean_inference_ms: 1.870863113065449\n",
      "    mean_raw_obs_processing_ms: 0.1205144678243947\n",
      "  time_since_restore: 81591.57941150665\n",
      "  time_this_iter_s: 229.53475713729858\n",
      "  time_total_s: 81591.57941150665\n",
      "  timers:\n",
      "    learn_throughput: 18.409\n",
      "    learn_time_ms: 217282.445\n",
      "    load_throughput: 6352840.32\n",
      "    load_time_ms: 0.63\n",
      "    sample_throughput: 17.568\n",
      "    sample_time_ms: 227692.676\n",
      "    update_time_ms: 33.77\n",
      "  timestamp: 1650299956\n",
      "  timesteps_since_restore: 1492000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1492000\n",
      "  training_iteration: 373\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1492000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-39-16\n",
      "  done: false\n",
      "  episode_len_mean: 587.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.37\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3846\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6938529014587402\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01633037067949772\n",
      "          model: {}\n",
      "          policy_loss: -0.04052555561065674\n",
      "          total_loss: 0.05192724987864494\n",
      "          vf_explained_var: 0.6901637315750122\n",
      "          vf_loss: 0.09245280921459198\n",
      "    num_agent_steps_sampled: 1492000\n",
      "    num_agent_steps_trained: 1492000\n",
      "    num_steps_sampled: 1492000\n",
      "    num_steps_trained: 1492000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.51628664495114\n",
      "    ram_util_percent: 95.83029315960911\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09636484702088172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9216440387198348\n",
      "    mean_inference_ms: 1.8575619829035241\n",
      "    mean_raw_obs_processing_ms: 0.17550494557561394\n",
      "  time_since_restore: 81596.62390303612\n",
      "  time_this_iter_s: 229.96893310546875\n",
      "  time_total_s: 81596.62390303612\n",
      "  timers:\n",
      "    learn_throughput: 18.413\n",
      "    learn_time_ms: 217238.71\n",
      "    load_throughput: 5374901.006\n",
      "    load_time_ms: 0.744\n",
      "    sample_throughput: 17.564\n",
      "    sample_time_ms: 227740.98\n",
      "    update_time_ms: 9.576\n",
      "  timestamp: 1650299956\n",
      "  timesteps_since_restore: 1492000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1492000\n",
      "  training_iteration: 373\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-43-05\n",
      "  done: false\n",
      "  episode_len_mean: 1428.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 864\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.686394281889638e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.041380143722529e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014128051698207855\n",
      "          total_loss: -0.014127977192401886\n",
      "          vf_explained_var: -1.5202388681245793e-07\n",
      "          vf_loss: 7.59151248530543e-08\n",
      "    num_agent_steps_sampled: 1496000\n",
      "    num_agent_steps_trained: 1496000\n",
      "    num_steps_sampled: 1496000\n",
      "    num_steps_trained: 1496000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.64901960784313\n",
      "    ram_util_percent: 95.69150326797386\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09759698277269518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9086580010562092\n",
      "    mean_inference_ms: 1.870863113065449\n",
      "    mean_raw_obs_processing_ms: 0.1205144678243947\n",
      "  time_since_restore: 81820.69096565247\n",
      "  time_this_iter_s: 229.111554145813\n",
      "  time_total_s: 81820.69096565247\n",
      "  timers:\n",
      "    learn_throughput: 18.497\n",
      "    learn_time_ms: 216247.565\n",
      "    load_throughput: 7002176.962\n",
      "    load_time_ms: 0.571\n",
      "    sample_throughput: 17.633\n",
      "    sample_time_ms: 226843.017\n",
      "    update_time_ms: 33.026\n",
      "  timestamp: 1650300185\n",
      "  timesteps_since_restore: 1496000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1496000\n",
      "  training_iteration: 374\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-43-06\n",
      "  done: false\n",
      "  episode_len_mean: 588.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.41\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3853\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6534700989723206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01805872842669487\n",
      "          model: {}\n",
      "          policy_loss: -0.05107126384973526\n",
      "          total_loss: 0.012781425379216671\n",
      "          vf_explained_var: 0.7626084089279175\n",
      "          vf_loss: 0.06385268270969391\n",
      "    num_agent_steps_sampled: 1496000\n",
      "    num_agent_steps_trained: 1496000\n",
      "    num_steps_sampled: 1496000\n",
      "    num_steps_trained: 1496000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.68344155844156\n",
      "    ram_util_percent: 95.69545454545454\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09631985792336424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9213418182349915\n",
      "    mean_inference_ms: 1.8561814537316\n",
      "    mean_raw_obs_processing_ms: 0.1753948322594071\n",
      "  time_since_restore: 81826.59703111649\n",
      "  time_this_iter_s: 229.97312808036804\n",
      "  time_total_s: 81826.59703111649\n",
      "  timers:\n",
      "    learn_throughput: 18.505\n",
      "    learn_time_ms: 216157.686\n",
      "    load_throughput: 5388885.106\n",
      "    load_time_ms: 0.742\n",
      "    sample_throughput: 17.621\n",
      "    sample_time_ms: 227007.538\n",
      "    update_time_ms: 8.717\n",
      "  timestamp: 1650300186\n",
      "  timesteps_since_restore: 1496000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1496000\n",
      "  training_iteration: 374\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 17:46:01 (running for 22:47:20.98)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=8.41 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 1524.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 877\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.406901489931901e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.0224959064510186e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0006703361286781728\n",
      "          total_loss: 0.00099720130674541\n",
      "          vf_explained_var: 0.2874915301799774\n",
      "          vf_loss: 0.00032686020131222904\n",
      "    num_agent_steps_sampled: 1500000\n",
      "    num_agent_steps_trained: 1500000\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.42032258064515\n",
      "    ram_util_percent: 95.55935483870968\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09736741719755482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9072103795173136\n",
      "    mean_inference_ms: 1.8638810151045881\n",
      "    mean_raw_obs_processing_ms: 0.12029619050839649\n",
      "  time_since_restore: 82051.96663570404\n",
      "  time_this_iter_s: 231.2756700515747\n",
      "  time_total_s: 82051.96663570404\n",
      "  timers:\n",
      "    learn_throughput: 18.454\n",
      "    learn_time_ms: 216755.267\n",
      "    load_throughput: 6911314.521\n",
      "    load_time_ms: 0.579\n",
      "    sample_throughput: 17.705\n",
      "    sample_time_ms: 225921.666\n",
      "    update_time_ms: 36.221\n",
      "  timestamp: 1650300416\n",
      "  timesteps_since_restore: 1500000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 375\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-46-57\n",
      "  done: false\n",
      "  episode_len_mean: 595.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.6\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3861\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.744129478931427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014748985879123211\n",
      "          model: {}\n",
      "          policy_loss: -0.056820500642061234\n",
      "          total_loss: 0.021507853642106056\n",
      "          vf_explained_var: 0.6401576995849609\n",
      "          vf_loss: 0.07832834869623184\n",
      "    num_agent_steps_sampled: 1500000\n",
      "    num_agent_steps_trained: 1500000\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.17508090614886\n",
      "    ram_util_percent: 95.55242718446601\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09626926868551894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9210028053014025\n",
      "    mean_inference_ms: 1.8546092717076066\n",
      "    mean_raw_obs_processing_ms: 0.17527054592992827\n",
      "  time_since_restore: 82057.36703515053\n",
      "  time_this_iter_s: 230.77000403404236\n",
      "  time_total_s: 82057.36703515053\n",
      "  timers:\n",
      "    learn_throughput: 18.459\n",
      "    learn_time_ms: 216698.501\n",
      "    load_throughput: 5254373.943\n",
      "    load_time_ms: 0.761\n",
      "    sample_throughput: 17.706\n",
      "    sample_time_ms: 225916.504\n",
      "    update_time_ms: 9.788\n",
      "  timestamp: 1650300417\n",
      "  timesteps_since_restore: 1500000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 375\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-50-45\n",
      "  done: false\n",
      "  episode_len_mean: 1426.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 893\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0930723920854285e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -5.634014925767323e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0005845149862580001\n",
      "          total_loss: 0.0008064749999903142\n",
      "          vf_explained_var: 0.14463898539543152\n",
      "          vf_loss: 0.00022195647761691362\n",
      "    num_agent_steps_sampled: 1504000\n",
      "    num_agent_steps_trained: 1504000\n",
      "    num_steps_sampled: 1504000\n",
      "    num_steps_trained: 1504000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.75849673202615\n",
      "    ram_util_percent: 95.45261437908496\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09710051384228802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9055296719807561\n",
      "    mean_inference_ms: 1.8557026861531027\n",
      "    mean_raw_obs_processing_ms: 0.12005919253943526\n",
      "  time_since_restore: 82280.70506858826\n",
      "  time_this_iter_s: 228.7384328842163\n",
      "  time_total_s: 82280.70506858826\n",
      "  timers:\n",
      "    learn_throughput: 18.494\n",
      "    learn_time_ms: 216291.983\n",
      "    load_throughput: 6751123.094\n",
      "    load_time_ms: 0.592\n",
      "    sample_throughput: 17.663\n",
      "    sample_time_ms: 226466.237\n",
      "    update_time_ms: 30.882\n",
      "  timestamp: 1650300645\n",
      "  timesteps_since_restore: 1504000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1504000\n",
      "  training_iteration: 376\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-50-45\n",
      "  done: false\n",
      "  episode_len_mean: 589.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.45\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3869\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7802693843841553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012876066379249096\n",
      "          model: {}\n",
      "          policy_loss: -0.05601555109024048\n",
      "          total_loss: 0.037033047527074814\n",
      "          vf_explained_var: 0.7041769027709961\n",
      "          vf_loss: 0.09304860234260559\n",
      "    num_agent_steps_sampled: 1504000\n",
      "    num_agent_steps_trained: 1504000\n",
      "    num_steps_sampled: 1504000\n",
      "    num_steps_trained: 1504000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.45849673202615\n",
      "    ram_util_percent: 95.44771241830065\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09621800879781273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9206644025644934\n",
      "    mean_inference_ms: 1.853040286523097\n",
      "    mean_raw_obs_processing_ms: 0.17515043208150274\n",
      "  time_since_restore: 82285.52980422974\n",
      "  time_this_iter_s: 228.16276907920837\n",
      "  time_total_s: 82285.52980422974\n",
      "  timers:\n",
      "    learn_throughput: 18.499\n",
      "    learn_time_ms: 216225.416\n",
      "    load_throughput: 4642669.84\n",
      "    load_time_ms: 0.862\n",
      "    sample_throughput: 17.665\n",
      "    sample_time_ms: 226430.445\n",
      "    update_time_ms: 10.015\n",
      "  timestamp: 1650300645\n",
      "  timesteps_since_restore: 1504000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1504000\n",
      "  training_iteration: 376\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1508000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-54-26\n",
      "  done: false\n",
      "  episode_len_mean: 1426.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 893\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.014128029346466064\n",
      "          total_loss: 0.014128145761787891\n",
      "          vf_explained_var: -2.8860185352641565e-07\n",
      "          vf_loss: 1.1885779116482809e-07\n",
      "    num_agent_steps_sampled: 1508000\n",
      "    num_agent_steps_trained: 1508000\n",
      "    num_steps_sampled: 1508000\n",
      "    num_steps_trained: 1508000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.38383838383838\n",
      "    ram_util_percent: 95.35791245791246\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09710051384228802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9055296719807561\n",
      "    mean_inference_ms: 1.8557026861531027\n",
      "    mean_raw_obs_processing_ms: 0.12005919253943526\n",
      "  time_since_restore: 82501.44858384132\n",
      "  time_this_iter_s: 220.74351525306702\n",
      "  time_total_s: 82501.44858384132\n",
      "  timers:\n",
      "    learn_throughput: 18.508\n",
      "    learn_time_ms: 216127.835\n",
      "    load_throughput: 6936460.082\n",
      "    load_time_ms: 0.577\n",
      "    sample_throughput: 17.705\n",
      "    sample_time_ms: 225929.862\n",
      "    update_time_ms: 30.413\n",
      "  timestamp: 1650300866\n",
      "  timesteps_since_restore: 1508000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1508000\n",
      "  training_iteration: 377\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1508000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-54-26\n",
      "  done: false\n",
      "  episode_len_mean: 580.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.29\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3877\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.726914644241333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01755763217806816\n",
      "          model: {}\n",
      "          policy_loss: -0.043566275388002396\n",
      "          total_loss: 0.026026813313364983\n",
      "          vf_explained_var: 0.753765344619751\n",
      "          vf_loss: 0.06959308683872223\n",
      "    num_agent_steps_sampled: 1508000\n",
      "    num_agent_steps_trained: 1508000\n",
      "    num_steps_sampled: 1508000\n",
      "    num_steps_trained: 1508000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.93905723905725\n",
      "    ram_util_percent: 95.33872053872054\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09616615876583465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.920332236467722\n",
      "    mean_inference_ms: 1.8515014590938992\n",
      "    mean_raw_obs_processing_ms: 0.17503210566094743\n",
      "  time_since_restore: 82506.8521592617\n",
      "  time_this_iter_s: 221.32235503196716\n",
      "  time_total_s: 82506.8521592617\n",
      "  timers:\n",
      "    learn_throughput: 18.508\n",
      "    learn_time_ms: 216120.74\n",
      "    load_throughput: 4672928.725\n",
      "    load_time_ms: 0.856\n",
      "    sample_throughput: 17.708\n",
      "    sample_time_ms: 225891.166\n",
      "    update_time_ms: 9.973\n",
      "  timestamp: 1650300866\n",
      "  timesteps_since_restore: 1508000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1508000\n",
      "  training_iteration: 377\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-58-14\n",
      "  done: false\n",
      "  episode_len_mean: 1426.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 893\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.6230661390998187e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -4.242232021806165e-26\n",
      "          model: {}\n",
      "          policy_loss: 0.011364974081516266\n",
      "          total_loss: 0.011364975944161415\n",
      "          vf_explained_var: 1.2946385297141205e-08\n",
      "          vf_loss: 9.030175718294231e-09\n",
      "    num_agent_steps_sampled: 1512000\n",
      "    num_agent_steps_trained: 1512000\n",
      "    num_steps_sampled: 1512000\n",
      "    num_steps_trained: 1512000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.03223684210526\n",
      "    ram_util_percent: 95.67697368421052\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09710051384228802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9055296719807561\n",
      "    mean_inference_ms: 1.8557026861531027\n",
      "    mean_raw_obs_processing_ms: 0.12005919253943526\n",
      "  time_since_restore: 82729.39614272118\n",
      "  time_this_iter_s: 227.9475588798523\n",
      "  time_total_s: 82729.39614272118\n",
      "  timers:\n",
      "    learn_throughput: 18.421\n",
      "    learn_time_ms: 217148.079\n",
      "    load_throughput: 7064090.947\n",
      "    load_time_ms: 0.566\n",
      "    sample_throughput: 17.716\n",
      "    sample_time_ms: 225784.541\n",
      "    update_time_ms: 30.1\n",
      "  timestamp: 1650301094\n",
      "  timesteps_since_restore: 1512000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1512000\n",
      "  training_iteration: 378\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_17-58-14\n",
      "  done: false\n",
      "  episode_len_mean: 579.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.35\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3884\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6940258145332336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016188209876418114\n",
      "          model: {}\n",
      "          policy_loss: -0.04263075813651085\n",
      "          total_loss: 0.03553905338048935\n",
      "          vf_explained_var: 0.767685055732727\n",
      "          vf_loss: 0.0781698077917099\n",
      "    num_agent_steps_sampled: 1512000\n",
      "    num_agent_steps_trained: 1512000\n",
      "    num_steps_sampled: 1512000\n",
      "    num_steps_trained: 1512000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.15934426229508\n",
      "    ram_util_percent: 95.64852459016393\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09612074840282442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.92004373464678\n",
      "    mean_inference_ms: 1.8501467954870585\n",
      "    mean_raw_obs_processing_ms: 0.17492808100971466\n",
      "  time_since_restore: 82734.43529629707\n",
      "  time_this_iter_s: 227.58313703536987\n",
      "  time_total_s: 82734.43529629707\n",
      "  timers:\n",
      "    learn_throughput: 18.427\n",
      "    learn_time_ms: 217067.171\n",
      "    load_throughput: 4698052.701\n",
      "    load_time_ms: 0.851\n",
      "    sample_throughput: 17.717\n",
      "    sample_time_ms: 225777.692\n",
      "    update_time_ms: 10.88\n",
      "  timestamp: 1650301094\n",
      "  timesteps_since_restore: 1512000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1512000\n",
      "  training_iteration: 378\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1516000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-01-57\n",
      "  done: false\n",
      "  episode_len_mean: 585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.52\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3891\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6031438112258911\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01661057583987713\n",
      "          model: {}\n",
      "          policy_loss: -0.03138624504208565\n",
      "          total_loss: 0.024558529257774353\n",
      "          vf_explained_var: 0.7752217054367065\n",
      "          vf_loss: 0.05594477429986\n",
      "    num_agent_steps_sampled: 1516000\n",
      "    num_agent_steps_trained: 1516000\n",
      "    num_steps_sampled: 1516000\n",
      "    num_steps_trained: 1516000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.73478260869565\n",
      "    ram_util_percent: 95.77926421404682\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09607425995898536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9197491181203097\n",
      "    mean_inference_ms: 1.8487612800490865\n",
      "    mean_raw_obs_processing_ms: 0.17482131436460618\n",
      "  time_since_restore: 82957.69870328903\n",
      "  time_this_iter_s: 223.26340699195862\n",
      "  time_total_s: 82957.69870328903\n",
      "  timers:\n",
      "    learn_throughput: 18.409\n",
      "    learn_time_ms: 217290.48\n",
      "    load_throughput: 4708600.937\n",
      "    load_time_ms: 0.85\n",
      "    sample_throughput: 17.645\n",
      "    sample_time_ms: 226695.471\n",
      "    update_time_ms: 12.229\n",
      "  timestamp: 1650301317\n",
      "  timesteps_since_restore: 1516000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1516000\n",
      "  training_iteration: 379\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1516000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-01-58\n",
      "  done: false\n",
      "  episode_len_mean: 1429.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 904\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.255747023819814e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7120333914244695e-26\n",
      "          model: {}\n",
      "          policy_loss: -0.0012203429359942675\n",
      "          total_loss: -0.0007787240319885314\n",
      "          vf_explained_var: 0.28877702355384827\n",
      "          vf_loss: 0.00044161584810353816\n",
      "    num_agent_steps_sampled: 1516000\n",
      "    num_agent_steps_trained: 1516000\n",
      "    num_steps_sampled: 1516000\n",
      "    num_steps_trained: 1516000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.48733333333332\n",
      "    ram_util_percent: 95.76599999999999\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09690987790735794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9043393933090307\n",
      "    mean_inference_ms: 1.8499135987764765\n",
      "    mean_raw_obs_processing_ms: 0.11988388314064644\n",
      "  time_since_restore: 82953.10629272461\n",
      "  time_this_iter_s: 223.71015000343323\n",
      "  time_total_s: 82953.10629272461\n",
      "  timers:\n",
      "    learn_throughput: 18.397\n",
      "    learn_time_ms: 217423.715\n",
      "    load_throughput: 7059038.162\n",
      "    load_time_ms: 0.567\n",
      "    sample_throughput: 17.638\n",
      "    sample_time_ms: 226783.749\n",
      "    update_time_ms: 26.334\n",
      "  timestamp: 1650301318\n",
      "  timesteps_since_restore: 1516000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1516000\n",
      "  training_iteration: 379\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 18:02:41 (running for 23:04:01.28)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=8.52 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 1236.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 913\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 9.116562387395496e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -3.277578394237308e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0010705534368753433\n",
      "          total_loss: -0.0007647863822057843\n",
      "          vf_explained_var: 0.4504950940608978\n",
      "          vf_loss: 0.0003057663270737976\n",
      "    num_agent_steps_sampled: 1520000\n",
      "    num_agent_steps_trained: 1520000\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.69767441860465\n",
      "    ram_util_percent: 95.68106312292359\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09676570699939861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9034563493294333\n",
      "    mean_inference_ms: 1.8456311981233613\n",
      "    mean_raw_obs_processing_ms: 0.11976249600042894\n",
      "  time_since_restore: 83177.84001779556\n",
      "  time_this_iter_s: 224.73372507095337\n",
      "  time_total_s: 83177.84001779556\n",
      "  timers:\n",
      "    learn_throughput: 18.406\n",
      "    learn_time_ms: 217319.34\n",
      "    load_throughput: 6968150.517\n",
      "    load_time_ms: 0.574\n",
      "    sample_throughput: 17.62\n",
      "    sample_time_ms: 227017.129\n",
      "    update_time_ms: 27.243\n",
      "  timestamp: 1650301542\n",
      "  timesteps_since_restore: 1520000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 380\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-05-43\n",
      "  done: false\n",
      "  episode_len_mean: 569.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.2\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3900\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7652130722999573\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016447734087705612\n",
      "          model: {}\n",
      "          policy_loss: -0.05017225071787834\n",
      "          total_loss: 0.011588821187615395\n",
      "          vf_explained_var: 0.7570359706878662\n",
      "          vf_loss: 0.061761073768138885\n",
      "    num_agent_steps_sampled: 1520000\n",
      "    num_agent_steps_trained: 1520000\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.37880794701987\n",
      "    ram_util_percent: 95.7046357615894\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09601433196369304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9193733628257001\n",
      "    mean_inference_ms: 1.8469899066636386\n",
      "    mean_raw_obs_processing_ms: 0.1746878674581146\n",
      "  time_since_restore: 83183.20951604843\n",
      "  time_this_iter_s: 225.5108127593994\n",
      "  time_total_s: 83183.20951604843\n",
      "  timers:\n",
      "    learn_throughput: 18.407\n",
      "    learn_time_ms: 217314.427\n",
      "    load_throughput: 5822995.974\n",
      "    load_time_ms: 0.687\n",
      "    sample_throughput: 17.633\n",
      "    sample_time_ms: 226842.092\n",
      "    update_time_ms: 13.176\n",
      "  timestamp: 1650301543\n",
      "  timesteps_since_restore: 1520000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 380\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1524000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-09-32\n",
      "  done: false\n",
      "  episode_len_mean: 1236.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 913\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.686394281889638e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.041380143722529e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.008324949070811272\n",
      "          total_loss: 0.008324950933456421\n",
      "          vf_explained_var: 1.4770544112252537e-05\n",
      "          vf_loss: 5.6703055584250706e-09\n",
      "    num_agent_steps_sampled: 1524000\n",
      "    num_agent_steps_trained: 1524000\n",
      "    num_steps_sampled: 1524000\n",
      "    num_steps_trained: 1524000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.9071661237785\n",
      "    ram_util_percent: 95.71661237785017\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09676570699939861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9034563493294333\n",
      "    mean_inference_ms: 1.8456311981233613\n",
      "    mean_raw_obs_processing_ms: 0.11976249600042894\n",
      "  time_since_restore: 83407.51418685913\n",
      "  time_this_iter_s: 229.67416906356812\n",
      "  time_total_s: 83407.51418685913\n",
      "  timers:\n",
      "    learn_throughput: 18.349\n",
      "    learn_time_ms: 217990.598\n",
      "    load_throughput: 6872246.754\n",
      "    load_time_ms: 0.582\n",
      "    sample_throughput: 17.629\n",
      "    sample_time_ms: 226900.375\n",
      "    update_time_ms: 25.829\n",
      "  timestamp: 1650301772\n",
      "  timesteps_since_restore: 1524000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1524000\n",
      "  training_iteration: 381\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1524000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-09-32\n",
      "  done: false\n",
      "  episode_len_mean: 568.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.21\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3908\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7785383462905884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01671138033270836\n",
      "          model: {}\n",
      "          policy_loss: -0.04860961437225342\n",
      "          total_loss: 0.03357560187578201\n",
      "          vf_explained_var: 0.7320834398269653\n",
      "          vf_loss: 0.08218521624803543\n",
      "    num_agent_steps_sampled: 1524000\n",
      "    num_agent_steps_trained: 1524000\n",
      "    num_steps_sampled: 1524000\n",
      "    num_steps_trained: 1524000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.09575163398692\n",
      "    ram_util_percent: 95.73169934640524\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0959623775186773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.919047303033653\n",
      "    mean_inference_ms: 1.845447952115141\n",
      "    mean_raw_obs_processing_ms: 0.17457154376618247\n",
      "  time_since_restore: 83412.48959994316\n",
      "  time_this_iter_s: 229.28008389472961\n",
      "  time_total_s: 83412.48959994316\n",
      "  timers:\n",
      "    learn_throughput: 18.353\n",
      "    learn_time_ms: 217947.246\n",
      "    load_throughput: 5810090.04\n",
      "    load_time_ms: 0.688\n",
      "    sample_throughput: 17.628\n",
      "    sample_time_ms: 226912.849\n",
      "    update_time_ms: 12.608\n",
      "  timestamp: 1650301772\n",
      "  timesteps_since_restore: 1524000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1524000\n",
      "  training_iteration: 381\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 18:13:22,985\tWARNING ray_trial_executor.py:655 -- Over the last 60 seconds, the Tune event loop has been backlogged processing new results. Consider increasing your period of result reporting to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 1236.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 913\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.686394281889638e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.041380143722529e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.00414908304810524\n",
      "          total_loss: 0.00414908304810524\n",
      "          vf_explained_var: 4.26134756708052e-05\n",
      "          vf_loss: 1.1843762637298028e-09\n",
      "    num_agent_steps_sampled: 1528000\n",
      "    num_agent_steps_trained: 1528000\n",
      "    num_steps_sampled: 1528000\n",
      "    num_steps_trained: 1528000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.34772727272727\n",
      "    ram_util_percent: 95.75551948051948\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09676570699939861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9034563493294333\n",
      "    mean_inference_ms: 1.8456311981233613\n",
      "    mean_raw_obs_processing_ms: 0.11976249600042894\n",
      "  time_since_restore: 83637.67257285118\n",
      "  time_this_iter_s: 230.15838599205017\n",
      "  time_total_s: 83637.67257285118\n",
      "  timers:\n",
      "    learn_throughput: 18.34\n",
      "    learn_time_ms: 218102.404\n",
      "    load_throughput: 6860444.081\n",
      "    load_time_ms: 0.583\n",
      "    sample_throughput: 17.579\n",
      "    sample_time_ms: 227540.119\n",
      "    update_time_ms: 22.769\n",
      "  timestamp: 1650302002\n",
      "  timesteps_since_restore: 1528000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1528000\n",
      "  training_iteration: 382\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-13-23\n",
      "  done: false\n",
      "  episode_len_mean: 564.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 7.98\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3915\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.703846275806427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01747364178299904\n",
      "          model: {}\n",
      "          policy_loss: -0.05483056232333183\n",
      "          total_loss: 0.014747348614037037\n",
      "          vf_explained_var: 0.7428541779518127\n",
      "          vf_loss: 0.0695778951048851\n",
      "    num_agent_steps_sampled: 1528000\n",
      "    num_agent_steps_trained: 1528000\n",
      "    num_steps_sampled: 1528000\n",
      "    num_steps_trained: 1528000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.78964401294499\n",
      "    ram_util_percent: 95.75566343042071\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09591682784755694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9187615208623021\n",
      "    mean_inference_ms: 1.8441007283187698\n",
      "    mean_raw_obs_processing_ms: 0.17446733598844757\n",
      "  time_since_restore: 83643.50363111496\n",
      "  time_this_iter_s: 231.0140311717987\n",
      "  time_total_s: 83643.50363111496\n",
      "  timers:\n",
      "    learn_throughput: 18.342\n",
      "    learn_time_ms: 218075.495\n",
      "    load_throughput: 5864519.016\n",
      "    load_time_ms: 0.682\n",
      "    sample_throughput: 17.575\n",
      "    sample_time_ms: 227599.683\n",
      "    update_time_ms: 12.568\n",
      "  timestamp: 1650302003\n",
      "  timesteps_since_restore: 1528000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1528000\n",
      "  training_iteration: 382\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1532000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-17-06\n",
      "  done: false\n",
      "  episode_len_mean: 1235.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 916\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.184799035286393e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.0143280375568723e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0008210867526941001\n",
      "          total_loss: 0.0008937856764532626\n",
      "          vf_explained_var: -0.031109262257814407\n",
      "          vf_loss: 7.269666093634441e-05\n",
      "    num_agent_steps_sampled: 1532000\n",
      "    num_agent_steps_trained: 1532000\n",
      "    num_steps_sampled: 1532000\n",
      "    num_steps_trained: 1532000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.53\n",
      "    ram_util_percent: 95.69366666666666\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09671926749197583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9031684991758355\n",
      "    mean_inference_ms: 1.8442351355443751\n",
      "    mean_raw_obs_processing_ms: 0.11972295504857602\n",
      "  time_since_restore: 83861.3354048729\n",
      "  time_this_iter_s: 223.66283202171326\n",
      "  time_total_s: 83861.3354048729\n",
      "  timers:\n",
      "    learn_throughput: 18.394\n",
      "    learn_time_ms: 217465.888\n",
      "    load_throughput: 6854557.934\n",
      "    load_time_ms: 0.584\n",
      "    sample_throughput: 17.567\n",
      "    sample_time_ms: 227697.77\n",
      "    update_time_ms: 23.607\n",
      "  timestamp: 1650302226\n",
      "  timesteps_since_restore: 1532000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1532000\n",
      "  training_iteration: 383\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1532000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 568.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 21.0\n",
      "  episode_reward_mean: 8.05\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3922\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7156354188919067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016156813129782677\n",
      "          model: {}\n",
      "          policy_loss: -0.03780383989214897\n",
      "          total_loss: 0.026599962264299393\n",
      "          vf_explained_var: 0.7672892808914185\n",
      "          vf_loss: 0.06440379470586777\n",
      "    num_agent_steps_sampled: 1532000\n",
      "    num_agent_steps_trained: 1532000\n",
      "    num_steps_sampled: 1532000\n",
      "    num_steps_trained: 1532000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.33333333333333\n",
      "    ram_util_percent: 95.68233333333333\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0958721959772146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9184803070124168\n",
      "    mean_inference_ms: 1.8427669362158796\n",
      "    mean_raw_obs_processing_ms: 0.17436510591044985\n",
      "  time_since_restore: 83867.6340470314\n",
      "  time_this_iter_s: 224.13041591644287\n",
      "  time_total_s: 83867.6340470314\n",
      "  timers:\n",
      "    learn_throughput: 18.394\n",
      "    learn_time_ms: 217457.844\n",
      "    load_throughput: 3628761.517\n",
      "    load_time_ms: 1.102\n",
      "    sample_throughput: 17.563\n",
      "    sample_time_ms: 227756.817\n",
      "    update_time_ms: 12.847\n",
      "  timestamp: 1650302228\n",
      "  timesteps_since_restore: 1532000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1532000\n",
      "  training_iteration: 383\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 18:19:22 (running for 23:20:42.17)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=8.05 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-20-54\n",
      "  done: false\n",
      "  episode_len_mean: 1235.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 916\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.013434133492410183\n",
      "          total_loss: 0.013434136286377907\n",
      "          vf_explained_var: 2.9930504297226435e-07\n",
      "          vf_loss: 1.1794935694808828e-08\n",
      "    num_agent_steps_sampled: 1536000\n",
      "    num_agent_steps_trained: 1536000\n",
      "    num_steps_sampled: 1536000\n",
      "    num_steps_trained: 1536000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.8672131147541\n",
      "    ram_util_percent: 95.83245901639343\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09671926749197583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9031684991758355\n",
      "    mean_inference_ms: 1.8442351355443751\n",
      "    mean_raw_obs_processing_ms: 0.11972295504857602\n",
      "  time_since_restore: 84089.36010980606\n",
      "  time_this_iter_s: 228.0247049331665\n",
      "  time_total_s: 84089.36010980606\n",
      "  timers:\n",
      "    learn_throughput: 18.406\n",
      "    learn_time_ms: 217317.895\n",
      "    load_throughput: 6721102.476\n",
      "    load_time_ms: 0.595\n",
      "    sample_throughput: 17.613\n",
      "    sample_time_ms: 227103.053\n",
      "    update_time_ms: 23.631\n",
      "  timestamp: 1650302454\n",
      "  timesteps_since_restore: 1536000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1536000\n",
      "  training_iteration: 384\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-20-55\n",
      "  done: false\n",
      "  episode_len_mean: 558.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.81\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3930\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6751568913459778\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01807267777621746\n",
      "          model: {}\n",
      "          policy_loss: -0.04672294110059738\n",
      "          total_loss: 0.016790984198451042\n",
      "          vf_explained_var: 0.7620713710784912\n",
      "          vf_loss: 0.06351392716169357\n",
      "    num_agent_steps_sampled: 1536000\n",
      "    num_agent_steps_trained: 1536000\n",
      "    num_steps_sampled: 1536000\n",
      "    num_steps_trained: 1536000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.22098360655737\n",
      "    ram_util_percent: 95.81737704918034\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0958223371691007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9181614845876811\n",
      "    mean_inference_ms: 1.8412502309491725\n",
      "    mean_raw_obs_processing_ms: 0.17425059076953645\n",
      "  time_since_restore: 84095.52966809273\n",
      "  time_this_iter_s: 227.89562106132507\n",
      "  time_total_s: 84095.52966809273\n",
      "  timers:\n",
      "    learn_throughput: 18.414\n",
      "    learn_time_ms: 217226.46\n",
      "    load_throughput: 3583803.136\n",
      "    load_time_ms: 1.116\n",
      "    sample_throughput: 17.609\n",
      "    sample_time_ms: 227161.997\n",
      "    update_time_ms: 13.328\n",
      "  timestamp: 1650302455\n",
      "  timesteps_since_restore: 1536000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1536000\n",
      "  training_iteration: 384\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-24-45\n",
      "  done: false\n",
      "  episode_len_mean: 1234.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 921\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.363894623896634e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.8872750169046297e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0004801579052582383\n",
      "          total_loss: 0.0006096136057749391\n",
      "          vf_explained_var: 0.06420809775590897\n",
      "          vf_loss: 0.00012945529306307435\n",
      "    num_agent_steps_sampled: 1540000\n",
      "    num_agent_steps_trained: 1540000\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.79674267100977\n",
      "    ram_util_percent: 95.83159609120523\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09664941578979828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9027259314277651\n",
      "    mean_inference_ms: 1.8420914619980084\n",
      "    mean_raw_obs_processing_ms: 0.11966448223323031\n",
      "  time_since_restore: 84319.43882083893\n",
      "  time_this_iter_s: 230.07871103286743\n",
      "  time_total_s: 84319.43882083893\n",
      "  timers:\n",
      "    learn_throughput: 18.409\n",
      "    learn_time_ms: 217284.719\n",
      "    load_throughput: 6782783.909\n",
      "    load_time_ms: 0.59\n",
      "    sample_throughput: 17.631\n",
      "    sample_time_ms: 226871.447\n",
      "    update_time_ms: 22.896\n",
      "  timestamp: 1650302685\n",
      "  timesteps_since_restore: 1540000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 385\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-24-46\n",
      "  done: false\n",
      "  episode_len_mean: 550.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.67\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3938\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7387935519218445\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014681565575301647\n",
      "          model: {}\n",
      "          policy_loss: -0.049282632768154144\n",
      "          total_loss: 0.01822245679795742\n",
      "          vf_explained_var: 0.7843276262283325\n",
      "          vf_loss: 0.06750509142875671\n",
      "    num_agent_steps_sampled: 1540000\n",
      "    num_agent_steps_trained: 1540000\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.4853896103896\n",
      "    ram_util_percent: 95.83896103896105\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09577551297989509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9178598074113126\n",
      "    mean_inference_ms: 1.8398124576395287\n",
      "    mean_raw_obs_processing_ms: 0.17414513425057568\n",
      "  time_since_restore: 84325.83092427254\n",
      "  time_this_iter_s: 230.30125617980957\n",
      "  time_total_s: 84325.83092427254\n",
      "  timers:\n",
      "    learn_throughput: 18.418\n",
      "    learn_time_ms: 217180.493\n",
      "    load_throughput: 3615935.17\n",
      "    load_time_ms: 1.106\n",
      "    sample_throughput: 17.627\n",
      "    sample_time_ms: 226930.787\n",
      "    update_time_ms: 12.876\n",
      "  timestamp: 1650302686\n",
      "  timesteps_since_restore: 1540000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 385\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-28-32\n",
      "  done: false\n",
      "  episode_len_mean: 1234.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 921\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0075917840003967285\n",
      "          total_loss: -0.007591781672090292\n",
      "          vf_explained_var: -9.066122402145993e-06\n",
      "          vf_loss: 6.081659176260246e-09\n",
      "    num_agent_steps_sampled: 1544000\n",
      "    num_agent_steps_trained: 1544000\n",
      "    num_steps_sampled: 1544000\n",
      "    num_steps_trained: 1544000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.91419141914191\n",
      "    ram_util_percent: 95.64752475247523\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09664941578979828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9027259314277651\n",
      "    mean_inference_ms: 1.8420914619980084\n",
      "    mean_raw_obs_processing_ms: 0.11966448223323031\n",
      "  time_since_restore: 84546.39932084084\n",
      "  time_this_iter_s: 226.96050000190735\n",
      "  time_total_s: 84546.39932084084\n",
      "  timers:\n",
      "    learn_throughput: 18.424\n",
      "    learn_time_ms: 217112.19\n",
      "    load_throughput: 6888046.968\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 17.635\n",
      "    sample_time_ms: 226827.821\n",
      "    update_time_ms: 22.814\n",
      "  timestamp: 1650302912\n",
      "  timesteps_since_restore: 1544000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1544000\n",
      "  training_iteration: 386\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-28-33\n",
      "  done: false\n",
      "  episode_len_mean: 549.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.61\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3946\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6912219524383545\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01508418656885624\n",
      "          model: {}\n",
      "          policy_loss: -0.04229298233985901\n",
      "          total_loss: 0.01575544849038124\n",
      "          vf_explained_var: 0.7846122980117798\n",
      "          vf_loss: 0.05804842710494995\n",
      "    num_agent_steps_sampled: 1544000\n",
      "    num_agent_steps_trained: 1544000\n",
      "    num_steps_sampled: 1544000\n",
      "    num_steps_trained: 1544000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.81677631578945\n",
      "    ram_util_percent: 95.66019736842105\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09573163372464727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9175726516614233\n",
      "    mean_inference_ms: 1.8384460405558882\n",
      "    mean_raw_obs_processing_ms: 0.174046174782365\n",
      "  time_since_restore: 84553.17305898666\n",
      "  time_this_iter_s: 227.3421347141266\n",
      "  time_total_s: 84553.17305898666\n",
      "  timers:\n",
      "    learn_throughput: 18.434\n",
      "    learn_time_ms: 216987.216\n",
      "    load_throughput: 3976774.438\n",
      "    load_time_ms: 1.006\n",
      "    sample_throughput: 17.622\n",
      "    sample_time_ms: 226993.96\n",
      "    update_time_ms: 12.388\n",
      "  timestamp: 1650302913\n",
      "  timesteps_since_restore: 1544000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1544000\n",
      "  training_iteration: 386\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1548000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-32-13\n",
      "  done: false\n",
      "  episode_len_mean: 1234.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 921\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0066240401938557625\n",
      "          total_loss: -0.006624039728194475\n",
      "          vf_explained_var: -9.110922292165924e-06\n",
      "          vf_loss: 4.073890114142387e-09\n",
      "    num_agent_steps_sampled: 1548000\n",
      "    num_agent_steps_trained: 1548000\n",
      "    num_steps_sampled: 1548000\n",
      "    num_steps_trained: 1548000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.5016835016835\n",
      "    ram_util_percent: 94.78350168350167\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09664941578979828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9027259314277651\n",
      "    mean_inference_ms: 1.8420914619980084\n",
      "    mean_raw_obs_processing_ms: 0.11966448223323031\n",
      "  time_since_restore: 84767.1660168171\n",
      "  time_this_iter_s: 220.76669597625732\n",
      "  time_total_s: 84767.1660168171\n",
      "  timers:\n",
      "    learn_throughput: 18.435\n",
      "    learn_time_ms: 216978.46\n",
      "    load_throughput: 6781687.215\n",
      "    load_time_ms: 0.59\n",
      "    sample_throughput: 17.636\n",
      "    sample_time_ms: 226806.762\n",
      "    update_time_ms: 23.409\n",
      "  timestamp: 1650303133\n",
      "  timesteps_since_restore: 1548000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1548000\n",
      "  training_iteration: 387\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1548000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-32-14\n",
      "  done: false\n",
      "  episode_len_mean: 549.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.65\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3953\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6942206025123596\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018918922170996666\n",
      "          model: {}\n",
      "          policy_loss: -0.048541776835918427\n",
      "          total_loss: 0.009247828274965286\n",
      "          vf_explained_var: 0.7991594076156616\n",
      "          vf_loss: 0.057789597660303116\n",
      "    num_agent_steps_sampled: 1548000\n",
      "    num_agent_steps_trained: 1548000\n",
      "    num_steps_sampled: 1548000\n",
      "    num_steps_trained: 1548000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.46824324324324\n",
      "    ram_util_percent: 94.74864864864865\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09569542847931975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9173303991225724\n",
      "    mean_inference_ms: 1.8372775666831223\n",
      "    mean_raw_obs_processing_ms: 0.173959354556637\n",
      "  time_since_restore: 84773.9086382389\n",
      "  time_this_iter_s: 220.73557925224304\n",
      "  time_total_s: 84773.9086382389\n",
      "  timers:\n",
      "    learn_throughput: 18.451\n",
      "    learn_time_ms: 216793.202\n",
      "    load_throughput: 3921009.629\n",
      "    load_time_ms: 1.02\n",
      "    sample_throughput: 17.626\n",
      "    sample_time_ms: 226939.256\n",
      "    update_time_ms: 12.388\n",
      "  timestamp: 1650303134\n",
      "  timesteps_since_restore: 1548000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1548000\n",
      "  training_iteration: 387\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-35-55\n",
      "  done: false\n",
      "  episode_len_mean: 1237.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 927\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 7.43650104839833e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8705193991432596e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0014393862802535295\n",
      "          total_loss: -0.0011908648302778602\n",
      "          vf_explained_var: 0.06971503794193268\n",
      "          vf_loss: 0.00024853087961673737\n",
      "    num_agent_steps_sampled: 1552000\n",
      "    num_agent_steps_trained: 1552000\n",
      "    num_steps_sampled: 1552000\n",
      "    num_steps_trained: 1552000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.41043771043772\n",
      "    ram_util_percent: 95.05993265993266\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09656706163542687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9022009598102444\n",
      "    mean_inference_ms: 1.8395570886183434\n",
      "    mean_raw_obs_processing_ms: 0.11959736418842637\n",
      "  time_since_restore: 84989.33444786072\n",
      "  time_this_iter_s: 222.16843104362488\n",
      "  time_total_s: 84989.33444786072\n",
      "  timers:\n",
      "    learn_throughput: 18.489\n",
      "    learn_time_ms: 216344.153\n",
      "    load_throughput: 6532420.667\n",
      "    load_time_ms: 0.612\n",
      "    sample_throughput: 17.643\n",
      "    sample_time_ms: 226722.537\n",
      "    update_time_ms: 23.879\n",
      "  timestamp: 1650303355\n",
      "  timesteps_since_restore: 1552000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1552000\n",
      "  training_iteration: 388\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-35-57\n",
      "  done: false\n",
      "  episode_len_mean: 550.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.65\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3961\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6763577461242676\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016420837491750717\n",
      "          model: {}\n",
      "          policy_loss: -0.05271551385521889\n",
      "          total_loss: 0.017231762409210205\n",
      "          vf_explained_var: 0.7380648255348206\n",
      "          vf_loss: 0.0699472650885582\n",
      "    num_agent_steps_sampled: 1552000\n",
      "    num_agent_steps_trained: 1552000\n",
      "    num_steps_sampled: 1552000\n",
      "    num_steps_trained: 1552000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.65939597315436\n",
      "    ram_util_percent: 95.03758389261745\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0956545016573943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9170547041253161\n",
      "    mean_inference_ms: 1.8359566234077784\n",
      "    mean_raw_obs_processing_ms: 0.17386024333502992\n",
      "  time_since_restore: 84996.57029485703\n",
      "  time_this_iter_s: 222.6616566181183\n",
      "  time_total_s: 84996.57029485703\n",
      "  timers:\n",
      "    learn_throughput: 18.494\n",
      "    learn_time_ms: 216283.284\n",
      "    load_throughput: 3869105.669\n",
      "    load_time_ms: 1.034\n",
      "    sample_throughput: 17.64\n",
      "    sample_time_ms: 226761.958\n",
      "    update_time_ms: 11.904\n",
      "  timestamp: 1650303357\n",
      "  timesteps_since_restore: 1552000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1552000\n",
      "  training_iteration: 388\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 18:36:03 (running for 23:37:23.01)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.65 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1556000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-39-34\n",
      "  done: false\n",
      "  episode_len_mean: 1237.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 927\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0013213349739089608\n",
      "          total_loss: -0.0013213310157880187\n",
      "          vf_explained_var: -7.756179547868669e-05\n",
      "          vf_loss: 3.97454069656078e-09\n",
      "    num_agent_steps_sampled: 1556000\n",
      "    num_agent_steps_trained: 1556000\n",
      "    num_steps_sampled: 1556000\n",
      "    num_steps_trained: 1556000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.02081911262798\n",
      "    ram_util_percent: 95.19180887372012\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09656706163542687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9022009598102444\n",
      "    mean_inference_ms: 1.8395570886183434\n",
      "    mean_raw_obs_processing_ms: 0.11959736418842637\n",
      "  time_since_restore: 85208.61390709877\n",
      "  time_this_iter_s: 219.27945923805237\n",
      "  time_total_s: 85208.61390709877\n",
      "  timers:\n",
      "    learn_throughput: 18.533\n",
      "    learn_time_ms: 215833.065\n",
      "    load_throughput: 6562315.575\n",
      "    load_time_ms: 0.61\n",
      "    sample_throughput: 17.687\n",
      "    sample_time_ms: 226156.494\n",
      "    update_time_ms: 24.579\n",
      "  timestamp: 1650303574\n",
      "  timesteps_since_restore: 1556000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1556000\n",
      "  training_iteration: 389\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1556000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-39-36\n",
      "  done: false\n",
      "  episode_len_mean: 540.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.42\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 3972\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8322871327400208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012886181473731995\n",
      "          model: {}\n",
      "          policy_loss: -0.048771243542432785\n",
      "          total_loss: 0.026721792295575142\n",
      "          vf_explained_var: 0.7262046933174133\n",
      "          vf_loss: 0.07549302279949188\n",
      "    num_agent_steps_sampled: 1556000\n",
      "    num_agent_steps_trained: 1556000\n",
      "    num_steps_sampled: 1556000\n",
      "    num_steps_trained: 1556000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.98737201365186\n",
      "    ram_util_percent: 95.20375426621159\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09560148402523339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9166944234734027\n",
      "    mean_inference_ms: 1.8342385100634908\n",
      "    mean_raw_obs_processing_ms: 0.17373039484502523\n",
      "  time_since_restore: 85215.6229608059\n",
      "  time_this_iter_s: 219.0526659488678\n",
      "  time_total_s: 85215.6229608059\n",
      "  timers:\n",
      "    learn_throughput: 18.539\n",
      "    learn_time_ms: 215764.402\n",
      "    load_throughput: 3896603.493\n",
      "    load_time_ms: 1.027\n",
      "    sample_throughput: 17.672\n",
      "    sample_time_ms: 226350.708\n",
      "    update_time_ms: 10.314\n",
      "  timestamp: 1650303576\n",
      "  timesteps_since_restore: 1556000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1556000\n",
      "  training_iteration: 389\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-43-10\n",
      "  done: false\n",
      "  episode_len_mean: 1237.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 927\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.428951764573554e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2428531069680286e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0005873477784916759\n",
      "          total_loss: -0.0005873477784916759\n",
      "          vf_explained_var: -0.00016075321764219552\n",
      "          vf_loss: 2.835730261718794e-11\n",
      "    num_agent_steps_sampled: 1560000\n",
      "    num_agent_steps_trained: 1560000\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.81546391752579\n",
      "    ram_util_percent: 95.15189003436427\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09656706163542687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9022009598102444\n",
      "    mean_inference_ms: 1.8395570886183434\n",
      "    mean_raw_obs_processing_ms: 0.11959736418842637\n",
      "  time_since_restore: 85424.6996178627\n",
      "  time_this_iter_s: 216.08571076393127\n",
      "  time_total_s: 85424.6996178627\n",
      "  timers:\n",
      "    learn_throughput: 18.61\n",
      "    learn_time_ms: 214934.866\n",
      "    load_throughput: 6670861.233\n",
      "    load_time_ms: 0.6\n",
      "    sample_throughput: 17.725\n",
      "    sample_time_ms: 225674.058\n",
      "    update_time_ms: 23.003\n",
      "  timestamp: 1650303790\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 390\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 539.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.35\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3979\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.5630291104316711\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019085220992565155\n",
      "          model: {}\n",
      "          policy_loss: -0.04543735831975937\n",
      "          total_loss: 0.03528246656060219\n",
      "          vf_explained_var: 0.7407189011573792\n",
      "          vf_loss: 0.08071982860565186\n",
      "    num_agent_steps_sampled: 1560000\n",
      "    num_agent_steps_trained: 1560000\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.11512027491409\n",
      "    ram_util_percent: 95.15498281786942\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09556845884193572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9164702803127011\n",
      "    mean_inference_ms: 1.833176679320965\n",
      "    mean_raw_obs_processing_ms: 0.1736500066323742\n",
      "  time_since_restore: 85432.39110994339\n",
      "  time_this_iter_s: 216.76814913749695\n",
      "  time_total_s: 85432.39110994339\n",
      "  timers:\n",
      "    learn_throughput: 18.622\n",
      "    learn_time_ms: 214794.415\n",
      "    load_throughput: 2807103.586\n",
      "    load_time_ms: 1.425\n",
      "    sample_throughput: 17.705\n",
      "    sample_time_ms: 225920.645\n",
      "    update_time_ms: 9.2\n",
      "  timestamp: 1650303793\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 390\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1564000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-46-50\n",
      "  done: false\n",
      "  episode_len_mean: 1238.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 932\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.0196761393457156e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.4542718195297406e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0009750305325724185\n",
      "          total_loss: -0.0008355408790521324\n",
      "          vf_explained_var: 0.19511233270168304\n",
      "          vf_loss: 0.0001394889986841008\n",
      "    num_agent_steps_sampled: 1564000\n",
      "    num_agent_steps_trained: 1564000\n",
      "    num_steps_sampled: 1564000\n",
      "    num_steps_trained: 1564000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.52380952380952\n",
      "    ram_util_percent: 95.4078231292517\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09650106411620599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9017852795776287\n",
      "    mean_inference_ms: 1.8375480760172218\n",
      "    mean_raw_obs_processing_ms: 0.11954543119464617\n",
      "  time_since_restore: 85644.1746878624\n",
      "  time_this_iter_s: 219.47506999969482\n",
      "  time_total_s: 85644.1746878624\n",
      "  timers:\n",
      "    learn_throughput: 18.705\n",
      "    learn_time_ms: 213840.874\n",
      "    load_throughput: 6770193.293\n",
      "    load_time_ms: 0.591\n",
      "    sample_throughput: 17.79\n",
      "    sample_time_ms: 224848.102\n",
      "    update_time_ms: 23.245\n",
      "  timestamp: 1650304010\n",
      "  timesteps_since_restore: 1564000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1564000\n",
      "  training_iteration: 391\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1564000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-46-51\n",
      "  done: false\n",
      "  episode_len_mean: 539.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: 7.36\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3986\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6999236345291138\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01725885458290577\n",
      "          model: {}\n",
      "          policy_loss: -0.05671004578471184\n",
      "          total_loss: 0.04205828905105591\n",
      "          vf_explained_var: 0.6941454410552979\n",
      "          vf_loss: 0.09876833856105804\n",
      "    num_agent_steps_sampled: 1564000\n",
      "    num_agent_steps_trained: 1564000\n",
      "    num_steps_sampled: 1564000\n",
      "    num_steps_trained: 1564000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.4952218430034\n",
      "    ram_util_percent: 95.39078498293514\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09553585565391483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9162509306801493\n",
      "    mean_inference_ms: 1.8321349516745287\n",
      "    mean_raw_obs_processing_ms: 0.17357121477912824\n",
      "  time_since_restore: 85650.80656814575\n",
      "  time_this_iter_s: 218.41545820236206\n",
      "  time_total_s: 85650.80656814575\n",
      "  timers:\n",
      "    learn_throughput: 18.717\n",
      "    learn_time_ms: 213713.668\n",
      "    load_throughput: 2805554.515\n",
      "    load_time_ms: 1.426\n",
      "    sample_throughput: 17.782\n",
      "    sample_time_ms: 224942.005\n",
      "    update_time_ms: 11.105\n",
      "  timestamp: 1650304011\n",
      "  timesteps_since_restore: 1564000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1564000\n",
      "  training_iteration: 391\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-50-31\n",
      "  done: false\n",
      "  episode_len_mean: 1238.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 932\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1214171574912359e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7975046986496173e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.014127999544143677\n",
      "          total_loss: -0.014126857742667198\n",
      "          vf_explained_var: 2.4258449116132397e-07\n",
      "          vf_loss: 1.138880634243833e-06\n",
      "    num_agent_steps_sampled: 1568000\n",
      "    num_agent_steps_trained: 1568000\n",
      "    num_steps_sampled: 1568000\n",
      "    num_steps_trained: 1568000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.93288135593221\n",
      "    ram_util_percent: 95.67016949152544\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09650106411620599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9017852795776287\n",
      "    mean_inference_ms: 1.8375480760172218\n",
      "    mean_raw_obs_processing_ms: 0.11954543119464617\n",
      "  time_since_restore: 85864.98912096024\n",
      "  time_this_iter_s: 220.81443309783936\n",
      "  time_total_s: 85864.98912096024\n",
      "  timers:\n",
      "    learn_throughput: 18.791\n",
      "    learn_time_ms: 212872.953\n",
      "    load_throughput: 6857359.601\n",
      "    load_time_ms: 0.583\n",
      "    sample_throughput: 17.874\n",
      "    sample_time_ms: 223790.074\n",
      "    update_time_ms: 21.703\n",
      "  timestamp: 1650304231\n",
      "  timesteps_since_restore: 1568000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1568000\n",
      "  training_iteration: 392\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-50-32\n",
      "  done: false\n",
      "  episode_len_mean: 539.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.0\n",
      "  episode_reward_mean: 7.23\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3993\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.431473368844308e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6041303277015686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02113368734717369\n",
      "          model: {}\n",
      "          policy_loss: -0.035412415862083435\n",
      "          total_loss: 0.039213649928569794\n",
      "          vf_explained_var: 0.6486552953720093\n",
      "          vf_loss: 0.07462605834007263\n",
      "    num_agent_steps_sampled: 1568000\n",
      "    num_agent_steps_trained: 1568000\n",
      "    num_steps_sampled: 1568000\n",
      "    num_steps_trained: 1568000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.65136054421768\n",
      "    ram_util_percent: 95.6591836734694\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09550449913170905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9160377592277947\n",
      "    mean_inference_ms: 1.8311215160888132\n",
      "    mean_raw_obs_processing_ms: 0.17349270207865466\n",
      "  time_since_restore: 85871.73831105232\n",
      "  time_this_iter_s: 220.93174290657043\n",
      "  time_total_s: 85871.73831105232\n",
      "  timers:\n",
      "    learn_throughput: 18.808\n",
      "    learn_time_ms: 212676.932\n",
      "    load_throughput: 2800496.762\n",
      "    load_time_ms: 1.428\n",
      "    sample_throughput: 17.866\n",
      "    sample_time_ms: 223890.907\n",
      "    update_time_ms: 11.045\n",
      "  timestamp: 1650304232\n",
      "  timesteps_since_restore: 1568000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1568000\n",
      "  training_iteration: 392\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 18:52:43 (running for 23:54:03.39)<br>Memory usage on this node: 7.7/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.23 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1572000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-54-12\n",
      "  done: false\n",
      "  episode_len_mean: 1336.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 934\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 1.1217223095890855e-19\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.7083189273779276e-27\n",
      "          model: {}\n",
      "          policy_loss: -0.0010564385447651148\n",
      "          total_loss: -0.0010024551302194595\n",
      "          vf_explained_var: 0.06486812233924866\n",
      "          vf_loss: 5.3986350394552574e-05\n",
      "    num_agent_steps_sampled: 1572000\n",
      "    num_agent_steps_trained: 1572000\n",
      "    num_steps_sampled: 1572000\n",
      "    num_steps_trained: 1572000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.9331081081081\n",
      "    ram_util_percent: 95.69966216216217\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09647334093189006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9016111088557051\n",
      "    mean_inference_ms: 1.8367125209104265\n",
      "    mean_raw_obs_processing_ms: 0.11952223615999041\n",
      "  time_since_restore: 86086.49877023697\n",
      "  time_this_iter_s: 221.5096492767334\n",
      "  time_total_s: 86086.49877023697\n",
      "  timers:\n",
      "    learn_throughput: 18.812\n",
      "    learn_time_ms: 212633.316\n",
      "    load_throughput: 6985558.563\n",
      "    load_time_ms: 0.573\n",
      "    sample_throughput: 17.95\n",
      "    sample_time_ms: 222840.786\n",
      "    update_time_ms: 20.009\n",
      "  timestamp: 1650304452\n",
      "  timesteps_since_restore: 1572000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1572000\n",
      "  training_iteration: 393\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1572000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-54-14\n",
      "  done: false\n",
      "  episode_len_mean: 528.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.0\n",
      "  episode_reward_mean: 7.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 4004\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.147209769049368e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.797877311706543\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014929954893887043\n",
      "          model: {}\n",
      "          policy_loss: -0.04973161593079567\n",
      "          total_loss: 0.0437660776078701\n",
      "          vf_explained_var: 0.6123875379562378\n",
      "          vf_loss: 0.09349768608808517\n",
      "    num_agent_steps_sampled: 1572000\n",
      "    num_agent_steps_trained: 1572000\n",
      "    num_steps_sampled: 1572000\n",
      "    num_steps_trained: 1572000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.61010101010102\n",
      "    ram_util_percent: 95.6895622895623\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09545764860206081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9157131326217208\n",
      "    mean_inference_ms: 1.8295861377125675\n",
      "    mean_raw_obs_processing_ms: 0.17337605006692058\n",
      "  time_since_restore: 86093.6870880127\n",
      "  time_this_iter_s: 221.94877696037292\n",
      "  time_total_s: 86093.6870880127\n",
      "  timers:\n",
      "    learn_throughput: 18.831\n",
      "    learn_time_ms: 212410.489\n",
      "    load_throughput: 4010042.545\n",
      "    load_time_ms: 0.997\n",
      "    sample_throughput: 17.945\n",
      "    sample_time_ms: 222901.718\n",
      "    update_time_ms: 10.725\n",
      "  timestamp: 1650304454\n",
      "  timesteps_since_restore: 1572000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1572000\n",
      "  training_iteration: 393\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-57-54\n",
      "  done: false\n",
      "  episode_len_mean: 1336.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 934\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.85856733927962e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.67308352844109e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.014128033071756363\n",
      "          total_loss: -0.01412415225058794\n",
      "          vf_explained_var: -7.197421325599862e-08\n",
      "          vf_loss: 3.879717951349448e-06\n",
      "    num_agent_steps_sampled: 1576000\n",
      "    num_agent_steps_trained: 1576000\n",
      "    num_steps_sampled: 1576000\n",
      "    num_steps_trained: 1576000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.97661016949152\n",
      "    ram_util_percent: 95.73389830508475\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09647334093189006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9016111088557051\n",
      "    mean_inference_ms: 1.8367125209104265\n",
      "    mean_raw_obs_processing_ms: 0.11952223615999041\n",
      "  time_since_restore: 86307.76983213425\n",
      "  time_this_iter_s: 221.27106189727783\n",
      "  time_total_s: 86307.76983213425\n",
      "  timers:\n",
      "    learn_throughput: 18.871\n",
      "    learn_time_ms: 211965.41\n",
      "    load_throughput: 7022400.067\n",
      "    load_time_ms: 0.57\n",
      "    sample_throughput: 17.97\n",
      "    sample_time_ms: 222589.397\n",
      "    update_time_ms: 19.884\n",
      "  timestamp: 1650304674\n",
      "  timesteps_since_restore: 1576000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1576000\n",
      "  training_iteration: 394\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_18-57-55\n",
      "  done: false\n",
      "  episode_len_mean: 532.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.0\n",
      "  episode_reward_mean: 7.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 4011\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.147209769049368e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.6751320958137512\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01793050393462181\n",
      "          model: {}\n",
      "          policy_loss: -0.046582259237766266\n",
      "          total_loss: 0.011351050809025764\n",
      "          vf_explained_var: 0.740090012550354\n",
      "          vf_loss: 0.057933297008275986\n",
      "    num_agent_steps_sampled: 1576000\n",
      "    num_agent_steps_trained: 1576000\n",
      "    num_steps_sampled: 1576000\n",
      "    num_steps_trained: 1576000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.55525423728814\n",
      "    ram_util_percent: 95.76338983050846\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0954290539849928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9155126952781347\n",
      "    mean_inference_ms: 1.8286389597518524\n",
      "    mean_raw_obs_processing_ms: 0.17330573485510758\n",
      "  time_since_restore: 86314.74170517921\n",
      "  time_this_iter_s: 221.05461716651917\n",
      "  time_total_s: 86314.74170517921\n",
      "  timers:\n",
      "    learn_throughput: 18.892\n",
      "    learn_time_ms: 211726.917\n",
      "    load_throughput: 4072436.342\n",
      "    load_time_ms: 0.982\n",
      "    sample_throughput: 17.967\n",
      "    sample_time_ms: 222635.458\n",
      "    update_time_ms: 10.12\n",
      "  timestamp: 1650304675\n",
      "  timesteps_since_restore: 1576000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1576000\n",
      "  training_iteration: 394\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 1336.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 934\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.85856733927962e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -7.67308352844109e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.014128260314464569\n",
      "          total_loss: -0.014126806519925594\n",
      "          vf_explained_var: -1.202988357817958e-07\n",
      "          vf_loss: 1.4561857142325607e-06\n",
      "    num_agent_steps_sampled: 1580000\n",
      "    num_agent_steps_trained: 1580000\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.24242424242425\n",
      "    ram_util_percent: 95.75185185185185\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09647334093189006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9016111088557051\n",
      "    mean_inference_ms: 1.8367125209104265\n",
      "    mean_raw_obs_processing_ms: 0.11952223615999041\n",
      "  time_since_restore: 86530.96535205841\n",
      "  time_this_iter_s: 223.19551992416382\n",
      "  time_total_s: 86530.96535205841\n",
      "  timers:\n",
      "    learn_throughput: 18.933\n",
      "    learn_time_ms: 211265.847\n",
      "    load_throughput: 7160570.209\n",
      "    load_time_ms: 0.559\n",
      "    sample_throughput: 18.022\n",
      "    sample_time_ms: 221946.425\n",
      "    update_time_ms: 15.799\n",
      "  timestamp: 1650304897\n",
      "  timesteps_since_restore: 1580000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 395\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-01-38\n",
      "  done: false\n",
      "  episode_len_mean: 519.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.0\n",
      "  episode_reward_mean: 6.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 4021\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.147209769049368e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8215359449386597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015375475399196148\n",
      "          model: {}\n",
      "          policy_loss: -0.05402018874883652\n",
      "          total_loss: 0.014963974244892597\n",
      "          vf_explained_var: 0.7717959880828857\n",
      "          vf_loss: 0.06898416578769684\n",
      "    num_agent_steps_sampled: 1580000\n",
      "    num_agent_steps_trained: 1580000\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.49393939393939\n",
      "    ram_util_percent: 95.77878787878788\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09539069417322071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9152397468512234\n",
      "    mean_inference_ms: 1.8273336573089451\n",
      "    mean_raw_obs_processing_ms: 0.17321364331163097\n",
      "  time_since_restore: 86537.4958062172\n",
      "  time_this_iter_s: 222.75410103797913\n",
      "  time_total_s: 86537.4958062172\n",
      "  timers:\n",
      "    learn_throughput: 18.962\n",
      "    learn_time_ms: 210944.607\n",
      "    load_throughput: 2450480.684\n",
      "    load_time_ms: 1.632\n",
      "    sample_throughput: 18.02\n",
      "    sample_time_ms: 221980.834\n",
      "    update_time_ms: 9.542\n",
      "  timestamp: 1650304898\n",
      "  timesteps_since_restore: 1580000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 395\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-05-26\n",
      "  done: false\n",
      "  episode_len_mean: 1238.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.46\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 939\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 8.699368675575548e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.560396157346623e-28\n",
      "          model: {}\n",
      "          policy_loss: -0.0003122170746792108\n",
      "          total_loss: -0.0002789073623716831\n",
      "          vf_explained_var: -4.764462210005149e-05\n",
      "          vf_loss: 3.330465915496461e-05\n",
      "    num_agent_steps_sampled: 1584000\n",
      "    num_agent_steps_trained: 1584000\n",
      "    num_steps_sampled: 1584000\n",
      "    num_steps_trained: 1584000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.6171052631579\n",
      "    ram_util_percent: 95.54934210526316\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09640691882903374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9011966719582429\n",
      "    mean_inference_ms: 1.8347345914862745\n",
      "    mean_raw_obs_processing_ms: 0.11946962635419421\n",
      "  time_since_restore: 86759.42504811287\n",
      "  time_this_iter_s: 228.45969605445862\n",
      "  time_total_s: 86759.42504811287\n",
      "  timers:\n",
      "    learn_throughput: 18.917\n",
      "    learn_time_ms: 211453.665\n",
      "    load_throughput: 7094260.222\n",
      "    load_time_ms: 0.564\n",
      "    sample_throughput: 18.084\n",
      "    sample_time_ms: 221194.931\n",
      "    update_time_ms: 17.83\n",
      "  timestamp: 1650305126\n",
      "  timesteps_since_restore: 1584000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1584000\n",
      "  training_iteration: 396\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-05-26\n",
      "  done: false\n",
      "  episode_len_mean: 516.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.0\n",
      "  episode_reward_mean: 7.02\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 4029\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.147209769049368e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7252366542816162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01601594127714634\n",
      "          model: {}\n",
      "          policy_loss: -0.04140178859233856\n",
      "          total_loss: 0.042199332267045975\n",
      "          vf_explained_var: 0.7470954060554504\n",
      "          vf_loss: 0.08360111713409424\n",
      "    num_agent_steps_sampled: 1584000\n",
      "    num_agent_steps_trained: 1584000\n",
      "    num_steps_sampled: 1584000\n",
      "    num_steps_trained: 1584000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.9881188118812\n",
      "    ram_util_percent: 95.54554455445546\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09536051877940142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9150231417726546\n",
      "    mean_inference_ms: 1.8262963744707001\n",
      "    mean_raw_obs_processing_ms: 0.1731400809893196\n",
      "  time_since_restore: 86765.27808713913\n",
      "  time_this_iter_s: 227.78228092193604\n",
      "  time_total_s: 86765.27808713913\n",
      "  timers:\n",
      "    learn_throughput: 18.949\n",
      "    learn_time_ms: 211094.562\n",
      "    load_throughput: 2444518.009\n",
      "    load_time_ms: 1.636\n",
      "    sample_throughput: 18.092\n",
      "    sample_time_ms: 221095.409\n",
      "    update_time_ms: 9.661\n",
      "  timestamp: 1650305126\n",
      "  timesteps_since_restore: 1584000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1584000\n",
      "  training_iteration: 396\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1588000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-09-06\n",
      "  done: false\n",
      "  episode_len_mean: 1238.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.46\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 939\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.014128146693110466\n",
      "          total_loss: 0.014128439128398895\n",
      "          vf_explained_var: 3.046245922178059e-07\n",
      "          vf_loss: 2.9547572921728715e-07\n",
      "    num_agent_steps_sampled: 1588000\n",
      "    num_agent_steps_trained: 1588000\n",
      "    num_steps_sampled: 1588000\n",
      "    num_steps_trained: 1588000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.67830508474576\n",
      "    ram_util_percent: 95.06000000000002\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09640691882903374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9011966719582429\n",
      "    mean_inference_ms: 1.8347345914862745\n",
      "    mean_raw_obs_processing_ms: 0.11946962635419421\n",
      "  time_since_restore: 86979.64049911499\n",
      "  time_this_iter_s: 220.21545100212097\n",
      "  time_total_s: 86979.64049911499\n",
      "  timers:\n",
      "    learn_throughput: 18.911\n",
      "    learn_time_ms: 211521.126\n",
      "    load_throughput: 7332058.387\n",
      "    load_time_ms: 0.546\n",
      "    sample_throughput: 18.079\n",
      "    sample_time_ms: 221254.683\n",
      "    update_time_ms: 17.099\n",
      "  timestamp: 1650305346\n",
      "  timesteps_since_restore: 1588000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1588000\n",
      "  training_iteration: 397\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1588000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-09-07\n",
      "  done: false\n",
      "  episode_len_mean: 522.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.0\n",
      "  episode_reward_mean: 7.24\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 4035\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.147209769049368e-07\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.623254656791687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02510131523013115\n",
      "          model: {}\n",
      "          policy_loss: -0.03792900592088699\n",
      "          total_loss: 0.04366438835859299\n",
      "          vf_explained_var: 0.7561636567115784\n",
      "          vf_loss: 0.08159337192773819\n",
      "    num_agent_steps_sampled: 1588000\n",
      "    num_agent_steps_trained: 1588000\n",
      "    num_steps_sampled: 1588000\n",
      "    num_steps_trained: 1588000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.72881355932203\n",
      "    ram_util_percent: 95.0671186440678\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09533755938436415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9148569337897966\n",
      "    mean_inference_ms: 1.8255150447901252\n",
      "    mean_raw_obs_processing_ms: 0.17308216980526922\n",
      "  time_since_restore: 86985.99596118927\n",
      "  time_this_iter_s: 220.71787405014038\n",
      "  time_total_s: 86985.99596118927\n",
      "  timers:\n",
      "    learn_throughput: 18.938\n",
      "    learn_time_ms: 211216.2\n",
      "    load_throughput: 2457012.141\n",
      "    load_time_ms: 1.628\n",
      "    sample_throughput: 18.09\n",
      "    sample_time_ms: 221121.729\n",
      "    update_time_ms: 9.658\n",
      "  timestamp: 1650305347\n",
      "  timesteps_since_restore: 1588000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1588000\n",
      "  training_iteration: 397\n",
      "  trial_id: 0358e_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 19:09:24 (running for 1 days, 00:10:44.31)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=7.24 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-12-48\n",
      "  done: false\n",
      "  episode_len_mean: 528.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.0\n",
      "  episode_reward_mean: 7.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 4043\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2220814369356958e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7133259177207947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015245115384459496\n",
      "          model: {}\n",
      "          policy_loss: -0.040091533213853836\n",
      "          total_loss: 0.02726101689040661\n",
      "          vf_explained_var: 0.7081007361412048\n",
      "          vf_loss: 0.0673525333404541\n",
      "    num_agent_steps_sampled: 1592000\n",
      "    num_agent_steps_trained: 1592000\n",
      "    num_steps_sampled: 1592000\n",
      "    num_steps_trained: 1592000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.39662162162162\n",
      "    ram_util_percent: 95.23445945945946\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0953061266319974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9146265368838297\n",
      "    mean_inference_ms: 1.824441023763428\n",
      "    mean_raw_obs_processing_ms: 0.1729999335670468\n",
      "  time_since_restore: 87207.1031961441\n",
      "  time_this_iter_s: 221.10723495483398\n",
      "  time_total_s: 87207.1031961441\n",
      "  timers:\n",
      "    learn_throughput: 18.95\n",
      "    learn_time_ms: 211080.55\n",
      "    load_throughput: 2474041.261\n",
      "    load_time_ms: 1.617\n",
      "    sample_throughput: 18.081\n",
      "    sample_time_ms: 221221.603\n",
      "    update_time_ms: 10.138\n",
      "  timestamp: 1650305568\n",
      "  timesteps_since_restore: 1592000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1592000\n",
      "  training_iteration: 398\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-12-48\n",
      "  done: false\n",
      "  episode_len_mean: 1238.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.46\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 939\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.637099487260306e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.097731038822746e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.01412828266620636\n",
      "          total_loss: 0.014128313399851322\n",
      "          vf_explained_var: -3.140459625683434e-08\n",
      "          vf_loss: 3.2604503985567135e-08\n",
      "    num_agent_steps_sampled: 1592000\n",
      "    num_agent_steps_trained: 1592000\n",
      "    num_steps_sampled: 1592000\n",
      "    num_steps_trained: 1592000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.10641891891892\n",
      "    ram_util_percent: 95.23175675675677\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09640691882903374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9011966719582429\n",
      "    mean_inference_ms: 1.8347345914862745\n",
      "    mean_raw_obs_processing_ms: 0.11946962635419421\n",
      "  time_since_restore: 87201.49340415001\n",
      "  time_this_iter_s: 221.85290503501892\n",
      "  time_total_s: 87201.49340415001\n",
      "  timers:\n",
      "    learn_throughput: 18.909\n",
      "    learn_time_ms: 211544.951\n",
      "    load_throughput: 7513644.14\n",
      "    load_time_ms: 0.532\n",
      "    sample_throughput: 18.077\n",
      "    sample_time_ms: 221276.665\n",
      "    update_time_ms: 17.142\n",
      "  timestamp: 1650305568\n",
      "  timesteps_since_restore: 1592000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1592000\n",
      "  training_iteration: 398\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1596000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-16-29\n",
      "  done: false\n",
      "  episode_len_mean: 1236.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 941\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.892610408599107e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -1.6292387031174322e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.0002786997065413743\n",
      "          total_loss: 0.0003485805355012417\n",
      "          vf_explained_var: 0.033155616372823715\n",
      "          vf_loss: 6.988175300648436e-05\n",
      "    num_agent_steps_sampled: 1596000\n",
      "    num_agent_steps_trained: 1596000\n",
      "    num_steps_sampled: 1596000\n",
      "    num_steps_trained: 1596000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.43661016949153\n",
      "    ram_util_percent: 95.39118644067796\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09638052484811281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9010285797766671\n",
      "    mean_inference_ms: 1.8339376857880296\n",
      "    mean_raw_obs_processing_ms: 0.11944663749214068\n",
      "  time_since_restore: 87422.35955095291\n",
      "  time_this_iter_s: 220.86614680290222\n",
      "  time_total_s: 87422.35955095291\n",
      "  timers:\n",
      "    learn_throughput: 18.886\n",
      "    learn_time_ms: 211800.779\n",
      "    load_throughput: 7514317.194\n",
      "    load_time_ms: 0.532\n",
      "    sample_throughput: 18.084\n",
      "    sample_time_ms: 221193.345\n",
      "    update_time_ms: 18.977\n",
      "  timestamp: 1650305789\n",
      "  timesteps_since_restore: 1596000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1596000\n",
      "  training_iteration: 399\n",
      "  trial_id: 0358e_00000\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1596000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-16-29\n",
      "  done: false\n",
      "  episode_len_mean: 510.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.0\n",
      "  episode_reward_mean: 6.97\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 4053\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2220814369356958e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.8762773871421814\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014819376170635223\n",
      "          model: {}\n",
      "          policy_loss: -0.03947902470827103\n",
      "          total_loss: 0.02410367876291275\n",
      "          vf_explained_var: 0.7252617478370667\n",
      "          vf_loss: 0.06358268857002258\n",
      "    num_agent_steps_sampled: 1596000\n",
      "    num_agent_steps_trained: 1596000\n",
      "    num_steps_sampled: 1596000\n",
      "    num_steps_trained: 1596000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.40844594594596\n",
      "    ram_util_percent: 95.40912162162162\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09526423450306222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9143230245011196\n",
      "    mean_inference_ms: 1.8230450408611079\n",
      "    mean_raw_obs_processing_ms: 0.17289921751206258\n",
      "  time_since_restore: 87428.5188729763\n",
      "  time_this_iter_s: 221.4156768321991\n",
      "  time_total_s: 87428.5188729763\n",
      "  timers:\n",
      "    learn_throughput: 18.921\n",
      "    learn_time_ms: 211408.818\n",
      "    load_throughput: 2461518.237\n",
      "    load_time_ms: 1.625\n",
      "    sample_throughput: 18.099\n",
      "    sample_time_ms: 221003.252\n",
      "    update_time_ms: 10.786\n",
      "  timestamp: 1650305789\n",
      "  timesteps_since_restore: 1596000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1596000\n",
      "  training_iteration: 399\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00001:\n",
      "  agent_timesteps_total: 1600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-20-11\n",
      "  done: true\n",
      "  episode_len_mean: 506.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 31.0\n",
      "  episode_reward_mean: 6.88\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 4062\n",
      "  experiment_id: 1d57374ab85b44be8132b99fd19a989f\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2220814369356958e-06\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 0.7816181182861328\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014545651152729988\n",
      "          model: {}\n",
      "          policy_loss: -0.053122881799936295\n",
      "          total_loss: 0.0052426718175411224\n",
      "          vf_explained_var: 0.8089796304702759\n",
      "          vf_loss: 0.05836554989218712\n",
      "    num_agent_steps_sampled: 1600000\n",
      "    num_agent_steps_trained: 1600000\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.51722972972972\n",
      "    ram_util_percent: 95.52466216216217\n",
      "  pid: 98216\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09522713881695498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9140535789461205\n",
      "    mean_inference_ms: 1.8218282390642497\n",
      "    mean_raw_obs_processing_ms: 0.17281010977463396\n",
      "  time_since_restore: 87650.57076025009\n",
      "  time_this_iter_s: 222.05188727378845\n",
      "  time_total_s: 87650.57076025009\n",
      "  timers:\n",
      "    learn_throughput: 18.871\n",
      "    learn_time_ms: 211967.876\n",
      "    load_throughput: 3277056.02\n",
      "    load_time_ms: 1.221\n",
      "    sample_throughput: 18.075\n",
      "    sample_time_ms: 221304.081\n",
      "    update_time_ms: 10.796\n",
      "  timestamp: 1650306011\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 400\n",
      "  trial_id: 0358e_00001\n",
      "  \n",
      "Result for PPOTrainer_Breakout-v0_0358e_00000:\n",
      "  agent_timesteps_total: 1600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-18_19-20-11\n",
      "  done: true\n",
      "  episode_len_mean: 1236.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 941\n",
      "  experiment_id: 7358aea039544e02948ddc141f498aa4\n",
      "  hostname: Suens-MBP\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-06\n",
      "          entropy: 6.662416383883257e-20\n",
      "          entropy_coeff: 0.0\n",
      "          kl: -2.5498326387555476e-27\n",
      "          model: {}\n",
      "          policy_loss: 0.007334247697144747\n",
      "          total_loss: 0.007334248162806034\n",
      "          vf_explained_var: -2.34387243835954e-06\n",
      "          vf_loss: 3.6558043259304895e-09\n",
      "    num_agent_steps_sampled: 1600000\n",
      "    num_agent_steps_trained: 1600000\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.39324324324325\n",
      "    ram_util_percent: 95.52331081081081\n",
      "  pid: 98215\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09638052484811281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9010285797766671\n",
      "    mean_inference_ms: 1.8339376857880296\n",
      "    mean_raw_obs_processing_ms: 0.11944663749214068\n",
      "  time_since_restore: 87644.78141522408\n",
      "  time_this_iter_s: 222.42186427116394\n",
      "  time_total_s: 87644.78141522408\n",
      "  timers:\n",
      "    learn_throughput: 18.827\n",
      "    learn_time_ms: 212464.865\n",
      "    load_throughput: 7391820.945\n",
      "    load_time_ms: 0.541\n",
      "    sample_throughput: 18.065\n",
      "    sample_time_ms: 221428.032\n",
      "    update_time_ms: 18.741\n",
      "  timestamp: 1650306011\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 400\n",
      "  trial_id: 0358e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=98215)\u001b[0m 2022-04-18 19:20:11,864\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=98216)\u001b[0m 2022-04-18 19:20:11,844\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-18 19:20:12 (running for 1 days, 00:21:31.76)<br>Memory usage on this node: 7.6/8.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.37 GiB heap, 0.0/2.0 GiB objects<br>Current best trial: 0358e_00001 with episode_reward_mean=6.88 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 1e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Breakout-v0', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([[[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]\n",
       "\n",
       " [[0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  ...\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]\n",
       "  [0 0 0 0]]], [[[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]\n",
       "\n",
       " [[255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  ...\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]\n",
       "  [255 255 255 255]]], (84, 84, 4), uint8), action_space=Discrete(4), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.1, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}<br>Result logdir: /Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  clip_param</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_Breakout-v0_0358e_00000</td><td>TERMINATED</td><td>127.0.0.1:98215</td><td style=\"text-align: right;\">         0.2</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         87644.8</td><td style=\"text-align: right;\">1600000</td><td style=\"text-align: right;\">    1.41</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           1236.29</td></tr>\n",
       "<tr><td>PPOTrainer_Breakout-v0_0358e_00001</td><td>TERMINATED</td><td>127.0.0.1:98216</td><td style=\"text-align: right;\">         0.1</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         87650.6</td><td style=\"text-align: right;\">1600000</td><td style=\"text-align: right;\">    6.88</td><td style=\"text-align: right;\">                  31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            506.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 19:20:12,916\tINFO tune.py:639 -- Total run time: 87692.61 seconds (87691.70 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "#Set hyperparameters to tune\n",
    "config_ppo_tune={\n",
    "        \"env\": \"Breakout-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"framework\": \"tf\",\n",
    "        \"clip_param\": tune.grid_search([0.2, 0.1]),\n",
    "        \"lr\": 1e-5,\n",
    "    }\n",
    "#set stopping criteria & reporting style/frequency for tuning\n",
    "stop = tune.stopper.MaximumIterationStopper(max_iter=400)\n",
    "reporter = JupyterNotebookReporter(overwrite=False, max_report_frequency=1000)   #unit: seconds\n",
    "\n",
    "#tune hyperparameters and save checkpoints\n",
    "randomize()\n",
    "# tune.run() allows setting a custom log directory (other than ``~/ray-results``)\n",
    "# and automatically saving the trained agent\n",
    "analysis = tune.run(\n",
    "    ppo.PPOTrainer,\n",
    "    config=config_ppo_tune,\n",
    "    stop=stop,\n",
    "    verbose=3,\n",
    "    progress_reporter=reporter,\n",
    "    metric=\"episode_reward_mean\", mode=\"max\",\n",
    "    checkpoint_at_end=True,)\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "# if there are multiple trials, select a specific trial or automatically\n",
    "# choose the best one according to a given metric\n",
    "last_checkpoint = analysis.get_last_checkpoint(\n",
    "    metric=\"episode_reward_mean\", mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c372f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/Users/suenchihang/ray_results/PPOTrainer_2022-04-17_18-58-40/PPOTrainer_Breakout-v0_0358e_00001_1_clip_param=0.1_2022-04-17_18-58-42/checkpoint_000400/checkpoint-400', 6.88)]\n"
     ]
    }
   ],
   "source": [
    "print(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d95cfe12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'env': 'Breakout-v0', 'num_gpus': 0, 'num_workers': 1, 'framework': 'tf', 'clip_param': 0.1, 'lr': 1e-05}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config/clip_param</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1144000</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1232000</td>\n",
       "      <td>8.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   config/clip_param  config/lr  timesteps_total  episode_reward_mean\n",
       "0                0.2    0.00001          1144000                 1.72\n",
       "1                0.1    0.00001          1232000                 8.60"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "df = analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "df[['config/clip_param', 'config/lr', 'timesteps_total', 'episode_reward_mean', ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf26b496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'PPO learning Breakout')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs = analysis.trial_dataframes\n",
    "# Plot by epoch\n",
    "ax = None  # This plots everything on the same plot\n",
    "for idx, d in enumerate(dfs.values()):\n",
    "    ax = d.episode_reward_mean.plot(ax=ax, label=\"Clip: {}\".format(df[[\"config/clip_param\"]].iloc[idx][0]), legend=True)\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Reward per episode\")\n",
    "ax.set_title(\"PPO learning Breakout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load DQN rewards\n",
    "with open(\"avg_rewards1_restore.txt\", \"r\") as file:\n",
    "    avg_rewards1_restore = eval(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b116244",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i+200 for i in range(800)], avg_rewards1_restore, label='DQN rainbow excl dueling')\n",
    "plt.plot(last_checkpoint[1], label='PPO')\n",
    "\n",
    "plt.xlabel('Count of Iterations')\n",
    "plt.ylabel('Reward per Episode')\n",
    "\n",
    "plt.title('Average Reward in Breakout')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42bf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
